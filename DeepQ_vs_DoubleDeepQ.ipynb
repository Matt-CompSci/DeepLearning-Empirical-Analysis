{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mountain.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyN4WkPJ/jA9AhqeaaTsoSlp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tP-7XstCkr9I"},"source":["Observation:\n","Type: Box(2)\n","\n","\n","Num    Observation              Min            Max\n","\n","\n","0      Car Position              -1.2           0.6\n","\n","\n","1      Car Velocity              -0.07          0.07\n","\n","\n","Actions:\n","    Type: Discrete(3)\n","    Num    Action\n","    0      Accelerate to the Left\n","    1      Don't accelerate\n","    2      Accelerate to the Right"]},{"cell_type":"code","metadata":{"id":"8o5UW_aWkopy","executionInfo":{"status":"ok","timestamp":1620581869268,"user_tz":-60,"elapsed":551,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}}},"source":["import gym\n","env = gym.make('MountainCar-v0')\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14wuCmVNy6mm","executionInfo":{"status":"ok","timestamp":1620581882718,"user_tz":-60,"elapsed":13989,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"a8a796df-acae-4902-e76a-9c891e1aaa3a"},"source":["import tensorflow as tf\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"\")\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","print(\"All devices: \", tf.config.list_logical_devices(\"TPU\"))\n","\n","strategy = tf.distribute.TPUStrategy(resolver)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.104.240.58:8470\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.104.240.58:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n","INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LPj4LPeltfcd","executionInfo":{"status":"ok","timestamp":1620581882721,"user_tz":-60,"elapsed":13983,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Activation, Flatten\n","from keras.optimizers import Adam"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbDi0xxrxMau","executionInfo":{"status":"ok","timestamp":1620584686765,"user_tz":-60,"elapsed":2818019,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"51d87018-406e-4700-e2fc-da29ee1796c2"},"source":["!pip install keras-rl2\n","\n","from rl.memory import SequentialMemory\n","from rl.policy import BoltzmannQPolicy\n","from rl.agents import DQNAgent\n","\n","model = Sequential()\n","model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n","model.add(Dense(128, activation=\"relu\"))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dense(32, activation=\"relu\"))\n","model.add(Dense(env.action_space.n, activation=\"linear\"))\n","\n","dqn = DQNAgent(\n","    model=model, \n","    nb_actions=env.action_space.n, \n","    memory=SequentialMemory(limit=50000, window_length=1), \n","    nb_steps_warmup=10,\n","    target_model_update=1e-2, \n","    policy=BoltzmannQPolicy())\n","\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","history_single = dqn.fit(env, nb_steps=300000, visualize=False, verbose=2)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.4)\n","Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.4.1)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.7.4.3)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.3.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.2)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.12.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.6.3)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (3.12.4)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.0)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.4.1)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.19.5)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.15.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12.1)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.32.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.1.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.36.2)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.2.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (0.3.3)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (2.10.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->keras-rl2) (1.12)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1.0->keras-rl2) (56.1.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.28.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.0.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.3.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.0.4)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.10.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->keras-rl2) (3.4.1)\n","Training for 300000 steps ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n","/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"],"name":"stderr"},{"output_type":"stream","text":["    200/300000: episode: 1, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.047220, mae: 0.917146, mean_q: -1.238412\n","    400/300000: episode: 2, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.005420, mae: 1.905265, mean_q: -2.822190\n","    600/300000: episode: 3, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.025026, mae: 3.045906, mean_q: -4.496369\n","    800/300000: episode: 4, duration: 1.793s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.068641, mae: 4.208888, mean_q: -6.206635\n","   1000/300000: episode: 5, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.160589, mae: 5.335208, mean_q: -7.850828\n","   1200/300000: episode: 6, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.170110, mae: 6.437790, mean_q: -9.522399\n","   1400/300000: episode: 7, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.257297, mae: 7.526653, mean_q: -11.100855\n","   1600/300000: episode: 8, duration: 1.732s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.255460, mae: 8.581074, mean_q: -12.707908\n","   1800/300000: episode: 9, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.516798, mae: 9.634701, mean_q: -14.209455\n","   2000/300000: episode: 10, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.527686, mae: 10.619761, mean_q: -15.695374\n","   2200/300000: episode: 11, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.604232, mae: 11.594547, mean_q: -17.113310\n","   2400/300000: episode: 12, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.588525, mae: 12.556545, mean_q: -18.597569\n","   2600/300000: episode: 13, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.832542, mae: 13.522757, mean_q: -19.990261\n","   2800/300000: episode: 14, duration: 1.767s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.748346, mae: 14.439406, mean_q: -21.418205\n","   3000/300000: episode: 15, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.130950, mae: 15.293875, mean_q: -22.637783\n","   3200/300000: episode: 16, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 1.272143, mae: 16.133570, mean_q: -23.894659\n","   3400/300000: episode: 17, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.352022, mae: 16.963675, mean_q: -25.072910\n","   3600/300000: episode: 18, duration: 1.794s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.382266, mae: 17.714275, mean_q: -26.237333\n","   3800/300000: episode: 19, duration: 1.770s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 1.430169, mae: 18.456352, mean_q: -27.326197\n","   4000/300000: episode: 20, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.433760, mae: 19.173311, mean_q: -28.403643\n","   4200/300000: episode: 21, duration: 1.774s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.837265, mae: 19.867294, mean_q: -29.391582\n","   4400/300000: episode: 22, duration: 1.720s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.989876, mae: 20.463123, mean_q: -30.305849\n","   4600/300000: episode: 23, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 1.908907, mae: 21.029034, mean_q: -31.140453\n","   4800/300000: episode: 24, duration: 1.743s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 2.389730, mae: 21.683111, mean_q: -32.070892\n","   5000/300000: episode: 25, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 1.782163, mae: 22.243183, mean_q: -32.960625\n","   5200/300000: episode: 26, duration: 1.772s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 2.866673, mae: 22.697485, mean_q: -33.556313\n","   5400/300000: episode: 27, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 3.559360, mae: 23.203884, mean_q: -34.288475\n","   5600/300000: episode: 28, duration: 1.738s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 3.192213, mae: 23.669876, mean_q: -34.983837\n","   5800/300000: episode: 29, duration: 1.742s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 3.972903, mae: 24.086460, mean_q: -35.595165\n","   6000/300000: episode: 30, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 3.114533, mae: 24.509506, mean_q: -36.208858\n","   6200/300000: episode: 31, duration: 1.765s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 2.642067, mae: 24.919790, mean_q: -36.905396\n","   6400/300000: episode: 32, duration: 1.749s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 2.992563, mae: 25.309620, mean_q: -37.504208\n","   6600/300000: episode: 33, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 2.681443, mae: 25.726780, mean_q: -38.097530\n","   6800/300000: episode: 34, duration: 1.778s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 3.703570, mae: 26.170305, mean_q: -38.740978\n","   7000/300000: episode: 35, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 3.958617, mae: 26.490578, mean_q: -39.143692\n","   7200/300000: episode: 36, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 3.717958, mae: 26.870520, mean_q: -39.775810\n","   7400/300000: episode: 37, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 4.555075, mae: 27.174038, mean_q: -40.095421\n","   7600/300000: episode: 38, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 3.369875, mae: 27.469887, mean_q: -40.686153\n","   7800/300000: episode: 39, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 3.332545, mae: 27.860239, mean_q: -41.239853\n","   8000/300000: episode: 40, duration: 1.765s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 2.834186, mae: 28.227886, mean_q: -41.846523\n","   8200/300000: episode: 41, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 3.992585, mae: 28.476337, mean_q: -42.098499\n","   8400/300000: episode: 42, duration: 1.779s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000],  loss: 4.099816, mae: 28.633482, mean_q: -42.314854\n","   8600/300000: episode: 43, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 5.445627, mae: 28.781235, mean_q: -42.560501\n","   8800/300000: episode: 44, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 4.887452, mae: 28.962774, mean_q: -42.774708\n","   9000/300000: episode: 45, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 5.844918, mae: 29.111460, mean_q: -42.928478\n","   9200/300000: episode: 46, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 5.109451, mae: 29.126972, mean_q: -42.925995\n","   9400/300000: episode: 47, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 4.594862, mae: 29.230686, mean_q: -43.150986\n","   9600/300000: episode: 48, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 4.761139, mae: 29.262007, mean_q: -43.244434\n","   9800/300000: episode: 49, duration: 1.768s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 4.513237, mae: 29.424444, mean_q: -43.462540\n","  10000/300000: episode: 50, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 4.494089, mae: 29.601206, mean_q: -43.727276\n","  10200/300000: episode: 51, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 4.566580, mae: 29.671547, mean_q: -43.817711\n","  10400/300000: episode: 52, duration: 1.785s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 6.860992, mae: 29.603565, mean_q: -43.660011\n","  10600/300000: episode: 53, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 5.616891, mae: 29.697983, mean_q: -43.859962\n","  10800/300000: episode: 54, duration: 1.762s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 5.110516, mae: 29.743120, mean_q: -43.839481\n","  11000/300000: episode: 55, duration: 1.753s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 3.714286, mae: 29.768700, mean_q: -44.063305\n","  11200/300000: episode: 56, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 4.349556, mae: 29.913347, mean_q: -44.164459\n","  11400/300000: episode: 57, duration: 1.769s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 4.005703, mae: 29.947542, mean_q: -44.262325\n","  11600/300000: episode: 58, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 4.657746, mae: 29.940395, mean_q: -44.144169\n","  11800/300000: episode: 59, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 5.918174, mae: 29.933664, mean_q: -44.182205\n","  12000/300000: episode: 60, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000],  loss: 4.854387, mae: 29.945341, mean_q: -44.154556\n","  12200/300000: episode: 61, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 4.661019, mae: 30.041513, mean_q: -44.392990\n","  12400/300000: episode: 62, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 4.705888, mae: 30.164898, mean_q: -44.648232\n","  12600/300000: episode: 63, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 6.856225, mae: 30.244715, mean_q: -44.509998\n","  12800/300000: episode: 64, duration: 1.765s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 4.292300, mae: 30.297884, mean_q: -44.760082\n","  13000/300000: episode: 65, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 4.811931, mae: 30.409767, mean_q: -44.917671\n","  13200/300000: episode: 66, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 5.866769, mae: 30.407656, mean_q: -44.849873\n","  13400/300000: episode: 67, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000],  loss: 5.211637, mae: 30.397812, mean_q: -44.909233\n","  13600/300000: episode: 68, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 5.070066, mae: 30.455547, mean_q: -45.035488\n","  13800/300000: episode: 69, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 4.894166, mae: 30.610506, mean_q: -45.183556\n","  14000/300000: episode: 70, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 5.385455, mae: 30.640781, mean_q: -45.303852\n","  14200/300000: episode: 71, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 5.466638, mae: 30.752327, mean_q: -45.481659\n","  14400/300000: episode: 72, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 5.399168, mae: 30.948845, mean_q: -45.826157\n","  14600/300000: episode: 73, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 4.232742, mae: 31.107065, mean_q: -46.105785\n","  14800/300000: episode: 74, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 6.092534, mae: 31.265600, mean_q: -46.244492\n","  15000/300000: episode: 75, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 5.309500, mae: 31.314137, mean_q: -46.357841\n","  15200/300000: episode: 76, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 5.078497, mae: 31.497648, mean_q: -46.558876\n","  15400/300000: episode: 77, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 4.985877, mae: 31.583992, mean_q: -46.755238\n","  15600/300000: episode: 78, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 6.318185, mae: 31.715454, mean_q: -46.913345\n","  15800/300000: episode: 79, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 3.965790, mae: 31.933510, mean_q: -47.398537\n","  16000/300000: episode: 80, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 5.869472, mae: 32.144562, mean_q: -47.520927\n","  16200/300000: episode: 81, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 6.176343, mae: 32.303677, mean_q: -47.743675\n","  16400/300000: episode: 82, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 4.672455, mae: 32.480957, mean_q: -48.196358\n","  16600/300000: episode: 83, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000],  loss: 8.150910, mae: 32.531029, mean_q: -47.907791\n","  16800/300000: episode: 84, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 5.276635, mae: 32.554039, mean_q: -48.224960\n","  17000/300000: episode: 85, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 5.450881, mae: 32.761650, mean_q: -48.500904\n","  17200/300000: episode: 86, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.258914, mae: 32.830826, mean_q: -48.571571\n","  17400/300000: episode: 87, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 7.482133, mae: 32.931061, mean_q: -48.600903\n","  17600/300000: episode: 88, duration: 1.771s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.560076, mae: 33.138927, mean_q: -49.132812\n","  17800/300000: episode: 89, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 6.288673, mae: 33.305054, mean_q: -49.361607\n","  18000/300000: episode: 90, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 6.152062, mae: 33.486729, mean_q: -49.565575\n","  18200/300000: episode: 91, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.435684, mae: 33.582062, mean_q: -49.697803\n","  18400/300000: episode: 92, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 7.144780, mae: 33.708630, mean_q: -49.768356\n","  18600/300000: episode: 93, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 5.943963, mae: 33.835716, mean_q: -50.116211\n","  18800/300000: episode: 94, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.763263, mae: 33.974087, mean_q: -50.321781\n","  19000/300000: episode: 95, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 6.253801, mae: 34.187347, mean_q: -50.692883\n","  19200/300000: episode: 96, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 6.327957, mae: 34.370148, mean_q: -50.921474\n","  19400/300000: episode: 97, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 6.827275, mae: 34.635773, mean_q: -51.243706\n","  19600/300000: episode: 98, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.195881, mae: 34.837906, mean_q: -51.654659\n","  19800/300000: episode: 99, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 5.643075, mae: 35.107052, mean_q: -52.046398\n","  20000/300000: episode: 100, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.447067, mae: 35.228912, mean_q: -52.179932\n","  20200/300000: episode: 101, duration: 1.812s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 7.332738, mae: 35.317982, mean_q: -52.318726\n","  20400/300000: episode: 102, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 6.175009, mae: 35.589497, mean_q: -52.725388\n","  20600/300000: episode: 103, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.131653, mae: 35.725616, mean_q: -52.846634\n","  20800/300000: episode: 104, duration: 1.878s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 8.181190, mae: 35.689156, mean_q: -52.798077\n","  21000/300000: episode: 105, duration: 1.982s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.822047, mae: 35.921104, mean_q: -53.290916\n","  21200/300000: episode: 106, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 6.592344, mae: 36.044456, mean_q: -53.398071\n","  21400/300000: episode: 107, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.328098, mae: 36.200771, mean_q: -53.525223\n","  21600/300000: episode: 108, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 7.871654, mae: 36.182362, mean_q: -53.650097\n","  21800/300000: episode: 109, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.270979, mae: 36.285297, mean_q: -53.839188\n","  22000/300000: episode: 110, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.938586, mae: 36.417000, mean_q: -53.966373\n","  22200/300000: episode: 111, duration: 1.902s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.739864, mae: 36.587379, mean_q: -54.170670\n","  22400/300000: episode: 112, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 6.674162, mae: 36.747841, mean_q: -54.559399\n","  22600/300000: episode: 113, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 5.713792, mae: 37.011471, mean_q: -54.942413\n","  22800/300000: episode: 114, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.857960, mae: 37.121487, mean_q: -54.971153\n","  23000/300000: episode: 115, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.773826, mae: 37.350330, mean_q: -55.464298\n","  23200/300000: episode: 116, duration: 1.977s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.364779, mae: 37.432056, mean_q: -55.502220\n","  23400/300000: episode: 117, duration: 1.943s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.144044, mae: 37.514481, mean_q: -55.386288\n","  23600/300000: episode: 118, duration: 1.982s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 10.370859, mae: 37.484924, mean_q: -55.439049\n","  23800/300000: episode: 119, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 5.097582, mae: 37.635971, mean_q: -55.965652\n","  24000/300000: episode: 120, duration: 1.919s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 9.267508, mae: 37.749630, mean_q: -55.860653\n","  24200/300000: episode: 121, duration: 1.943s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 5.262146, mae: 37.918011, mean_q: -56.343212\n","  24400/300000: episode: 122, duration: 1.967s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.264752, mae: 37.971210, mean_q: -56.196388\n","  24600/300000: episode: 123, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.780136, mae: 38.084980, mean_q: -56.442062\n","  24800/300000: episode: 124, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 11.030903, mae: 38.096256, mean_q: -56.305614\n","  25000/300000: episode: 125, duration: 1.958s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 6.127681, mae: 38.076103, mean_q: -56.559933\n","  25200/300000: episode: 126, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.447910, mae: 38.259712, mean_q: -56.769550\n","  25400/300000: episode: 127, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.516158, mae: 38.385151, mean_q: -56.868446\n","  25600/300000: episode: 128, duration: 1.985s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.058653, mae: 38.429920, mean_q: -56.972588\n","  25800/300000: episode: 129, duration: 1.955s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 6.461547, mae: 38.608055, mean_q: -57.331352\n","  26000/300000: episode: 130, duration: 2.025s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.644878, mae: 38.725189, mean_q: -57.364117\n","  26200/300000: episode: 131, duration: 1.992s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.799575, mae: 38.756489, mean_q: -57.423534\n","  26400/300000: episode: 132, duration: 1.916s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.108583, mae: 38.873631, mean_q: -57.633759\n","  26600/300000: episode: 133, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 6.169441, mae: 38.976463, mean_q: -57.834179\n","  26800/300000: episode: 134, duration: 1.941s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.432178, mae: 39.092205, mean_q: -57.988110\n","  27000/300000: episode: 135, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.820481, mae: 39.192574, mean_q: -58.099361\n","  27200/300000: episode: 136, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.691144, mae: 39.183556, mean_q: -58.018681\n","  27400/300000: episode: 137, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 5.814900, mae: 39.328922, mean_q: -58.415112\n","  27600/300000: episode: 138, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 6.707408, mae: 39.493477, mean_q: -58.733299\n","  27800/300000: episode: 139, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.609445, mae: 39.605606, mean_q: -58.819950\n","  28000/300000: episode: 140, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 11.152185, mae: 39.631371, mean_q: -58.608673\n","  28200/300000: episode: 141, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 9.094947, mae: 39.553375, mean_q: -58.630177\n","  28400/300000: episode: 142, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 10.282166, mae: 39.636833, mean_q: -58.555000\n","  28600/300000: episode: 143, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 6.765725, mae: 39.563709, mean_q: -58.750267\n","  28800/300000: episode: 144, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 9.356046, mae: 39.631737, mean_q: -58.686493\n","  29000/300000: episode: 145, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.437915, mae: 39.527176, mean_q: -58.548954\n","  29200/300000: episode: 146, duration: 1.933s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 8.807645, mae: 39.448952, mean_q: -58.468384\n","  29400/300000: episode: 147, duration: 1.943s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.048858, mae: 39.415558, mean_q: -58.405731\n","  29600/300000: episode: 148, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 10.648643, mae: 39.478592, mean_q: -58.437767\n","  29800/300000: episode: 149, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.280259, mae: 39.426746, mean_q: -58.354874\n","  30000/300000: episode: 150, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 9.605408, mae: 39.430584, mean_q: -58.465050\n","  30200/300000: episode: 151, duration: 1.932s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.899767, mae: 39.417904, mean_q: -58.524036\n","  30400/300000: episode: 152, duration: 1.963s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 7.809901, mae: 39.497398, mean_q: -58.658867\n","  30600/300000: episode: 153, duration: 2.000s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.997928, mae: 39.596764, mean_q: -58.790131\n","  30800/300000: episode: 154, duration: 1.968s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.185982, mae: 39.617317, mean_q: -58.689388\n","  31000/300000: episode: 155, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 8.336852, mae: 39.672050, mean_q: -58.838100\n","  31200/300000: episode: 156, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 8.910610, mae: 39.650108, mean_q: -58.788486\n","  31400/300000: episode: 157, duration: 1.965s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.025317, mae: 39.737831, mean_q: -58.932461\n","  31600/300000: episode: 158, duration: 1.951s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.533314, mae: 39.833813, mean_q: -59.146103\n","  31800/300000: episode: 159, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.628424, mae: 39.964638, mean_q: -59.375038\n","  32000/300000: episode: 160, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.981666, mae: 40.094780, mean_q: -59.554722\n","  32200/300000: episode: 161, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.643554, mae: 40.145931, mean_q: -59.462978\n","  32400/300000: episode: 162, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 10.001132, mae: 40.178082, mean_q: -59.521259\n","  32600/300000: episode: 163, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 8.879895, mae: 40.050262, mean_q: -59.408051\n","  32800/300000: episode: 164, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 10.812693, mae: 40.051315, mean_q: -59.220070\n","  33000/300000: episode: 165, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 9.827568, mae: 40.042377, mean_q: -59.327274\n","  33200/300000: episode: 166, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.199961, mae: 40.124081, mean_q: -59.606300\n","  33400/300000: episode: 167, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.952768, mae: 40.190781, mean_q: -59.463322\n","  33600/300000: episode: 168, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.535348, mae: 40.010406, mean_q: -59.239658\n","  33800/300000: episode: 169, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.784833, mae: 40.157318, mean_q: -59.586235\n","  34000/300000: episode: 170, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 9.156358, mae: 40.118702, mean_q: -59.510155\n","  34200/300000: episode: 171, duration: 1.949s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.797834, mae: 40.162472, mean_q: -59.594959\n","  34400/300000: episode: 172, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 7.008768, mae: 40.343403, mean_q: -59.899975\n","  34600/300000: episode: 173, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 9.222106, mae: 40.312546, mean_q: -59.548271\n","  34800/300000: episode: 174, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 6.258365, mae: 40.402950, mean_q: -60.072865\n","  35000/300000: episode: 175, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.740030, mae: 40.503296, mean_q: -59.983223\n","  35200/300000: episode: 176, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.205523, mae: 40.469017, mean_q: -60.061455\n","  35400/300000: episode: 177, duration: 1.790s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.696795, mae: 40.641544, mean_q: -60.480946\n","  35600/300000: episode: 178, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 7.179645, mae: 40.760468, mean_q: -60.494385\n","  35800/300000: episode: 179, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.574136, mae: 40.979855, mean_q: -61.004639\n","  36000/300000: episode: 180, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 7.847353, mae: 40.942562, mean_q: -60.667068\n","  36200/300000: episode: 181, duration: 1.765s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.595248, mae: 40.985077, mean_q: -60.821224\n","  36400/300000: episode: 182, duration: 1.782s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.032810, mae: 40.965885, mean_q: -60.790539\n","  36600/300000: episode: 183, duration: 1.764s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.392006, mae: 41.033970, mean_q: -60.861446\n","  36800/300000: episode: 184, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 9.933537, mae: 41.015656, mean_q: -60.793022\n","  37000/300000: episode: 185, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.529955, mae: 41.045708, mean_q: -60.971363\n","  37200/300000: episode: 186, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 6.748610, mae: 41.099571, mean_q: -61.084354\n","  37400/300000: episode: 187, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.636731, mae: 41.301388, mean_q: -61.430473\n","  37600/300000: episode: 188, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 6.923806, mae: 41.468712, mean_q: -61.641945\n","  37800/300000: episode: 189, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 6.525781, mae: 41.458389, mean_q: -61.619705\n","  38000/300000: episode: 190, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 10.367529, mae: 41.524094, mean_q: -61.523285\n","  38200/300000: episode: 191, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 10.494906, mae: 41.496124, mean_q: -61.485645\n","  38400/300000: episode: 192, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.157092, mae: 41.477783, mean_q: -61.538437\n","  38600/300000: episode: 193, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 11.791976, mae: 41.401352, mean_q: -61.341476\n","  38800/300000: episode: 194, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.115025, mae: 41.309414, mean_q: -61.132587\n","  39000/300000: episode: 195, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.434180, mae: 41.322296, mean_q: -61.449574\n","  39200/300000: episode: 196, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.335794, mae: 41.435036, mean_q: -61.372337\n","  39400/300000: episode: 197, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 10.644383, mae: 41.335926, mean_q: -61.213699\n","  39600/300000: episode: 198, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.675272, mae: 41.314747, mean_q: -61.285038\n","  39800/300000: episode: 199, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.359309, mae: 41.465313, mean_q: -61.472549\n","  40000/300000: episode: 200, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 9.612038, mae: 41.313000, mean_q: -61.128117\n","  40200/300000: episode: 201, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 6.653000, mae: 41.295918, mean_q: -61.367062\n","  40400/300000: episode: 202, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.324450, mae: 41.373783, mean_q: -61.312500\n","  40600/300000: episode: 203, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 8.119084, mae: 41.417690, mean_q: -61.477795\n","  40800/300000: episode: 204, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 9.026641, mae: 41.385632, mean_q: -61.382931\n","  41000/300000: episode: 205, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 9.904654, mae: 41.236130, mean_q: -61.106503\n","  41200/300000: episode: 206, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 9.578085, mae: 41.161564, mean_q: -60.944473\n","  41400/300000: episode: 207, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.802429, mae: 41.272705, mean_q: -61.260075\n","  41600/300000: episode: 208, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.186852, mae: 41.207714, mean_q: -61.012032\n","  41800/300000: episode: 209, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 7.287469, mae: 41.225761, mean_q: -61.251884\n","  42000/300000: episode: 210, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.858997, mae: 41.287392, mean_q: -61.169884\n","  42200/300000: episode: 211, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.186937, mae: 41.355362, mean_q: -61.489971\n","  42400/300000: episode: 212, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.746570, mae: 41.405205, mean_q: -61.355804\n","  42600/300000: episode: 213, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 9.133540, mae: 41.420116, mean_q: -61.401695\n","  42800/300000: episode: 214, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 8.377834, mae: 41.368092, mean_q: -61.398937\n","  43000/300000: episode: 215, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 11.581568, mae: 41.307442, mean_q: -61.186886\n","  43200/300000: episode: 216, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.146991, mae: 41.205002, mean_q: -61.059525\n","  43400/300000: episode: 217, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 5.830050, mae: 41.349430, mean_q: -61.482403\n","  43600/300000: episode: 218, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 10.899208, mae: 41.399803, mean_q: -61.381905\n","  43800/300000: episode: 219, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.756025, mae: 41.441319, mean_q: -61.558815\n","  44000/300000: episode: 220, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 10.375957, mae: 41.534863, mean_q: -61.629532\n","  44200/300000: episode: 221, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 9.070919, mae: 41.457958, mean_q: -61.445591\n","  44400/300000: episode: 222, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.005223, mae: 41.532150, mean_q: -61.641212\n","  44600/300000: episode: 223, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 9.894047, mae: 41.601952, mean_q: -61.712746\n","  44800/300000: episode: 224, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.009945, mae: 41.576126, mean_q: -61.637833\n","  45000/300000: episode: 225, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 10.711538, mae: 41.570675, mean_q: -61.633232\n","  45200/300000: episode: 226, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.962597, mae: 41.490337, mean_q: -61.502701\n","  45400/300000: episode: 227, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.550663, mae: 41.655190, mean_q: -61.925587\n","  45600/300000: episode: 228, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.955430, mae: 41.670353, mean_q: -61.845196\n","  45800/300000: episode: 229, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.725 [0.000, 2.000],  loss: 8.554770, mae: 41.717167, mean_q: -61.871212\n","  46000/300000: episode: 230, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.185764, mae: 41.704193, mean_q: -61.939625\n","  46200/300000: episode: 231, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.283844, mae: 41.675480, mean_q: -61.872738\n","  46400/300000: episode: 232, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.026478, mae: 41.745766, mean_q: -61.918427\n","  46600/300000: episode: 233, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.653464, mae: 41.697994, mean_q: -61.775826\n","  46800/300000: episode: 234, duration: 1.878s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.863148, mae: 41.715439, mean_q: -61.891167\n","  47000/300000: episode: 235, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 9.309083, mae: 41.676670, mean_q: -61.629742\n","  47200/300000: episode: 236, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.449795, mae: 41.646141, mean_q: -61.775166\n","  47400/300000: episode: 237, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.289303, mae: 41.690079, mean_q: -61.904152\n","  47600/300000: episode: 238, duration: 1.914s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 12.769218, mae: 41.561180, mean_q: -61.489239\n","  47800/300000: episode: 239, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 11.353661, mae: 41.420292, mean_q: -61.357689\n","  48000/300000: episode: 240, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 10.411489, mae: 41.308212, mean_q: -61.118984\n","  48200/300000: episode: 241, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.722757, mae: 41.261475, mean_q: -61.355900\n","  48400/300000: episode: 242, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.994624, mae: 41.360714, mean_q: -61.413830\n","  48600/300000: episode: 243, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.673022, mae: 41.427181, mean_q: -61.434677\n","  48800/300000: episode: 244, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 9.728783, mae: 41.390537, mean_q: -61.349102\n","  49000/300000: episode: 245, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 9.510328, mae: 41.385754, mean_q: -61.364132\n","  49200/300000: episode: 246, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 12.149620, mae: 41.341206, mean_q: -61.161015\n","  49400/300000: episode: 247, duration: 1.794s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 11.199942, mae: 41.137260, mean_q: -60.904247\n","  49600/300000: episode: 248, duration: 1.785s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 7.747886, mae: 41.165810, mean_q: -61.148087\n","  49800/300000: episode: 249, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 10.967485, mae: 41.105274, mean_q: -60.874359\n","  50000/300000: episode: 250, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 9.330088, mae: 40.907330, mean_q: -60.633301\n","  50200/300000: episode: 251, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.812729, mae: 40.801403, mean_q: -60.516083\n","  50400/300000: episode: 252, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 7.664176, mae: 40.808342, mean_q: -60.602070\n","  50600/300000: episode: 253, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.466260, mae: 40.852249, mean_q: -60.542850\n","  50800/300000: episode: 254, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.187006, mae: 40.877964, mean_q: -60.693348\n","  51000/300000: episode: 255, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.275072, mae: 40.944447, mean_q: -60.614464\n","  51200/300000: episode: 256, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.012997, mae: 40.914066, mean_q: -60.616505\n","  51400/300000: episode: 257, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.922165, mae: 40.937710, mean_q: -60.735031\n","  51600/300000: episode: 258, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 6.412889, mae: 40.999004, mean_q: -60.902714\n","  51800/300000: episode: 259, duration: 1.878s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.361311, mae: 41.131451, mean_q: -61.077404\n","  52000/300000: episode: 260, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 11.525241, mae: 40.996494, mean_q: -60.613838\n","  52200/300000: episode: 261, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 10.843557, mae: 40.958046, mean_q: -60.693634\n","  52400/300000: episode: 262, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 10.146036, mae: 40.925701, mean_q: -60.644325\n","  52600/300000: episode: 263, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.634259, mae: 40.796513, mean_q: -60.531147\n","  52800/300000: episode: 264, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 12.111675, mae: 40.699757, mean_q: -60.205616\n","  53000/300000: episode: 265, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 10.496726, mae: 40.601486, mean_q: -60.125118\n","  53200/300000: episode: 266, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.352065, mae: 40.466751, mean_q: -59.852448\n","  53400/300000: episode: 267, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.684179, mae: 40.326050, mean_q: -59.735321\n","  53600/300000: episode: 268, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.692273, mae: 40.310375, mean_q: -59.752872\n","  53800/300000: episode: 269, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 12.609241, mae: 40.145939, mean_q: -59.374344\n","  54000/300000: episode: 270, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.371105, mae: 40.084984, mean_q: -59.544033\n","  54200/300000: episode: 271, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 7.644947, mae: 40.150711, mean_q: -59.552490\n","  54400/300000: episode: 272, duration: 1.918s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 7.119797, mae: 40.262188, mean_q: -59.852108\n","  54600/300000: episode: 273, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000],  loss: 6.898477, mae: 40.300541, mean_q: -59.809128\n","  54800/300000: episode: 274, duration: 1.974s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 9.993848, mae: 40.251602, mean_q: -59.554386\n","  55000/300000: episode: 275, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 7.907213, mae: 40.077915, mean_q: -59.568497\n","  55200/300000: episode: 276, duration: 1.896s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 7.851820, mae: 39.984943, mean_q: -59.270721\n","  55400/300000: episode: 277, duration: 1.970s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.219714, mae: 39.967648, mean_q: -59.438267\n","  55600/300000: episode: 278, duration: 1.968s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 11.194492, mae: 40.048592, mean_q: -59.233311\n","  55800/300000: episode: 279, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.534737, mae: 40.056961, mean_q: -59.354691\n","  56000/300000: episode: 280, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 7.318063, mae: 40.119511, mean_q: -59.532612\n","  56200/300000: episode: 281, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 11.254005, mae: 40.080479, mean_q: -59.346649\n","  56400/300000: episode: 282, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.859455, mae: 40.073467, mean_q: -59.409359\n","  56600/300000: episode: 283, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.002769, mae: 39.942974, mean_q: -59.232010\n","  56800/300000: episode: 284, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.701393, mae: 39.958664, mean_q: -59.236759\n","  57000/300000: episode: 285, duration: 1.902s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.344419, mae: 40.063953, mean_q: -59.465469\n","  57200/300000: episode: 286, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 6.762600, mae: 40.155834, mean_q: -59.642532\n","  57400/300000: episode: 287, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 10.017533, mae: 40.024609, mean_q: -59.261768\n","  57600/300000: episode: 288, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.780913, mae: 39.897900, mean_q: -59.087959\n","  57800/300000: episode: 289, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 8.309251, mae: 39.981728, mean_q: -59.328175\n","  58000/300000: episode: 290, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 9.148311, mae: 39.885677, mean_q: -59.090786\n","  58200/300000: episode: 291, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.905493, mae: 40.018482, mean_q: -59.552330\n","  58400/300000: episode: 292, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.470038, mae: 40.160637, mean_q: -59.641186\n","  58600/300000: episode: 293, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 9.738404, mae: 40.157207, mean_q: -59.499580\n","  58800/300000: episode: 294, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.027933, mae: 40.188961, mean_q: -59.605789\n","  59000/300000: episode: 295, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 7.658825, mae: 40.217308, mean_q: -59.697296\n","  59200/300000: episode: 296, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.529057, mae: 40.303928, mean_q: -59.708599\n","  59400/300000: episode: 297, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.883895, mae: 40.331825, mean_q: -59.674786\n","  59600/300000: episode: 298, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.061338, mae: 40.161995, mean_q: -59.498722\n","  59800/300000: episode: 299, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 6.784576, mae: 40.151962, mean_q: -59.641434\n","  60000/300000: episode: 300, duration: 1.933s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 8.534645, mae: 40.099319, mean_q: -59.432686\n","  60200/300000: episode: 301, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.434533, mae: 39.970604, mean_q: -59.185947\n","  60400/300000: episode: 302, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 10.187569, mae: 39.940651, mean_q: -59.089989\n","  60600/300000: episode: 303, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.824297, mae: 39.851273, mean_q: -59.108788\n","  60800/300000: episode: 304, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.729511, mae: 39.880707, mean_q: -59.046070\n","  61000/300000: episode: 305, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 6.890975, mae: 39.834850, mean_q: -59.150867\n","  61200/300000: episode: 306, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 10.598707, mae: 39.715488, mean_q: -58.839912\n","  61400/300000: episode: 307, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.269042, mae: 39.824181, mean_q: -59.086582\n","  61600/300000: episode: 308, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.035286, mae: 39.892399, mean_q: -59.165249\n","  61800/300000: episode: 309, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.824671, mae: 39.883671, mean_q: -59.112568\n","  62000/300000: episode: 310, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 9.349335, mae: 39.881771, mean_q: -59.202805\n","  62200/300000: episode: 311, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.836948, mae: 39.940475, mean_q: -59.180008\n","  62400/300000: episode: 312, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 9.442133, mae: 39.927971, mean_q: -59.029911\n","  62600/300000: episode: 313, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.452959, mae: 39.816967, mean_q: -59.020451\n","  62800/300000: episode: 314, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 7.821033, mae: 39.688477, mean_q: -58.848629\n","  63000/300000: episode: 315, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 8.970778, mae: 39.525196, mean_q: -58.443310\n","  63200/300000: episode: 316, duration: 1.878s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.658445, mae: 39.351662, mean_q: -58.320057\n","  63400/300000: episode: 317, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 11.484295, mae: 39.216625, mean_q: -57.965679\n","  63600/300000: episode: 318, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.528621, mae: 39.175262, mean_q: -58.086788\n","  63800/300000: episode: 319, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.111461, mae: 39.336697, mean_q: -58.272148\n","  64000/300000: episode: 320, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 7.337550, mae: 39.350380, mean_q: -58.408890\n","  64200/300000: episode: 321, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 7.588151, mae: 39.368771, mean_q: -58.331284\n","  64400/300000: episode: 322, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.912820, mae: 39.424511, mean_q: -58.513008\n","  64600/300000: episode: 323, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 8.900179, mae: 39.519562, mean_q: -58.555771\n","  64800/300000: episode: 324, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 10.007522, mae: 39.434040, mean_q: -58.379288\n","  65000/300000: episode: 325, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 7.540022, mae: 39.537361, mean_q: -58.664829\n","  65200/300000: episode: 326, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 9.467727, mae: 39.424427, mean_q: -58.423729\n","  65400/300000: episode: 327, duration: 1.954s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.240510, mae: 39.458012, mean_q: -58.495018\n","  65600/300000: episode: 328, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 7.522339, mae: 39.599415, mean_q: -58.764515\n","  65800/300000: episode: 329, duration: 1.955s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.097721, mae: 39.708858, mean_q: -58.919102\n","  66000/300000: episode: 330, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 11.707554, mae: 39.671539, mean_q: -58.726151\n","  66200/300000: episode: 331, duration: 1.935s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 5.284385, mae: 39.777374, mean_q: -59.135464\n","  66400/300000: episode: 332, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 7.426252, mae: 39.941280, mean_q: -59.180088\n","  66600/300000: episode: 333, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.277230, mae: 39.883137, mean_q: -59.031212\n","  66800/300000: episode: 334, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.286868, mae: 39.955860, mean_q: -59.248837\n","  67000/300000: episode: 335, duration: 1.933s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.542369, mae: 40.010464, mean_q: -59.335709\n","  67200/300000: episode: 336, duration: 1.935s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.587748, mae: 40.036915, mean_q: -59.303288\n","  67400/300000: episode: 337, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 6.581873, mae: 40.003277, mean_q: -59.475266\n","  67600/300000: episode: 338, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 7.228842, mae: 40.126667, mean_q: -59.485096\n","  67800/300000: episode: 339, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 10.325001, mae: 40.078629, mean_q: -59.298477\n","  68000/300000: episode: 340, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 7.723984, mae: 40.044331, mean_q: -59.338436\n","  68200/300000: episode: 341, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.234851, mae: 40.054203, mean_q: -59.366241\n","  68400/300000: episode: 342, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.979809, mae: 40.045517, mean_q: -59.379406\n","  68600/300000: episode: 343, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 6.631635, mae: 40.101925, mean_q: -59.580742\n","  68800/300000: episode: 344, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.299191, mae: 40.219254, mean_q: -59.548630\n","  69000/300000: episode: 345, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.744277, mae: 40.255585, mean_q: -59.730087\n","  69200/300000: episode: 346, duration: 1.790s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.266565, mae: 40.311832, mean_q: -59.806519\n","  69400/300000: episode: 347, duration: 1.753s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 9.551286, mae: 40.281216, mean_q: -59.711048\n","  69600/300000: episode: 348, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.606638, mae: 40.299015, mean_q: -59.747032\n","  69800/300000: episode: 349, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 6.488671, mae: 40.230515, mean_q: -59.586407\n","  70000/300000: episode: 350, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 9.395339, mae: 40.266861, mean_q: -59.722340\n","  70200/300000: episode: 351, duration: 1.768s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.840839, mae: 40.371098, mean_q: -59.859386\n","  70400/300000: episode: 352, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.924019, mae: 40.319382, mean_q: -59.730663\n","  70600/300000: episode: 353, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 9.395685, mae: 40.242210, mean_q: -59.602936\n","  70800/300000: episode: 354, duration: 1.798s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 8.395341, mae: 40.197105, mean_q: -59.661640\n","  71000/300000: episode: 355, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.701804, mae: 40.220058, mean_q: -59.648117\n","  71200/300000: episode: 356, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.665508, mae: 40.213638, mean_q: -59.563202\n","  71400/300000: episode: 357, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.914792, mae: 40.228676, mean_q: -59.667778\n","  71600/300000: episode: 358, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 10.028242, mae: 40.236961, mean_q: -59.693554\n","  71800/300000: episode: 359, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 6.823914, mae: 40.353012, mean_q: -59.973839\n","  72000/300000: episode: 360, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.671750, mae: 40.483833, mean_q: -60.057541\n","  72200/300000: episode: 361, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.972258, mae: 40.409462, mean_q: -59.938866\n","  72400/300000: episode: 362, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 7.213215, mae: 40.561459, mean_q: -60.199951\n","  72600/300000: episode: 363, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.998112, mae: 40.504898, mean_q: -60.099716\n","  72800/300000: episode: 364, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.046222, mae: 40.635452, mean_q: -60.255409\n","  73000/300000: episode: 365, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.264582, mae: 40.741894, mean_q: -60.450058\n","  73200/300000: episode: 366, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 5.840362, mae: 40.761490, mean_q: -60.559132\n","  73400/300000: episode: 367, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.073586, mae: 40.900021, mean_q: -60.515064\n","  73600/300000: episode: 368, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 10.043972, mae: 40.961109, mean_q: -60.611828\n","  73800/300000: episode: 369, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 11.301536, mae: 40.678032, mean_q: -60.208832\n","  74000/300000: episode: 370, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 10.413998, mae: 40.554085, mean_q: -60.021446\n","  74200/300000: episode: 371, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.336399, mae: 40.531265, mean_q: -60.065556\n","  74400/300000: episode: 372, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.436514, mae: 40.526203, mean_q: -60.154575\n","  74600/300000: episode: 373, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 10.349340, mae: 40.594608, mean_q: -60.134125\n","  74800/300000: episode: 374, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.820437, mae: 40.510181, mean_q: -60.077625\n","  75000/300000: episode: 375, duration: 1.896s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.502389, mae: 40.463478, mean_q: -59.893898\n","  75200/300000: episode: 376, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.153061, mae: 40.472271, mean_q: -60.008236\n","  75400/300000: episode: 377, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.242315, mae: 40.519581, mean_q: -60.082664\n","  75600/300000: episode: 378, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 10.602143, mae: 40.518562, mean_q: -60.020454\n","  75800/300000: episode: 379, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 8.870825, mae: 40.499893, mean_q: -60.030975\n","  76000/300000: episode: 380, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 8.699317, mae: 40.584686, mean_q: -60.209755\n","  76200/300000: episode: 381, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.740060, mae: 40.651363, mean_q: -60.308517\n","  76400/300000: episode: 382, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 10.896697, mae: 40.563889, mean_q: -60.017052\n","  76600/300000: episode: 383, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.719835, mae: 40.566845, mean_q: -60.172436\n","  76800/300000: episode: 384, duration: 1.944s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.030644, mae: 40.486256, mean_q: -59.964264\n","  77000/300000: episode: 385, duration: 1.988s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.928403, mae: 40.605595, mean_q: -60.357002\n","  77200/300000: episode: 386, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.906250, mae: 40.632015, mean_q: -60.277153\n","  77400/300000: episode: 387, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.281853, mae: 40.703262, mean_q: -60.341896\n","  77600/300000: episode: 388, duration: 1.980s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.101904, mae: 40.773823, mean_q: -60.533913\n","  77800/300000: episode: 389, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 6.269679, mae: 40.905010, mean_q: -60.784718\n","  78000/300000: episode: 390, duration: 1.958s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.709826, mae: 41.009579, mean_q: -60.745029\n","  78200/300000: episode: 391, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.847674, mae: 41.068516, mean_q: -60.888058\n","  78400/300000: episode: 392, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.818748, mae: 41.100605, mean_q: -61.008408\n","  78600/300000: episode: 393, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.681914, mae: 41.210373, mean_q: -61.066296\n","  78800/300000: episode: 394, duration: 1.965s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 12.999759, mae: 41.034546, mean_q: -60.631683\n","  79000/300000: episode: 395, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.066527, mae: 41.043846, mean_q: -60.879639\n","  79200/300000: episode: 396, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 10.557566, mae: 40.912071, mean_q: -60.527725\n","  79400/300000: episode: 397, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 10.586329, mae: 40.820560, mean_q: -60.384464\n","  79600/300000: episode: 398, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 8.762463, mae: 40.801434, mean_q: -60.467304\n","  79800/300000: episode: 399, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 9.661929, mae: 40.709293, mean_q: -60.306339\n","  80000/300000: episode: 400, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 6.290881, mae: 40.847267, mean_q: -60.675323\n","  80200/300000: episode: 401, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.991601, mae: 40.908051, mean_q: -60.701134\n","  80400/300000: episode: 402, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.498799, mae: 41.058525, mean_q: -60.990570\n","  80600/300000: episode: 403, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 7.104067, mae: 41.080288, mean_q: -60.913071\n","  80800/300000: episode: 404, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.548864, mae: 40.899197, mean_q: -60.612381\n","  81000/300000: episode: 405, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 7.347708, mae: 41.049072, mean_q: -60.920692\n","  81200/300000: episode: 406, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.633579, mae: 41.053417, mean_q: -60.761494\n","  81400/300000: episode: 407, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 10.205401, mae: 41.047020, mean_q: -60.744267\n","  81600/300000: episode: 408, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 12.063284, mae: 41.003193, mean_q: -60.717682\n","  81800/300000: episode: 409, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 8.625050, mae: 40.925140, mean_q: -60.669170\n","  82000/300000: episode: 410, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 8.632419, mae: 40.846718, mean_q: -60.614647\n","  82200/300000: episode: 411, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.695739, mae: 40.861061, mean_q: -60.591915\n","  82400/300000: episode: 412, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 10.153774, mae: 40.709843, mean_q: -60.205898\n","  82600/300000: episode: 413, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 9.856532, mae: 40.386387, mean_q: -59.792885\n","  82800/300000: episode: 414, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.391555, mae: 40.266296, mean_q: -59.668716\n","  83000/300000: episode: 415, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 8.689719, mae: 40.422985, mean_q: -59.915848\n","  83200/300000: episode: 416, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.504059, mae: 40.397240, mean_q: -59.946789\n","  83400/300000: episode: 417, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.387860, mae: 40.442364, mean_q: -59.956734\n","  83600/300000: episode: 418, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 9.535574, mae: 40.420547, mean_q: -59.870586\n","  83800/300000: episode: 419, duration: 1.778s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 6.804247, mae: 40.565418, mean_q: -60.159237\n","  84000/300000: episode: 420, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.445599, mae: 40.436413, mean_q: -59.887463\n","  84200/300000: episode: 421, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 8.516607, mae: 40.409794, mean_q: -59.914867\n","  84400/300000: episode: 422, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.360505, mae: 40.455910, mean_q: -59.925293\n","  84600/300000: episode: 423, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 11.069900, mae: 40.327457, mean_q: -59.603466\n","  84800/300000: episode: 424, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.575742, mae: 40.235439, mean_q: -59.625641\n","  85000/300000: episode: 425, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 9.620829, mae: 40.296734, mean_q: -59.654911\n","  85200/300000: episode: 426, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 5.870615, mae: 40.460594, mean_q: -60.116379\n","  85400/300000: episode: 427, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 6.222085, mae: 40.524544, mean_q: -60.135616\n","  85600/300000: episode: 428, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.493675, mae: 40.585850, mean_q: -60.115448\n","  85800/300000: episode: 429, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 12.370022, mae: 40.504234, mean_q: -59.760902\n","  86000/300000: episode: 430, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 9.231235, mae: 40.396652, mean_q: -59.820293\n","  86200/300000: episode: 431, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.832785, mae: 40.441723, mean_q: -59.898540\n","  86400/300000: episode: 432, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 11.422890, mae: 40.335106, mean_q: -59.638107\n","  86600/300000: episode: 433, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.611872, mae: 40.218979, mean_q: -59.554611\n","  86800/300000: episode: 434, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 7.908472, mae: 40.194099, mean_q: -59.570187\n","  87000/300000: episode: 435, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 5.639695, mae: 40.243145, mean_q: -59.731308\n","  87200/300000: episode: 436, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 7.080898, mae: 40.414879, mean_q: -59.921738\n","  87400/300000: episode: 437, duration: 1.896s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.420889, mae: 40.420937, mean_q: -59.877697\n","  87600/300000: episode: 438, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 8.019342, mae: 40.381424, mean_q: -59.880192\n","  87800/300000: episode: 439, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 11.669348, mae: 40.330177, mean_q: -59.569698\n","  88000/300000: episode: 440, duration: 2.017s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.834513, mae: 40.218231, mean_q: -59.597198\n","  88200/300000: episode: 441, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 10.618590, mae: 40.237457, mean_q: -59.582752\n","  88400/300000: episode: 442, duration: 2.006s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.183942, mae: 40.176250, mean_q: -59.575741\n","  88600/300000: episode: 443, duration: 2.020s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.519479, mae: 40.154484, mean_q: -59.508270\n","  88800/300000: episode: 444, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.493689, mae: 40.088024, mean_q: -59.440155\n","  89000/300000: episode: 445, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 5.138151, mae: 40.316307, mean_q: -59.907181\n","  89200/300000: episode: 446, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 7.911062, mae: 40.325916, mean_q: -59.847099\n","  89400/300000: episode: 447, duration: 1.984s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.619951, mae: 40.448521, mean_q: -59.971481\n","  89600/300000: episode: 448, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 8.929222, mae: 40.439938, mean_q: -59.994747\n","  89800/300000: episode: 449, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 10.027600, mae: 40.476845, mean_q: -60.010380\n","  90000/300000: episode: 450, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 8.248497, mae: 40.398460, mean_q: -59.902050\n","  90200/300000: episode: 451, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.777290, mae: 40.500870, mean_q: -60.028008\n","  90400/300000: episode: 452, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 9.666950, mae: 40.574776, mean_q: -60.134109\n","  90600/300000: episode: 453, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.157109, mae: 40.502022, mean_q: -59.978271\n","  90800/300000: episode: 454, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 9.004260, mae: 40.514427, mean_q: -59.995220\n","  91000/300000: episode: 455, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 8.307351, mae: 40.581524, mean_q: -60.135586\n","  91200/300000: episode: 456, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 7.948612, mae: 40.654964, mean_q: -60.325214\n","  91400/300000: episode: 457, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 11.120335, mae: 40.598923, mean_q: -60.025558\n","  91600/300000: episode: 458, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.624613, mae: 40.504440, mean_q: -60.087521\n","  91800/300000: episode: 459, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 9.091652, mae: 40.506584, mean_q: -60.025948\n","  92000/300000: episode: 460, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 12.136469, mae: 40.406071, mean_q: -59.763233\n","  92200/300000: episode: 461, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.150259, mae: 40.376038, mean_q: -59.824688\n","  92400/300000: episode: 462, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 9.347785, mae: 40.360573, mean_q: -59.752125\n","  92600/300000: episode: 463, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 10.568625, mae: 40.291439, mean_q: -59.687061\n","  92800/300000: episode: 464, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.415606, mae: 40.280121, mean_q: -59.742176\n","  93000/300000: episode: 465, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.368969, mae: 40.261330, mean_q: -59.680576\n","  93200/300000: episode: 466, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 7.406681, mae: 40.253990, mean_q: -59.768585\n","  93400/300000: episode: 467, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.282257, mae: 40.253029, mean_q: -59.715321\n","  93600/300000: episode: 468, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 5.953061, mae: 40.407455, mean_q: -59.994797\n","  93800/300000: episode: 469, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.507176, mae: 40.500256, mean_q: -60.135101\n","  94000/300000: episode: 470, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.876096, mae: 40.518326, mean_q: -60.014542\n","  94200/300000: episode: 471, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 9.403669, mae: 40.373096, mean_q: -59.780605\n","  94400/300000: episode: 472, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.596287, mae: 40.357979, mean_q: -59.879673\n","  94600/300000: episode: 473, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.901471, mae: 40.290840, mean_q: -59.641441\n","  94800/300000: episode: 474, duration: 1.806s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 7.900886, mae: 40.267200, mean_q: -59.672787\n","  95000/300000: episode: 475, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.403181, mae: 40.189106, mean_q: -59.560917\n","  95200/300000: episode: 476, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.312065, mae: 40.043125, mean_q: -59.314709\n","  95400/300000: episode: 477, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.660414, mae: 40.079227, mean_q: -59.418922\n","  95600/300000: episode: 478, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 10.493486, mae: 40.015553, mean_q: -59.266544\n","  95800/300000: episode: 479, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.333482, mae: 39.989662, mean_q: -59.325039\n","  96000/300000: episode: 480, duration: 1.916s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.133066, mae: 40.021652, mean_q: -59.414181\n","  96200/300000: episode: 481, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.207922, mae: 40.045216, mean_q: -59.318565\n","  96400/300000: episode: 482, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.957123, mae: 40.034031, mean_q: -59.382694\n","  96600/300000: episode: 483, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 9.539790, mae: 40.089287, mean_q: -59.305676\n","  96800/300000: episode: 484, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.479928, mae: 40.019497, mean_q: -59.328766\n","  97000/300000: episode: 485, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 9.346568, mae: 40.067154, mean_q: -59.338593\n","  97200/300000: episode: 486, duration: 1.916s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 7.840145, mae: 40.049393, mean_q: -59.371727\n","  97400/300000: episode: 487, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 6.805037, mae: 39.964447, mean_q: -59.357452\n","  97600/300000: episode: 488, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 7.791770, mae: 40.077675, mean_q: -59.436329\n","  97800/300000: episode: 489, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.028421, mae: 40.166752, mean_q: -59.542656\n","  98000/300000: episode: 490, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 8.940104, mae: 39.904724, mean_q: -59.130775\n","  98200/300000: episode: 491, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.847568, mae: 39.910538, mean_q: -59.098690\n","  98400/300000: episode: 492, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.116198, mae: 39.862392, mean_q: -59.128422\n","  98600/300000: episode: 493, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 7.063492, mae: 40.026432, mean_q: -59.306622\n","  98800/300000: episode: 494, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.646539, mae: 40.067909, mean_q: -59.402588\n","  99000/300000: episode: 495, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 9.905722, mae: 40.185650, mean_q: -59.568115\n","  99200/300000: episode: 496, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 6.154636, mae: 40.069901, mean_q: -59.491966\n","  99400/300000: episode: 497, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.674320, mae: 40.174488, mean_q: -59.532570\n","  99600/300000: episode: 498, duration: 1.962s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.945333, mae: 40.174515, mean_q: -59.561718\n","  99800/300000: episode: 499, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 6.781252, mae: 40.213364, mean_q: -59.595562\n"," 100000/300000: episode: 500, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.815254, mae: 40.116207, mean_q: -59.433617\n"," 100200/300000: episode: 501, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 6.794091, mae: 40.161816, mean_q: -59.605381\n"," 100400/300000: episode: 502, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.732035, mae: 40.151623, mean_q: -59.404354\n"," 100600/300000: episode: 503, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 7.903821, mae: 40.170361, mean_q: -59.552628\n"," 100800/300000: episode: 504, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.782658, mae: 40.128681, mean_q: -59.438427\n"," 101000/300000: episode: 505, duration: 1.896s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 5.551165, mae: 40.224457, mean_q: -59.754852\n"," 101200/300000: episode: 506, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 5.761919, mae: 40.349194, mean_q: -60.000439\n"," 101400/300000: episode: 507, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 9.013416, mae: 40.537552, mean_q: -60.066174\n"," 101600/300000: episode: 508, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.024079, mae: 40.578846, mean_q: -60.108261\n"," 101800/300000: episode: 509, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 11.102971, mae: 40.591328, mean_q: -60.080761\n"," 102000/300000: episode: 510, duration: 1.809s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 6.130929, mae: 40.583794, mean_q: -60.215546\n"," 102200/300000: episode: 511, duration: 1.809s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.668668, mae: 40.741844, mean_q: -60.461826\n"," 102400/300000: episode: 512, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.164795, mae: 40.828712, mean_q: -60.523373\n"," 102600/300000: episode: 513, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.437508, mae: 40.805931, mean_q: -60.426113\n"," 102800/300000: episode: 514, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 10.459187, mae: 40.806870, mean_q: -60.406605\n"," 103000/300000: episode: 515, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.882219, mae: 40.705204, mean_q: -60.299686\n"," 103200/300000: episode: 516, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.293639, mae: 40.859283, mean_q: -60.671665\n"," 103400/300000: episode: 517, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 9.300707, mae: 40.779118, mean_q: -60.427078\n"," 103600/300000: episode: 518, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 7.385026, mae: 40.811520, mean_q: -60.525063\n"," 103800/300000: episode: 519, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.552667, mae: 40.995937, mean_q: -60.820763\n"," 104000/300000: episode: 520, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.207222, mae: 40.978062, mean_q: -60.815086\n"," 104200/300000: episode: 521, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.738754, mae: 41.054073, mean_q: -60.841045\n"," 104400/300000: episode: 522, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.921490, mae: 40.912663, mean_q: -60.647789\n"," 104600/300000: episode: 523, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.103577, mae: 41.025986, mean_q: -60.829540\n"," 104800/300000: episode: 524, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.986788, mae: 41.137188, mean_q: -61.006420\n"," 105000/300000: episode: 525, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.399515, mae: 41.075253, mean_q: -60.812401\n"," 105200/300000: episode: 526, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 9.268702, mae: 40.992821, mean_q: -60.617851\n"," 105400/300000: episode: 527, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 6.254114, mae: 40.923466, mean_q: -60.758801\n"," 105600/300000: episode: 528, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.930545, mae: 40.968624, mean_q: -60.606689\n"," 105800/300000: episode: 529, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 9.399114, mae: 40.923981, mean_q: -60.616581\n"," 106000/300000: episode: 530, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.407013, mae: 40.821926, mean_q: -60.402321\n"," 106200/300000: episode: 531, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.160489, mae: 40.708778, mean_q: -60.308388\n"," 106400/300000: episode: 532, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 7.328046, mae: 40.687378, mean_q: -60.345226\n"," 106600/300000: episode: 533, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.524479, mae: 40.696327, mean_q: -60.397850\n"," 106800/300000: episode: 534, duration: 1.809s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.206047, mae: 40.666752, mean_q: -60.187412\n"," 107000/300000: episode: 535, duration: 1.812s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 10.580722, mae: 40.736790, mean_q: -60.299805\n"," 107200/300000: episode: 536, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.400093, mae: 40.739994, mean_q: -60.501846\n"," 107400/300000: episode: 537, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.111020, mae: 40.789120, mean_q: -60.454796\n"," 107600/300000: episode: 538, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 6.180920, mae: 40.722458, mean_q: -60.489513\n"," 107800/300000: episode: 539, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 6.773679, mae: 40.634315, mean_q: -60.287010\n"," 108000/300000: episode: 540, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 6.015779, mae: 40.819038, mean_q: -60.670547\n"," 108200/300000: episode: 541, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.178763, mae: 40.930405, mean_q: -60.712799\n"," 108400/300000: episode: 542, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.717120, mae: 40.861874, mean_q: -60.493954\n"," 108600/300000: episode: 543, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 7.322567, mae: 40.770496, mean_q: -60.395241\n"," 108800/300000: episode: 544, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.750 [0.000, 2.000],  loss: 9.649707, mae: 40.664421, mean_q: -60.070545\n"," 109000/300000: episode: 545, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 10.741528, mae: 40.514668, mean_q: -59.942665\n"," 109200/300000: episode: 546, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 7.977243, mae: 40.306137, mean_q: -59.745461\n"," 109400/300000: episode: 547, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.497389, mae: 40.460556, mean_q: -59.968609\n"," 109600/300000: episode: 548, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.021955, mae: 40.342381, mean_q: -59.885761\n"," 109800/300000: episode: 549, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 8.482290, mae: 40.332199, mean_q: -59.683643\n"," 110000/300000: episode: 550, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.590313, mae: 40.225201, mean_q: -59.490089\n"," 110200/300000: episode: 551, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.086754, mae: 40.242008, mean_q: -59.666996\n"," 110400/300000: episode: 552, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 7.776718, mae: 40.152763, mean_q: -59.539181\n"," 110600/300000: episode: 553, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.678602, mae: 40.121151, mean_q: -59.489727\n"," 110800/300000: episode: 554, duration: 1.806s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 6.448595, mae: 40.308746, mean_q: -59.848080\n"," 111000/300000: episode: 555, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.823480, mae: 40.241299, mean_q: -59.601612\n"," 111200/300000: episode: 556, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 7.332311, mae: 40.343834, mean_q: -59.829666\n"," 111400/300000: episode: 557, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 7.715372, mae: 40.358936, mean_q: -59.823467\n"," 111600/300000: episode: 558, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 8.184412, mae: 40.404423, mean_q: -59.901703\n"," 111800/300000: episode: 559, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 10.479485, mae: 40.241634, mean_q: -59.492188\n"," 112000/300000: episode: 560, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 6.068367, mae: 40.138046, mean_q: -59.538956\n"," 112200/300000: episode: 561, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 6.733359, mae: 40.196884, mean_q: -59.664707\n"," 112400/300000: episode: 562, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 8.465706, mae: 40.236122, mean_q: -59.602772\n"," 112600/300000: episode: 563, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 7.845934, mae: 40.340923, mean_q: -59.794483\n"," 112800/300000: episode: 564, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 6.188744, mae: 40.461620, mean_q: -60.106133\n"," 113000/300000: episode: 565, duration: 1.878s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.268948, mae: 40.447357, mean_q: -60.008579\n"," 113200/300000: episode: 566, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 9.765166, mae: 40.551662, mean_q: -60.037010\n"," 113400/300000: episode: 567, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.036222, mae: 40.544514, mean_q: -60.152206\n"," 113600/300000: episode: 568, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.515656, mae: 40.575436, mean_q: -60.093315\n"," 113800/300000: episode: 569, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.735257, mae: 40.604904, mean_q: -60.069191\n"," 114000/300000: episode: 570, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 5.985344, mae: 40.655552, mean_q: -60.332291\n"," 114200/300000: episode: 571, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 8.935821, mae: 40.611290, mean_q: -60.192841\n"," 114400/300000: episode: 572, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.007855, mae: 40.489880, mean_q: -60.045265\n"," 114600/300000: episode: 573, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 7.461742, mae: 40.656612, mean_q: -60.331589\n"," 114800/300000: episode: 574, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 6.394444, mae: 40.777157, mean_q: -60.476620\n"," 115000/300000: episode: 575, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 7.655752, mae: 40.766468, mean_q: -60.446301\n"," 115200/300000: episode: 576, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.353725, mae: 40.703659, mean_q: -60.300938\n"," 115400/300000: episode: 577, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 7.882592, mae: 40.690903, mean_q: -60.268311\n"," 115600/300000: episode: 578, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.667758, mae: 40.777706, mean_q: -60.463623\n"," 115800/300000: episode: 579, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.648444, mae: 40.836803, mean_q: -60.571415\n"," 116000/300000: episode: 580, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 5.997920, mae: 40.845444, mean_q: -60.653858\n"," 116200/300000: episode: 581, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 8.121181, mae: 41.006084, mean_q: -60.852753\n"," 116400/300000: episode: 582, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.252693, mae: 40.996532, mean_q: -60.770336\n"," 116600/300000: episode: 583, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.810256, mae: 40.814205, mean_q: -60.361069\n"," 116800/300000: episode: 584, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 9.565722, mae: 40.862514, mean_q: -60.454132\n"," 117000/300000: episode: 585, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 7.686890, mae: 40.918716, mean_q: -60.710136\n"," 117200/300000: episode: 586, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 7.084445, mae: 40.931908, mean_q: -60.671364\n"," 117400/300000: episode: 587, duration: 1.816s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.058430, mae: 41.040970, mean_q: -60.856880\n"," 117600/300000: episode: 588, duration: 1.794s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.521380, mae: 40.962215, mean_q: -60.555275\n"," 117800/300000: episode: 589, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 9.026034, mae: 41.026714, mean_q: -60.706131\n"," 118000/300000: episode: 590, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.670544, mae: 41.018345, mean_q: -60.816681\n"," 118200/300000: episode: 591, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.653999, mae: 40.929688, mean_q: -60.682198\n"," 118400/300000: episode: 592, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 13.020053, mae: 40.869926, mean_q: -60.225567\n"," 118600/300000: episode: 593, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 7.487787, mae: 40.771591, mean_q: -60.430614\n"," 118800/300000: episode: 594, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 7.876364, mae: 40.753361, mean_q: -60.337349\n"," 119000/300000: episode: 595, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.813358, mae: 40.831329, mean_q: -60.451641\n"," 119200/300000: episode: 596, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 6.656539, mae: 40.900738, mean_q: -60.707840\n"," 119400/300000: episode: 597, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.477289, mae: 40.925026, mean_q: -60.705376\n"," 119600/300000: episode: 598, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.225852, mae: 40.829178, mean_q: -60.551769\n"," 119800/300000: episode: 599, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 6.101044, mae: 40.873421, mean_q: -60.632267\n"," 120000/300000: episode: 600, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 6.579716, mae: 40.957359, mean_q: -60.780922\n"," 120200/300000: episode: 601, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 7.696490, mae: 41.155029, mean_q: -61.023312\n"," 120400/300000: episode: 602, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.613358, mae: 40.948505, mean_q: -60.717674\n"," 120600/300000: episode: 603, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 10.000815, mae: 40.884884, mean_q: -60.505630\n"," 120800/300000: episode: 604, duration: 1.764s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.203163, mae: 40.788517, mean_q: -60.294559\n"," 121000/300000: episode: 605, duration: 1.750s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 7.414919, mae: 40.686966, mean_q: -60.265480\n"," 121200/300000: episode: 606, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.635498, mae: 40.903133, mean_q: -60.648056\n"," 121400/300000: episode: 607, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 9.432372, mae: 40.788620, mean_q: -60.345097\n"," 121600/300000: episode: 608, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 5.643809, mae: 40.873310, mean_q: -60.655907\n"," 121800/300000: episode: 609, duration: 1.793s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 7.274294, mae: 40.900108, mean_q: -60.629223\n"," 122000/300000: episode: 610, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 6.724951, mae: 41.001785, mean_q: -60.816288\n"," 122200/300000: episode: 611, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 9.170800, mae: 40.920807, mean_q: -60.439640\n"," 122400/300000: episode: 612, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.073108, mae: 41.013817, mean_q: -60.765926\n"," 122600/300000: episode: 613, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 7.147671, mae: 40.956200, mean_q: -60.776421\n"," 122800/300000: episode: 614, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 6.773991, mae: 41.050869, mean_q: -60.825577\n"," 123000/300000: episode: 615, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 9.716801, mae: 40.965652, mean_q: -60.724686\n"," 123200/300000: episode: 616, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 6.341883, mae: 41.037296, mean_q: -60.889580\n"," 123400/300000: episode: 617, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.807745, mae: 41.064911, mean_q: -60.830997\n"," 123600/300000: episode: 618, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 11.427277, mae: 40.944191, mean_q: -60.588070\n"," 123800/300000: episode: 619, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.317211, mae: 40.897354, mean_q: -60.563271\n"," 124000/300000: episode: 620, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.043492, mae: 40.771820, mean_q: -60.327869\n"," 124200/300000: episode: 621, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 7.076109, mae: 40.622166, mean_q: -60.145382\n"," 124400/300000: episode: 622, duration: 1.778s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.968173, mae: 40.822536, mean_q: -60.491417\n"," 124600/300000: episode: 623, duration: 1.745s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 11.102422, mae: 40.693436, mean_q: -60.035156\n"," 124800/300000: episode: 624, duration: 1.739s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.408403, mae: 40.614410, mean_q: -60.058243\n"," 125000/300000: episode: 625, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 4.606138, mae: 40.550648, mean_q: -60.205299\n"," 125200/300000: episode: 626, duration: 1.811s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.231021, mae: 40.822594, mean_q: -60.475533\n"," 125400/300000: episode: 627, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 9.253681, mae: 40.481659, mean_q: -59.827606\n"," 125600/300000: episode: 628, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 11.651004, mae: 40.374012, mean_q: -59.613350\n"," 125800/300000: episode: 629, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 9.007449, mae: 40.264652, mean_q: -59.582619\n"," 126000/300000: episode: 630, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.039972, mae: 40.263645, mean_q: -59.530594\n"," 126200/300000: episode: 631, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 8.256894, mae: 40.181770, mean_q: -59.480980\n"," 126400/300000: episode: 632, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 7.008333, mae: 40.002151, mean_q: -59.196854\n"," 126600/300000: episode: 633, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.052588, mae: 39.918419, mean_q: -59.161991\n"," 126800/300000: episode: 634, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 9.142892, mae: 39.857784, mean_q: -59.030781\n"," 127000/300000: episode: 635, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 9.094560, mae: 39.833004, mean_q: -58.846436\n"," 127200/300000: episode: 636, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 5.744778, mae: 39.679810, mean_q: -58.770241\n"," 127400/300000: episode: 637, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 7.642681, mae: 39.483131, mean_q: -58.459522\n"," 127600/300000: episode: 638, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 6.190286, mae: 39.543667, mean_q: -58.636562\n"," 127800/300000: episode: 639, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.147090, mae: 39.610706, mean_q: -58.549347\n"," 128000/300000: episode: 640, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 5.730467, mae: 39.279980, mean_q: -58.145031\n"," 128200/300000: episode: 641, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 5.416958, mae: 39.288010, mean_q: -58.226410\n"," 128400/300000: episode: 642, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.160144, mae: 39.146683, mean_q: -57.849762\n"," 128600/300000: episode: 643, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.041894, mae: 39.024853, mean_q: -57.804337\n"," 128800/300000: episode: 644, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.088728, mae: 39.020119, mean_q: -57.751453\n"," 129000/300000: episode: 645, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 9.585087, mae: 38.945667, mean_q: -57.552948\n"," 129200/300000: episode: 646, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 8.335682, mae: 38.768497, mean_q: -57.248066\n"," 129400/300000: episode: 647, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 8.450391, mae: 38.725834, mean_q: -57.309990\n"," 129600/300000: episode: 648, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.478738, mae: 38.749985, mean_q: -57.329288\n"," 129800/300000: episode: 649, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 5.822614, mae: 38.706394, mean_q: -57.329796\n"," 130000/300000: episode: 650, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.004498, mae: 38.777470, mean_q: -57.441753\n"," 130200/300000: episode: 651, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.538537, mae: 38.911514, mean_q: -57.589317\n"," 130400/300000: episode: 652, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 5.781025, mae: 38.956787, mean_q: -57.740250\n"," 130600/300000: episode: 653, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.554712, mae: 38.966412, mean_q: -57.683914\n"," 130800/300000: episode: 654, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 6.117153, mae: 39.067383, mean_q: -57.935478\n"," 131000/300000: episode: 655, duration: 1.761s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 6.079191, mae: 39.127831, mean_q: -58.047398\n"," 131200/300000: episode: 656, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 7.640353, mae: 39.235653, mean_q: -58.070568\n"," 131400/300000: episode: 657, duration: 1.742s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.521559, mae: 39.242756, mean_q: -58.101341\n"," 131600/300000: episode: 658, duration: 1.758s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.715053, mae: 39.263538, mean_q: -58.089981\n"," 131800/300000: episode: 659, duration: 1.751s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 7.291156, mae: 39.293373, mean_q: -58.197987\n"," 132000/300000: episode: 660, duration: 1.756s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 7.650018, mae: 39.382515, mean_q: -58.326817\n"," 132200/300000: episode: 661, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 8.626578, mae: 39.183605, mean_q: -57.944202\n"," 132400/300000: episode: 662, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 5.162535, mae: 39.101479, mean_q: -57.967648\n"," 132600/300000: episode: 663, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.995442, mae: 39.233116, mean_q: -58.059200\n"," 132800/300000: episode: 664, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.285115, mae: 39.220238, mean_q: -58.068008\n"," 133000/300000: episode: 665, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.802402, mae: 39.209091, mean_q: -57.885956\n"," 133200/300000: episode: 666, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 5.135469, mae: 39.178104, mean_q: -58.143204\n"," 133400/300000: episode: 667, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.972284, mae: 39.216454, mean_q: -58.085758\n"," 133600/300000: episode: 668, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 6.458361, mae: 39.366703, mean_q: -58.384258\n"," 133800/300000: episode: 669, duration: 1.778s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 6.605860, mae: 39.428379, mean_q: -58.487267\n"," 134000/300000: episode: 670, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 9.673238, mae: 39.461376, mean_q: -58.350407\n"," 134200/300000: episode: 671, duration: 1.809s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 5.244455, mae: 39.400078, mean_q: -58.465416\n"," 134400/300000: episode: 672, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 9.202363, mae: 39.443398, mean_q: -58.361649\n"," 134600/300000: episode: 673, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 9.502019, mae: 39.373989, mean_q: -58.260445\n"," 134800/300000: episode: 674, duration: 1.763s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 9.386295, mae: 39.358509, mean_q: -58.194176\n"," 135000/300000: episode: 675, duration: 1.811s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 7.022899, mae: 39.271645, mean_q: -58.187237\n"," 135200/300000: episode: 676, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 7.390174, mae: 39.236923, mean_q: -58.068363\n"," 135400/300000: episode: 677, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.647062, mae: 39.212318, mean_q: -58.027729\n"," 135600/300000: episode: 678, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.262526, mae: 39.321606, mean_q: -58.258133\n"," 135800/300000: episode: 679, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 6.690369, mae: 39.390793, mean_q: -58.399940\n"," 136000/300000: episode: 680, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.585839, mae: 39.291553, mean_q: -58.129616\n"," 136200/300000: episode: 681, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 7.122887, mae: 39.359478, mean_q: -58.305275\n"," 136400/300000: episode: 682, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 7.565207, mae: 39.284439, mean_q: -58.170731\n"," 136600/300000: episode: 683, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.505744, mae: 39.258434, mean_q: -58.050087\n"," 136800/300000: episode: 684, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.693477, mae: 39.312965, mean_q: -58.303379\n"," 137000/300000: episode: 685, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.121127, mae: 39.259274, mean_q: -58.064133\n"," 137200/300000: episode: 686, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 5.273760, mae: 39.185295, mean_q: -58.097652\n"," 137400/300000: episode: 687, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 6.837582, mae: 39.129360, mean_q: -57.910557\n"," 137600/300000: episode: 688, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.907318, mae: 39.087528, mean_q: -57.863983\n"," 137800/300000: episode: 689, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.648436, mae: 39.118492, mean_q: -57.897430\n"," 138000/300000: episode: 690, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.592896, mae: 39.150249, mean_q: -57.833866\n"," 138200/300000: episode: 691, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.790544, mae: 39.004185, mean_q: -57.721310\n"," 138400/300000: episode: 692, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.745 [0.000, 2.000],  loss: 10.246607, mae: 38.865074, mean_q: -57.401386\n"," 138600/300000: episode: 693, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 6.718469, mae: 38.691429, mean_q: -57.275547\n"," 138800/300000: episode: 694, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 6.250591, mae: 38.926994, mean_q: -57.681416\n"," 139000/300000: episode: 695, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 6.754432, mae: 38.786888, mean_q: -57.391338\n"," 139200/300000: episode: 696, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 6.692908, mae: 38.576542, mean_q: -57.102402\n"," 139400/300000: episode: 697, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.745 [0.000, 2.000],  loss: 7.846138, mae: 38.631821, mean_q: -57.128807\n"," 139600/300000: episode: 698, duration: 1.919s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.281204, mae: 38.638702, mean_q: -57.164658\n"," 139800/300000: episode: 699, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 7.905022, mae: 38.359806, mean_q: -56.659161\n"," 140000/300000: episode: 700, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 6.051338, mae: 38.450871, mean_q: -56.840782\n"," 140200/300000: episode: 701, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.680 [0.000, 2.000],  loss: 5.825322, mae: 38.338593, mean_q: -56.735210\n"," 140400/300000: episode: 702, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.070594, mae: 38.275898, mean_q: -56.613136\n"," 140600/300000: episode: 703, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 7.631921, mae: 37.903942, mean_q: -55.900658\n"," 140800/300000: episode: 704, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.750 [0.000, 2.000],  loss: 7.719546, mae: 37.716568, mean_q: -55.595558\n"," 141000/300000: episode: 705, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.680 [0.000, 2.000],  loss: 7.499423, mae: 37.310486, mean_q: -55.050137\n"," 141200/300000: episode: 706, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.660 [0.000, 2.000],  loss: 8.036429, mae: 37.148594, mean_q: -54.812130\n"," 141400/300000: episode: 707, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000],  loss: 9.230417, mae: 36.714073, mean_q: -54.071548\n"," 141600/300000: episode: 708, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000],  loss: 6.100926, mae: 36.550781, mean_q: -53.945263\n"," 141800/300000: episode: 709, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 6.967030, mae: 36.259998, mean_q: -53.540810\n"," 142000/300000: episode: 710, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000],  loss: 4.529055, mae: 36.238853, mean_q: -53.529510\n"," 142200/300000: episode: 711, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 7.463028, mae: 35.751041, mean_q: -52.661190\n"," 142400/300000: episode: 712, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000],  loss: 5.557089, mae: 35.560444, mean_q: -52.426666\n"," 142600/300000: episode: 713, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.715 [0.000, 2.000],  loss: 5.525538, mae: 35.358761, mean_q: -52.135395\n"," 142800/300000: episode: 714, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 7.493142, mae: 35.159943, mean_q: -51.755566\n"," 143000/300000: episode: 715, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 4.852292, mae: 34.878147, mean_q: -51.375870\n"," 143200/300000: episode: 716, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.702486, mae: 34.312378, mean_q: -50.396221\n"," 143400/300000: episode: 717, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 5.259413, mae: 34.034164, mean_q: -50.160324\n"," 143600/300000: episode: 718, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 5.775278, mae: 33.697708, mean_q: -49.615707\n"," 143800/300000: episode: 719, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 3.436657, mae: 33.555908, mean_q: -49.463203\n"," 144000/300000: episode: 720, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 4.980707, mae: 33.397110, mean_q: -49.127872\n"," 144200/300000: episode: 721, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 4.209303, mae: 32.892014, mean_q: -48.382816\n"," 144400/300000: episode: 722, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 5.347205, mae: 32.376663, mean_q: -47.590954\n"," 144600/300000: episode: 723, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 5.707532, mae: 32.168819, mean_q: -47.285362\n"," 144800/300000: episode: 724, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 4.619472, mae: 31.813799, mean_q: -46.750298\n"," 145000/300000: episode: 725, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 4.218144, mae: 31.372339, mean_q: -46.070629\n"," 145200/300000: episode: 726, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 4.533341, mae: 31.028328, mean_q: -45.504219\n"," 145400/300000: episode: 727, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 2.875250, mae: 30.610985, mean_q: -44.954151\n"," 145600/300000: episode: 728, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 4.826124, mae: 30.068666, mean_q: -43.983662\n"," 145800/300000: episode: 729, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 4.573105, mae: 29.552597, mean_q: -43.326485\n"," 146000/300000: episode: 730, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 3.646443, mae: 29.220005, mean_q: -42.768810\n"," 146200/300000: episode: 731, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 2.423605, mae: 28.744942, mean_q: -42.144745\n"," 146400/300000: episode: 732, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 4.422955, mae: 28.381048, mean_q: -41.464043\n"," 146594/300000: episode: 733, duration: 1.808s, episode steps: 194, steps per second: 107, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 3.152107, mae: 27.878487, mean_q: -40.776035\n"," 146794/300000: episode: 734, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 2.762604, mae: 27.753193, mean_q: -40.583916\n"," 146994/300000: episode: 735, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 3.023255, mae: 27.144331, mean_q: -39.711048\n"," 147194/300000: episode: 736, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.713606, mae: 26.796650, mean_q: -39.137558\n"," 147394/300000: episode: 737, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 3.245830, mae: 26.220837, mean_q: -38.300549\n"," 147594/300000: episode: 738, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 3.022529, mae: 25.904043, mean_q: -37.791611\n"," 147794/300000: episode: 739, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 2.378937, mae: 25.624418, mean_q: -37.411514\n"," 147994/300000: episode: 740, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 2.709756, mae: 25.285175, mean_q: -36.858227\n"," 148194/300000: episode: 741, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.581124, mae: 25.029600, mean_q: -36.532330\n"," 148394/300000: episode: 742, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000],  loss: 2.612772, mae: 24.545925, mean_q: -35.756557\n"," 148594/300000: episode: 743, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 2.073821, mae: 24.367582, mean_q: -35.533787\n"," 148794/300000: episode: 744, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 2.947269, mae: 23.976130, mean_q: -34.852127\n"," 148994/300000: episode: 745, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 1.952461, mae: 23.566832, mean_q: -34.354015\n"," 149194/300000: episode: 746, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.750 [0.000, 2.000],  loss: 1.390505, mae: 23.312271, mean_q: -34.015915\n"," 149394/300000: episode: 747, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 2.505706, mae: 23.022612, mean_q: -33.508102\n"," 149594/300000: episode: 748, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.493672, mae: 22.646982, mean_q: -32.980141\n"," 149794/300000: episode: 749, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 2.369712, mae: 22.502924, mean_q: -32.833141\n"," 149994/300000: episode: 750, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.489788, mae: 22.366053, mean_q: -32.685932\n"," 150194/300000: episode: 751, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 2.376876, mae: 22.175440, mean_q: -32.398815\n"," 150394/300000: episode: 752, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.294907, mae: 22.317749, mean_q: -32.599640\n"," 150594/300000: episode: 753, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 2.026032, mae: 22.155960, mean_q: -32.418339\n"," 150794/300000: episode: 754, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.629109, mae: 22.259514, mean_q: -32.529804\n"," 150994/300000: episode: 755, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.290969, mae: 22.065134, mean_q: -32.260487\n"," 151194/300000: episode: 756, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.758925, mae: 22.262651, mean_q: -32.633816\n"," 151394/300000: episode: 757, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 2.481241, mae: 22.235680, mean_q: -32.578224\n"," 151594/300000: episode: 758, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.678532, mae: 22.414595, mean_q: -32.924698\n"," 151794/300000: episode: 759, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.470858, mae: 22.492193, mean_q: -33.094284\n"," 151994/300000: episode: 760, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 3.198271, mae: 22.716274, mean_q: -33.312984\n"," 152194/300000: episode: 761, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.598397, mae: 22.842852, mean_q: -33.723068\n"," 152394/300000: episode: 762, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 3.118638, mae: 23.202374, mean_q: -34.137978\n"," 152594/300000: episode: 763, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 2.857179, mae: 23.444830, mean_q: -34.547215\n"," 152794/300000: episode: 764, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 2.792151, mae: 23.670683, mean_q: -34.911366\n"," 152994/300000: episode: 765, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.506871, mae: 23.909468, mean_q: -35.304459\n"," 153194/300000: episode: 766, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.546086, mae: 24.245085, mean_q: -35.799957\n"," 153394/300000: episode: 767, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 2.271128, mae: 24.621727, mean_q: -36.373852\n"," 153594/300000: episode: 768, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 3.900858, mae: 24.853296, mean_q: -36.667938\n"," 153794/300000: episode: 769, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 3.035367, mae: 25.208067, mean_q: -37.164074\n"," 153994/300000: episode: 770, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.593071, mae: 25.350470, mean_q: -37.378189\n"," 154194/300000: episode: 771, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 3.643320, mae: 25.557405, mean_q: -37.746292\n"," 154394/300000: episode: 772, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.361208, mae: 25.864714, mean_q: -38.327084\n"," 154594/300000: episode: 773, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.916182, mae: 26.255362, mean_q: -38.855713\n"," 154794/300000: episode: 774, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 3.193484, mae: 26.620869, mean_q: -39.360428\n"," 154994/300000: episode: 775, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 3.502631, mae: 26.795891, mean_q: -39.638031\n"," 155194/300000: episode: 776, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.979573, mae: 27.088228, mean_q: -40.107586\n"," 155394/300000: episode: 777, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.405908, mae: 27.343647, mean_q: -40.473900\n"," 155594/300000: episode: 778, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.480490, mae: 27.643608, mean_q: -40.927902\n"," 155794/300000: episode: 779, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 3.728770, mae: 27.841518, mean_q: -41.100239\n"," 155994/300000: episode: 780, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 3.341641, mae: 28.061108, mean_q: -41.504078\n"," 156194/300000: episode: 781, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 4.043169, mae: 28.288336, mean_q: -41.814316\n"," 156394/300000: episode: 782, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 3.106371, mae: 28.478844, mean_q: -42.133610\n"," 156594/300000: episode: 783, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 4.548885, mae: 28.638905, mean_q: -42.306034\n"," 156794/300000: episode: 784, duration: 1.962s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 4.140143, mae: 28.839024, mean_q: -42.604462\n"," 156994/300000: episode: 785, duration: 1.902s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 2.988464, mae: 28.887739, mean_q: -42.719574\n"," 157194/300000: episode: 786, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.758135, mae: 29.163441, mean_q: -43.196045\n"," 157394/300000: episode: 787, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 4.252842, mae: 29.299871, mean_q: -43.297626\n"," 157594/300000: episode: 788, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 5.194874, mae: 29.429859, mean_q: -43.388885\n"," 157794/300000: episode: 789, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.964027, mae: 29.609219, mean_q: -43.863163\n"," 157994/300000: episode: 790, duration: 1.918s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 4.276462, mae: 29.674490, mean_q: -43.846138\n"," 158194/300000: episode: 791, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 3.427669, mae: 29.793190, mean_q: -44.045517\n"," 158394/300000: episode: 792, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.957433, mae: 29.951826, mean_q: -44.353477\n"," 158594/300000: episode: 793, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.847515, mae: 30.070396, mean_q: -44.457069\n"," 158794/300000: episode: 794, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 3.768059, mae: 30.261215, mean_q: -44.759140\n"," 158994/300000: episode: 795, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 3.724803, mae: 30.373241, mean_q: -44.999004\n"," 159194/300000: episode: 796, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 5.355886, mae: 30.385996, mean_q: -44.851788\n"," 159394/300000: episode: 797, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 3.859290, mae: 30.437119, mean_q: -44.927860\n"," 159594/300000: episode: 798, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 3.277775, mae: 30.600344, mean_q: -45.268776\n"," 159794/300000: episode: 799, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 3.638322, mae: 30.596607, mean_q: -45.294777\n"," 159994/300000: episode: 800, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 4.420234, mae: 30.934746, mean_q: -45.794971\n"," 160194/300000: episode: 801, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 3.703845, mae: 30.981167, mean_q: -45.839970\n"," 160394/300000: episode: 802, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 3.878444, mae: 30.999477, mean_q: -45.845482\n"," 160594/300000: episode: 803, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 5.042815, mae: 31.148752, mean_q: -45.942841\n"," 160794/300000: episode: 804, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 4.068001, mae: 31.152216, mean_q: -46.019386\n"," 160994/300000: episode: 805, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 6.206266, mae: 31.134893, mean_q: -45.920704\n"," 161194/300000: episode: 806, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 4.324313, mae: 31.032415, mean_q: -45.823742\n"," 161394/300000: episode: 807, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 5.151226, mae: 31.037453, mean_q: -45.825584\n"," 161594/300000: episode: 808, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 4.249588, mae: 31.095512, mean_q: -45.932529\n"," 161794/300000: episode: 809, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 5.239710, mae: 31.064800, mean_q: -45.910980\n"," 161994/300000: episode: 810, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 5.048498, mae: 31.087915, mean_q: -45.945133\n"," 162194/300000: episode: 811, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 3.406959, mae: 31.140444, mean_q: -46.063908\n"," 162394/300000: episode: 812, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.251213, mae: 31.341713, mean_q: -46.408966\n"," 162594/300000: episode: 813, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 4.760771, mae: 31.332603, mean_q: -46.230415\n"," 162794/300000: episode: 814, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 5.865247, mae: 31.228735, mean_q: -46.061188\n"," 162994/300000: episode: 815, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 4.689695, mae: 31.190725, mean_q: -46.103050\n"," 163194/300000: episode: 816, duration: 1.809s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 5.667152, mae: 31.202122, mean_q: -46.089699\n"," 163394/300000: episode: 817, duration: 1.812s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.252224, mae: 31.201078, mean_q: -46.173904\n"," 163594/300000: episode: 818, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 4.610968, mae: 31.385874, mean_q: -46.386505\n"," 163794/300000: episode: 819, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 4.423720, mae: 31.394896, mean_q: -46.402794\n"," 163994/300000: episode: 820, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 4.432929, mae: 31.487358, mean_q: -46.528255\n"," 164194/300000: episode: 821, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 3.826584, mae: 31.577564, mean_q: -46.770050\n"," 164394/300000: episode: 822, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 4.275668, mae: 31.615139, mean_q: -46.753174\n"," 164594/300000: episode: 823, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 5.327175, mae: 31.816978, mean_q: -47.003086\n"," 164794/300000: episode: 824, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.678513, mae: 31.828037, mean_q: -47.095181\n"," 164994/300000: episode: 825, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.885521, mae: 31.891640, mean_q: -47.255722\n"," 165194/300000: episode: 826, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.294268, mae: 32.001087, mean_q: -47.176815\n"," 165394/300000: episode: 827, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 3.818560, mae: 31.981621, mean_q: -47.345703\n"," 165594/300000: episode: 828, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.887062, mae: 31.884392, mean_q: -46.995483\n"," 165794/300000: episode: 829, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 4.932798, mae: 31.876772, mean_q: -47.090660\n"," 165994/300000: episode: 830, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 4.971429, mae: 31.806641, mean_q: -46.982265\n"," 166194/300000: episode: 831, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 5.440930, mae: 31.680426, mean_q: -46.733692\n"," 166394/300000: episode: 832, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 5.039221, mae: 31.599138, mean_q: -46.646805\n"," 166594/300000: episode: 833, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.603179, mae: 31.669989, mean_q: -46.672573\n"," 166794/300000: episode: 834, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 4.724667, mae: 31.510752, mean_q: -46.516415\n"," 166994/300000: episode: 835, duration: 1.916s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 5.415576, mae: 31.373360, mean_q: -46.225674\n"," 167194/300000: episode: 836, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 4.354419, mae: 31.370422, mean_q: -46.295738\n"," 167394/300000: episode: 837, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 4.641146, mae: 31.318125, mean_q: -46.177559\n"," 167594/300000: episode: 838, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.149052, mae: 31.280787, mean_q: -46.265335\n"," 167794/300000: episode: 839, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 5.172904, mae: 31.244713, mean_q: -46.048653\n"," 167994/300000: episode: 840, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.924203, mae: 31.169828, mean_q: -46.107872\n"," 168194/300000: episode: 841, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 4.044065, mae: 31.141464, mean_q: -46.008965\n"," 168394/300000: episode: 842, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 4.410925, mae: 31.212929, mean_q: -46.063019\n"," 168594/300000: episode: 843, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.742347, mae: 31.113098, mean_q: -45.965874\n"," 168794/300000: episode: 844, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 4.581599, mae: 31.049454, mean_q: -45.807316\n"," 168994/300000: episode: 845, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 3.776299, mae: 31.158564, mean_q: -45.942337\n"," 169194/300000: episode: 846, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 2.883176, mae: 30.896015, mean_q: -45.674706\n"," 169394/300000: episode: 847, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 3.911211, mae: 31.015093, mean_q: -45.733707\n"," 169594/300000: episode: 848, duration: 1.793s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 3.814069, mae: 31.008989, mean_q: -45.816250\n"," 169794/300000: episode: 849, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 4.406143, mae: 30.979317, mean_q: -45.735817\n"," 169994/300000: episode: 850, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 4.246128, mae: 30.937038, mean_q: -45.646152\n"," 170194/300000: episode: 851, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.476635, mae: 30.993101, mean_q: -45.816757\n"," 170394/300000: episode: 852, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 4.468525, mae: 30.892706, mean_q: -45.542084\n"," 170594/300000: episode: 853, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 4.093693, mae: 30.941807, mean_q: -45.667892\n"," 170794/300000: episode: 854, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 4.025537, mae: 30.803213, mean_q: -45.396103\n"," 170994/300000: episode: 855, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.589221, mae: 30.807768, mean_q: -45.461456\n"," 171194/300000: episode: 856, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 5.228529, mae: 30.584457, mean_q: -45.067440\n"," 171394/300000: episode: 857, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 4.567549, mae: 30.669062, mean_q: -45.246841\n"," 171594/300000: episode: 858, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 3.473890, mae: 30.655407, mean_q: -45.253769\n"," 171794/300000: episode: 859, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 4.383426, mae: 30.519777, mean_q: -44.952637\n"," 171994/300000: episode: 860, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.911378, mae: 30.430498, mean_q: -44.899185\n"," 172194/300000: episode: 861, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 4.441448, mae: 30.374151, mean_q: -44.776058\n"," 172394/300000: episode: 862, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 4.285810, mae: 30.343740, mean_q: -44.719864\n"," 172594/300000: episode: 863, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 4.886526, mae: 30.384653, mean_q: -44.843674\n"," 172794/300000: episode: 864, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 3.762333, mae: 30.302444, mean_q: -44.644249\n"," 172994/300000: episode: 865, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 3.020215, mae: 30.127066, mean_q: -44.381630\n"," 173194/300000: episode: 866, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 3.950810, mae: 29.964146, mean_q: -44.154144\n"," 173394/300000: episode: 867, duration: 1.814s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 4.400517, mae: 29.815886, mean_q: -43.914024\n"," 173594/300000: episode: 868, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 3.894113, mae: 29.683741, mean_q: -43.747784\n"," 173794/300000: episode: 869, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 3.541517, mae: 29.770819, mean_q: -43.884289\n"," 173994/300000: episode: 870, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.710111, mae: 29.613960, mean_q: -43.656593\n"," 174194/300000: episode: 871, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.935337, mae: 29.533604, mean_q: -43.626865\n"," 174394/300000: episode: 872, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 2.792449, mae: 29.781157, mean_q: -43.973515\n"," 174594/300000: episode: 873, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 3.420191, mae: 29.660873, mean_q: -43.739834\n"," 174794/300000: episode: 874, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 3.477514, mae: 29.750826, mean_q: -43.890800\n"," 174994/300000: episode: 875, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 3.239727, mae: 29.757036, mean_q: -43.860802\n"," 175194/300000: episode: 876, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 3.391368, mae: 29.564140, mean_q: -43.609024\n"," 175394/300000: episode: 877, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.987295, mae: 29.759621, mean_q: -43.901066\n"," 175594/300000: episode: 878, duration: 1.809s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 3.715229, mae: 29.673018, mean_q: -43.711933\n"," 175794/300000: episode: 879, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 3.810419, mae: 29.488531, mean_q: -43.389233\n"," 175994/300000: episode: 880, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 3.996649, mae: 29.522223, mean_q: -43.396053\n"," 176194/300000: episode: 881, duration: 1.736s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.720732, mae: 29.539532, mean_q: -43.528332\n"," 176394/300000: episode: 882, duration: 1.736s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 3.510502, mae: 29.466192, mean_q: -43.340878\n"," 176594/300000: episode: 883, duration: 1.750s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.962996, mae: 29.411352, mean_q: -43.296581\n"," 176794/300000: episode: 884, duration: 1.739s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 2.978651, mae: 29.317095, mean_q: -43.169865\n"," 176994/300000: episode: 885, duration: 1.753s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 3.219403, mae: 29.109577, mean_q: -42.803486\n"," 177194/300000: episode: 886, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.657814, mae: 28.935188, mean_q: -42.628895\n"," 177394/300000: episode: 887, duration: 1.760s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 2.904441, mae: 28.975981, mean_q: -42.659855\n"," 177594/300000: episode: 888, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.624529, mae: 28.917036, mean_q: -42.637894\n"," 177794/300000: episode: 889, duration: 1.813s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.709400, mae: 29.022429, mean_q: -42.732197\n"," 177994/300000: episode: 890, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 2.302600, mae: 28.946976, mean_q: -42.679455\n"," 178194/300000: episode: 891, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.797751, mae: 28.935858, mean_q: -42.593700\n"," 178394/300000: episode: 892, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 2.904325, mae: 28.898405, mean_q: -42.609421\n"," 178594/300000: episode: 893, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.435228, mae: 29.131975, mean_q: -42.968086\n"," 178794/300000: episode: 894, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.433635, mae: 28.964218, mean_q: -42.739609\n"," 178994/300000: episode: 895, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.743500, mae: 28.844021, mean_q: -42.535767\n"," 179194/300000: episode: 896, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 2.659864, mae: 29.196274, mean_q: -43.182938\n"," 179394/300000: episode: 897, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 2.254187, mae: 29.226273, mean_q: -43.184536\n"," 179594/300000: episode: 898, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.793851, mae: 29.312145, mean_q: -43.350414\n"," 179794/300000: episode: 899, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 3.757528, mae: 29.248720, mean_q: -43.175968\n"," 179994/300000: episode: 900, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.672580, mae: 29.484007, mean_q: -43.573467\n"," 180194/300000: episode: 901, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 2.884623, mae: 29.513948, mean_q: -43.540897\n"," 180394/300000: episode: 902, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.767060, mae: 29.569500, mean_q: -43.741924\n"," 180594/300000: episode: 903, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 3.747063, mae: 29.572031, mean_q: -43.577930\n"," 180794/300000: episode: 904, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 2.403824, mae: 29.645630, mean_q: -43.737530\n"," 180994/300000: episode: 905, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.890444, mae: 29.543297, mean_q: -43.586594\n"," 181194/300000: episode: 906, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.122455, mae: 29.680849, mean_q: -43.861290\n"," 181394/300000: episode: 907, duration: 1.843s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.808326, mae: 29.719727, mean_q: -43.838955\n"," 181594/300000: episode: 908, duration: 1.794s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 4.011445, mae: 29.665752, mean_q: -43.640469\n"," 181794/300000: episode: 909, duration: 1.792s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 3.164857, mae: 29.643715, mean_q: -43.688934\n"," 181994/300000: episode: 910, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 4.141559, mae: 29.439453, mean_q: -43.245274\n"," 182194/300000: episode: 911, duration: 1.771s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 3.774145, mae: 29.369146, mean_q: -43.177193\n"," 182394/300000: episode: 912, duration: 1.793s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 3.376309, mae: 29.219063, mean_q: -42.979603\n"," 182594/300000: episode: 913, duration: 1.767s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 2.906635, mae: 29.170910, mean_q: -42.927013\n"," 182794/300000: episode: 914, duration: 1.774s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 2.801871, mae: 29.180798, mean_q: -42.963043\n"," 182994/300000: episode: 915, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 2.525111, mae: 29.058456, mean_q: -42.798843\n"," 183194/300000: episode: 916, duration: 1.769s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 3.238245, mae: 28.831787, mean_q: -42.417355\n"," 183394/300000: episode: 917, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 3.192954, mae: 28.880587, mean_q: -42.483578\n"," 183594/300000: episode: 918, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.150084, mae: 28.815445, mean_q: -42.498692\n"," 183794/300000: episode: 919, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.279971, mae: 28.816177, mean_q: -42.499256\n"," 183994/300000: episode: 920, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.496588, mae: 28.784683, mean_q: -42.431591\n"," 184194/300000: episode: 921, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.642124, mae: 28.900917, mean_q: -42.613106\n"," 184394/300000: episode: 922, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.857972, mae: 28.927008, mean_q: -42.615543\n"," 184594/300000: episode: 923, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 3.063325, mae: 28.686567, mean_q: -42.225399\n"," 184794/300000: episode: 924, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 2.561758, mae: 28.798784, mean_q: -42.465195\n"," 184994/300000: episode: 925, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.255100, mae: 28.832029, mean_q: -42.612217\n"," 185194/300000: episode: 926, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 2.624869, mae: 28.784809, mean_q: -42.452198\n"," 185394/300000: episode: 927, duration: 1.771s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.225307, mae: 28.662853, mean_q: -42.299084\n"," 185594/300000: episode: 928, duration: 1.765s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.451204, mae: 28.842941, mean_q: -42.592373\n"," 185794/300000: episode: 929, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 2.837006, mae: 28.844707, mean_q: -42.521152\n"," 185994/300000: episode: 930, duration: 1.764s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 3.568908, mae: 28.979519, mean_q: -42.672062\n"," 186194/300000: episode: 931, duration: 1.762s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 2.713806, mae: 28.877144, mean_q: -42.576004\n"," 186394/300000: episode: 932, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.358549, mae: 28.854895, mean_q: -42.499699\n"," 186594/300000: episode: 933, duration: 1.769s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 2.344106, mae: 28.861200, mean_q: -42.531723\n"," 186794/300000: episode: 934, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 3.646721, mae: 28.819538, mean_q: -42.375034\n"," 186994/300000: episode: 935, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 2.715947, mae: 28.689278, mean_q: -42.253056\n"," 187194/300000: episode: 936, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.341755, mae: 28.749086, mean_q: -42.332886\n"," 187394/300000: episode: 937, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.268390, mae: 28.719349, mean_q: -42.260780\n"," 187594/300000: episode: 938, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.520951, mae: 28.684702, mean_q: -42.214989\n"," 187794/300000: episode: 939, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.237955, mae: 28.492302, mean_q: -41.939102\n"," 187994/300000: episode: 940, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 2.024379, mae: 28.648420, mean_q: -42.206120\n"," 188194/300000: episode: 941, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.319715, mae: 28.430836, mean_q: -41.872627\n"," 188394/300000: episode: 942, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.128587, mae: 28.646492, mean_q: -42.194393\n"," 188594/300000: episode: 943, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 2.887625, mae: 28.349171, mean_q: -41.676483\n"," 188794/300000: episode: 944, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.033045, mae: 28.357193, mean_q: -41.811245\n"," 188994/300000: episode: 945, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.078705, mae: 28.311073, mean_q: -41.641739\n"," 189194/300000: episode: 946, duration: 1.935s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.942006, mae: 28.212435, mean_q: -41.500454\n"," 189394/300000: episode: 947, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.653755, mae: 28.331518, mean_q: -41.684315\n"," 189594/300000: episode: 948, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 2.267372, mae: 28.228470, mean_q: -41.608505\n"," 189794/300000: episode: 949, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 2.953455, mae: 28.325283, mean_q: -41.653194\n"," 189994/300000: episode: 950, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.068609, mae: 28.121210, mean_q: -41.443882\n"," 190194/300000: episode: 951, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.489349, mae: 28.360462, mean_q: -41.731277\n"," 190394/300000: episode: 952, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 2.658885, mae: 28.174385, mean_q: -41.471279\n"," 190594/300000: episode: 953, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.226830, mae: 28.250608, mean_q: -41.655930\n"," 190794/300000: episode: 954, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.957373, mae: 28.309517, mean_q: -41.771069\n"," 190994/300000: episode: 955, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.638685, mae: 28.308802, mean_q: -41.618622\n"," 191194/300000: episode: 956, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.805357, mae: 28.188686, mean_q: -41.562763\n"," 191394/300000: episode: 957, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 2.132452, mae: 28.150843, mean_q: -41.419991\n"," 191594/300000: episode: 958, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.581545, mae: 28.178862, mean_q: -41.525925\n"," 191794/300000: episode: 959, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.729372, mae: 28.305910, mean_q: -41.669693\n"," 191994/300000: episode: 960, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.954914, mae: 28.345510, mean_q: -41.725956\n"," 192194/300000: episode: 961, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.748047, mae: 28.437483, mean_q: -41.798954\n"," 192394/300000: episode: 962, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.830428, mae: 28.225014, mean_q: -41.549179\n"," 192594/300000: episode: 963, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 2.495937, mae: 28.195210, mean_q: -41.480068\n"," 192794/300000: episode: 964, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 2.257246, mae: 28.158117, mean_q: -41.486473\n"," 192994/300000: episode: 965, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 1.861070, mae: 28.272852, mean_q: -41.619450\n"," 193194/300000: episode: 966, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 1.495896, mae: 28.294634, mean_q: -41.726917\n"," 193394/300000: episode: 967, duration: 1.943s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.062627, mae: 28.222107, mean_q: -41.600285\n"," 193594/300000: episode: 968, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.509364, mae: 28.334461, mean_q: -41.739693\n"," 193794/300000: episode: 969, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.710031, mae: 28.422422, mean_q: -41.819843\n"," 193994/300000: episode: 970, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.645009, mae: 28.513330, mean_q: -41.933205\n"," 194194/300000: episode: 971, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 2.229733, mae: 28.194574, mean_q: -41.557480\n"," 194394/300000: episode: 972, duration: 1.878s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 2.976086, mae: 28.266804, mean_q: -41.521629\n"," 194594/300000: episode: 973, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.881054, mae: 28.281471, mean_q: -41.642834\n"," 194794/300000: episode: 974, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 2.433686, mae: 28.264381, mean_q: -41.604874\n"," 194994/300000: episode: 975, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 2.442069, mae: 28.076256, mean_q: -41.284946\n"," 195194/300000: episode: 976, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 2.073433, mae: 27.900991, mean_q: -41.057907\n"," 195394/300000: episode: 977, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 2.060875, mae: 28.185425, mean_q: -41.479683\n"," 195594/300000: episode: 978, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.791482, mae: 28.211084, mean_q: -41.501553\n"," 195794/300000: episode: 979, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.862901, mae: 28.191101, mean_q: -41.545265\n"," 195994/300000: episode: 980, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.981241, mae: 28.199577, mean_q: -41.418407\n"," 196194/300000: episode: 981, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.838469, mae: 27.923124, mean_q: -41.122715\n"," 196394/300000: episode: 982, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 1.598468, mae: 28.176138, mean_q: -41.469677\n"," 196594/300000: episode: 983, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.005415, mae: 28.101553, mean_q: -41.389839\n"," 196794/300000: episode: 984, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.689294, mae: 28.047041, mean_q: -41.292492\n"," 196994/300000: episode: 985, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 2.873737, mae: 28.194746, mean_q: -41.472076\n"," 197194/300000: episode: 986, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 2.044466, mae: 28.066015, mean_q: -41.281727\n"," 197394/300000: episode: 987, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.482337, mae: 28.202991, mean_q: -41.485416\n"," 197594/300000: episode: 988, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 2.730106, mae: 27.952454, mean_q: -41.064346\n"," 197794/300000: episode: 989, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 2.652548, mae: 27.828100, mean_q: -40.811073\n"," 197994/300000: episode: 990, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 2.138525, mae: 27.923489, mean_q: -41.075195\n"," 198194/300000: episode: 991, duration: 1.918s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.592943, mae: 27.834858, mean_q: -40.856045\n"," 198390/300000: episode: 992, duration: 1.867s, episode steps: 196, steps per second: 105, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.041 [0.000, 2.000],  loss: 2.235205, mae: 27.764847, mean_q: -40.767418\n"," 198590/300000: episode: 993, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 2.228169, mae: 27.596804, mean_q: -40.491451\n"," 198790/300000: episode: 994, duration: 1.957s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.696470, mae: 27.600069, mean_q: -40.502930\n"," 198990/300000: episode: 995, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.284509, mae: 27.576092, mean_q: -40.479847\n"," 199190/300000: episode: 996, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 1.803763, mae: 27.364307, mean_q: -40.199078\n"," 199390/300000: episode: 997, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.365577, mae: 27.376577, mean_q: -40.191769\n"," 199590/300000: episode: 998, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.936823, mae: 27.204206, mean_q: -39.909752\n"," 199790/300000: episode: 999, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 2.279646, mae: 27.298395, mean_q: -40.033634\n"," 199990/300000: episode: 1000, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.602933, mae: 26.940374, mean_q: -39.522537\n"," 200190/300000: episode: 1001, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.780557, mae: 26.920944, mean_q: -39.463280\n"," 200390/300000: episode: 1002, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.920065, mae: 26.887623, mean_q: -39.433483\n"," 200590/300000: episode: 1003, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.600217, mae: 26.760193, mean_q: -39.271183\n"," 200790/300000: episode: 1004, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.753343, mae: 26.675634, mean_q: -39.113026\n"," 200990/300000: episode: 1005, duration: 1.949s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 2.143833, mae: 26.666506, mean_q: -39.060326\n"," 201190/300000: episode: 1006, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.496668, mae: 26.568256, mean_q: -39.001270\n"," 201390/300000: episode: 1007, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.115415, mae: 26.713936, mean_q: -39.227688\n"," 201590/300000: episode: 1008, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.479514, mae: 26.447012, mean_q: -38.862415\n"," 201790/300000: episode: 1009, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.859247, mae: 26.487251, mean_q: -38.951965\n"," 201990/300000: episode: 1010, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 1.441518, mae: 26.370733, mean_q: -38.769512\n"," 202190/300000: episode: 1011, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.621843, mae: 26.197954, mean_q: -38.442455\n"," 202390/300000: episode: 1012, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 2.480847, mae: 26.267872, mean_q: -38.481712\n"," 202590/300000: episode: 1013, duration: 1.918s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.995958, mae: 26.275606, mean_q: -38.571484\n"," 202790/300000: episode: 1014, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.599620, mae: 26.098782, mean_q: -38.267578\n"," 202990/300000: episode: 1015, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.249500, mae: 26.271065, mean_q: -38.611713\n"," 203190/300000: episode: 1016, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.721769, mae: 26.256207, mean_q: -38.501297\n"," 203390/300000: episode: 1017, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.488590, mae: 26.012636, mean_q: -38.197769\n"," 203590/300000: episode: 1018, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 1.489133, mae: 26.015800, mean_q: -38.216568\n"," 203790/300000: episode: 1019, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.396363, mae: 26.150261, mean_q: -38.340836\n"," 203990/300000: episode: 1020, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.210968, mae: 25.907499, mean_q: -38.049572\n"," 204190/300000: episode: 1021, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.418082, mae: 26.071592, mean_q: -38.293037\n"," 204390/300000: episode: 1022, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.677452, mae: 25.943674, mean_q: -38.129662\n"," 204590/300000: episode: 1023, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.585021, mae: 25.859783, mean_q: -37.962509\n"," 204790/300000: episode: 1024, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 1.304810, mae: 25.906355, mean_q: -38.055161\n"," 204990/300000: episode: 1025, duration: 1.898s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 1.738930, mae: 25.889883, mean_q: -38.005726\n"," 205190/300000: episode: 1026, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.146820, mae: 26.037693, mean_q: -38.221699\n"," 205390/300000: episode: 1027, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.644665, mae: 25.797941, mean_q: -37.860695\n"," 205590/300000: episode: 1028, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.526827, mae: 25.866484, mean_q: -38.006428\n"," 205769/300000: episode: 1029, duration: 1.706s, episode steps: 179, steps per second: 105, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000],  loss: 1.902944, mae: 25.806032, mean_q: -37.882004\n"," 205969/300000: episode: 1030, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.434584, mae: 25.961687, mean_q: -38.080944\n"," 206169/300000: episode: 1031, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.504988, mae: 25.814228, mean_q: -37.880596\n"," 206369/300000: episode: 1032, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 1.732327, mae: 25.906343, mean_q: -38.011787\n"," 206569/300000: episode: 1033, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.889619, mae: 25.782940, mean_q: -37.853806\n"," 206769/300000: episode: 1034, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.558293, mae: 25.892401, mean_q: -38.003807\n"," 206969/300000: episode: 1035, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.247630, mae: 25.770937, mean_q: -37.735348\n"," 207169/300000: episode: 1036, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.261245, mae: 25.623377, mean_q: -37.616402\n"," 207369/300000: episode: 1037, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 1.265968, mae: 25.621723, mean_q: -37.672333\n"," 207569/300000: episode: 1038, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.228770, mae: 25.824369, mean_q: -37.944359\n"," 207769/300000: episode: 1039, duration: 1.824s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.328783, mae: 25.781572, mean_q: -37.906246\n"," 207969/300000: episode: 1040, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.268714, mae: 25.811455, mean_q: -37.811047\n"," 208169/300000: episode: 1041, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.845535, mae: 25.594114, mean_q: -37.517948\n"," 208369/300000: episode: 1042, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.132039, mae: 25.714422, mean_q: -37.800339\n"," 208569/300000: episode: 1043, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 1.595380, mae: 25.510679, mean_q: -37.454456\n"," 208769/300000: episode: 1044, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.762921, mae: 25.623070, mean_q: -37.608406\n"," 208969/300000: episode: 1045, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.307737, mae: 25.814215, mean_q: -37.875443\n"," 209169/300000: episode: 1046, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 2.038636, mae: 25.521278, mean_q: -37.434002\n"," 209369/300000: episode: 1047, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.442352, mae: 25.689425, mean_q: -37.672867\n"," 209569/300000: episode: 1048, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 1.368331, mae: 25.743418, mean_q: -37.789463\n"," 209769/300000: episode: 1049, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 1.202472, mae: 25.542242, mean_q: -37.508663\n"," 209969/300000: episode: 1050, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.528207, mae: 25.758144, mean_q: -37.840996\n"," 210169/300000: episode: 1051, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.087662, mae: 25.636484, mean_q: -37.637814\n"," 210369/300000: episode: 1052, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.636031, mae: 25.562084, mean_q: -37.472313\n"," 210569/300000: episode: 1053, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 1.527639, mae: 25.755396, mean_q: -37.801727\n"," 210769/300000: episode: 1054, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 1.107072, mae: 25.599550, mean_q: -37.635761\n"," 210969/300000: episode: 1055, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 1.407980, mae: 25.770903, mean_q: -37.806358\n"," 211169/300000: episode: 1056, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.454370, mae: 25.640913, mean_q: -37.685104\n"," 211369/300000: episode: 1057, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.361249, mae: 25.829224, mean_q: -37.930592\n"," 211569/300000: episode: 1058, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.201980, mae: 25.655371, mean_q: -37.692406\n"," 211769/300000: episode: 1059, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.678851, mae: 25.971191, mean_q: -38.134888\n"," 211969/300000: episode: 1060, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.560217, mae: 25.804974, mean_q: -37.860191\n"," 212169/300000: episode: 1061, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.462936, mae: 25.652552, mean_q: -37.603432\n"," 212369/300000: episode: 1062, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.729391, mae: 25.776445, mean_q: -37.862568\n"," 212569/300000: episode: 1063, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 2.089409, mae: 25.605785, mean_q: -37.556931\n"," 212769/300000: episode: 1064, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.520947, mae: 25.683006, mean_q: -37.686508\n"," 212969/300000: episode: 1065, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.535701, mae: 25.922016, mean_q: -38.034622\n"," 213169/300000: episode: 1066, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 1.073266, mae: 25.667068, mean_q: -37.683205\n"," 213369/300000: episode: 1067, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.622073, mae: 25.588696, mean_q: -37.510715\n"," 213569/300000: episode: 1068, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.165303, mae: 25.512119, mean_q: -37.436718\n"," 213769/300000: episode: 1069, duration: 1.973s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.700953, mae: 25.581976, mean_q: -37.507687\n"," 213969/300000: episode: 1070, duration: 1.944s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.831082, mae: 25.548367, mean_q: -37.499115\n"," 214169/300000: episode: 1071, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.171262, mae: 25.691774, mean_q: -37.699589\n"," 214369/300000: episode: 1072, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.979218, mae: 25.545158, mean_q: -37.509243\n"," 214569/300000: episode: 1073, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.520281, mae: 25.629814, mean_q: -37.590355\n"," 214769/300000: episode: 1074, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 1.295957, mae: 25.583813, mean_q: -37.559742\n"," 214969/300000: episode: 1075, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.413242, mae: 25.518799, mean_q: -37.443062\n"," 215158/300000: episode: 1076, duration: 1.707s, episode steps: 189, steps per second: 111, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.016 [0.000, 2.000],  loss: 1.583814, mae: 25.493134, mean_q: -37.362484\n"," 215358/300000: episode: 1077, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.295216, mae: 25.645636, mean_q: -37.660175\n"," 215558/300000: episode: 1078, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.746484, mae: 25.716459, mean_q: -37.806858\n"," 215758/300000: episode: 1079, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 1.277882, mae: 25.729282, mean_q: -37.749813\n"," 215958/300000: episode: 1080, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.535747, mae: 25.637190, mean_q: -37.600021\n"," 216158/300000: episode: 1081, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 1.493219, mae: 25.575779, mean_q: -37.546898\n"," 216358/300000: episode: 1082, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.982247, mae: 25.789873, mean_q: -37.911060\n"," 216558/300000: episode: 1083, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.893805, mae: 25.641539, mean_q: -37.667538\n"," 216758/300000: episode: 1084, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.381293, mae: 25.731678, mean_q: -37.754093\n"," 216958/300000: episode: 1085, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 1.510761, mae: 25.726299, mean_q: -37.755154\n"," 217158/300000: episode: 1086, duration: 1.932s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.463031, mae: 25.712769, mean_q: -37.749088\n"," 217358/300000: episode: 1087, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.804137, mae: 25.623631, mean_q: -37.549793\n"," 217558/300000: episode: 1088, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.575207, mae: 25.673603, mean_q: -37.652637\n"," 217758/300000: episode: 1089, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.478932, mae: 25.649990, mean_q: -37.558571\n"," 217958/300000: episode: 1090, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.583228, mae: 25.670073, mean_q: -37.616253\n"," 218158/300000: episode: 1091, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.992628, mae: 25.518068, mean_q: -37.398457\n"," 218358/300000: episode: 1092, duration: 1.938s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 1.056035, mae: 25.521500, mean_q: -37.445255\n"," 218551/300000: episode: 1093, duration: 1.843s, episode steps: 193, steps per second: 105, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.047 [0.000, 2.000],  loss: 1.015953, mae: 25.729885, mean_q: -37.711082\n"," 218751/300000: episode: 1094, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.066342, mae: 25.469517, mean_q: -37.363785\n"," 218951/300000: episode: 1095, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.262876, mae: 25.354956, mean_q: -37.175552\n"," 219151/300000: episode: 1096, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.090784, mae: 25.383940, mean_q: -37.231136\n"," 219351/300000: episode: 1097, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 1.061319, mae: 25.605169, mean_q: -37.593369\n"," 219551/300000: episode: 1098, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.068287, mae: 25.627329, mean_q: -37.604538\n"," 219751/300000: episode: 1099, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.193454, mae: 25.546181, mean_q: -37.448288\n"," 219951/300000: episode: 1100, duration: 1.973s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 1.023774, mae: 25.303562, mean_q: -37.085278\n"," 220151/300000: episode: 1101, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.863849, mae: 25.331497, mean_q: -37.148392\n"," 220351/300000: episode: 1102, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.081575, mae: 25.353125, mean_q: -37.145821\n"," 220551/300000: episode: 1103, duration: 1.887s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.886029, mae: 25.342197, mean_q: -37.218014\n"," 220751/300000: episode: 1104, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.602668, mae: 25.555420, mean_q: -37.468678\n"," 220948/300000: episode: 1105, duration: 1.785s, episode steps: 197, steps per second: 110, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000],  loss: 0.683059, mae: 25.495686, mean_q: -37.453835\n"," 221148/300000: episode: 1106, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 0.814620, mae: 25.395435, mean_q: -37.280941\n"," 221310/300000: episode: 1107, duration: 1.469s, episode steps: 162, steps per second: 110, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.907 [0.000, 2.000],  loss: 1.119907, mae: 25.355402, mean_q: -37.200401\n"," 221510/300000: episode: 1108, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.889059, mae: 25.456823, mean_q: -37.344166\n"," 221710/300000: episode: 1109, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.093026, mae: 25.651575, mean_q: -37.628990\n"," 221910/300000: episode: 1110, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.303068, mae: 25.448204, mean_q: -37.294827\n"," 222110/300000: episode: 1111, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.815489, mae: 25.647627, mean_q: -37.647881\n"," 222310/300000: episode: 1112, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 1.018925, mae: 25.799038, mean_q: -37.854607\n"," 222510/300000: episode: 1113, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.515876, mae: 25.641382, mean_q: -37.531471\n"," 222710/300000: episode: 1114, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 1.058263, mae: 25.636660, mean_q: -37.585140\n"," 222910/300000: episode: 1115, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.187112, mae: 25.690903, mean_q: -37.645523\n"," 223110/300000: episode: 1116, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.770 [0.000, 2.000],  loss: 1.543728, mae: 25.461031, mean_q: -37.295414\n"," 223288/300000: episode: 1117, duration: 1.726s, episode steps: 178, steps per second: 103, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.989 [0.000, 2.000],  loss: 1.284052, mae: 25.495167, mean_q: -37.364838\n"," 223488/300000: episode: 1118, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.895228, mae: 25.333040, mean_q: -37.182480\n"," 223688/300000: episode: 1119, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000],  loss: 1.041137, mae: 25.542971, mean_q: -37.475651\n"," 223888/300000: episode: 1120, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.082301, mae: 25.503740, mean_q: -37.497421\n"," 224088/300000: episode: 1121, duration: 2.000s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.039845, mae: 25.516567, mean_q: -37.465717\n"," 224288/300000: episode: 1122, duration: 1.981s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.321314, mae: 25.554106, mean_q: -37.490204\n"," 224488/300000: episode: 1123, duration: 1.986s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.044833, mae: 25.511734, mean_q: -37.431797\n"," 224688/300000: episode: 1124, duration: 2.002s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.409236, mae: 25.479677, mean_q: -37.400177\n"," 224888/300000: episode: 1125, duration: 1.914s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.786975, mae: 25.705708, mean_q: -37.788261\n"," 225088/300000: episode: 1126, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.917830, mae: 25.793612, mean_q: -37.899998\n"," 225288/300000: episode: 1127, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.413374, mae: 25.606617, mean_q: -37.597485\n"," 225488/300000: episode: 1128, duration: 1.994s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.051074, mae: 25.790640, mean_q: -37.912251\n"," 225688/300000: episode: 1129, duration: 1.944s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.703402, mae: 25.838467, mean_q: -37.964767\n"," 225888/300000: episode: 1130, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.353280, mae: 25.793285, mean_q: -37.923336\n"," 226088/300000: episode: 1131, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.194285, mae: 25.775826, mean_q: -37.882603\n"," 226288/300000: episode: 1132, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.145732, mae: 25.935127, mean_q: -38.120800\n"," 226488/300000: episode: 1133, duration: 1.983s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 1.246466, mae: 26.031061, mean_q: -38.199963\n"," 226688/300000: episode: 1134, duration: 1.974s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 1.373785, mae: 26.012344, mean_q: -38.239468\n"," 226888/300000: episode: 1135, duration: 1.970s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.196380, mae: 26.055376, mean_q: -38.288956\n"," 227088/300000: episode: 1136, duration: 1.986s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.976568, mae: 26.143744, mean_q: -38.470139\n"," 227288/300000: episode: 1137, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 1.067353, mae: 26.290627, mean_q: -38.671528\n"," 227488/300000: episode: 1138, duration: 2.017s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 1.392751, mae: 26.280106, mean_q: -38.585140\n"," 227688/300000: episode: 1139, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.332348, mae: 26.504332, mean_q: -38.978539\n"," 227888/300000: episode: 1140, duration: 1.904s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.995095, mae: 26.536781, mean_q: -38.980888\n"," 228088/300000: episode: 1141, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 1.058058, mae: 26.470713, mean_q: -38.912674\n"," 228288/300000: episode: 1142, duration: 1.831s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 1.582462, mae: 26.598207, mean_q: -39.034882\n"," 228488/300000: episode: 1143, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 1.015058, mae: 26.492714, mean_q: -38.947662\n"," 228688/300000: episode: 1144, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.914687, mae: 26.489990, mean_q: -38.825325\n"," 228888/300000: episode: 1145, duration: 1.812s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 1.353954, mae: 26.387363, mean_q: -38.757305\n"," 229088/300000: episode: 1146, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.881064, mae: 26.476151, mean_q: -38.914730\n"," 229288/300000: episode: 1147, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.497246, mae: 26.603687, mean_q: -39.058926\n"," 229488/300000: episode: 1148, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.441229, mae: 26.494967, mean_q: -38.885719\n"," 229688/300000: episode: 1149, duration: 1.911s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 1.263669, mae: 26.574419, mean_q: -39.071545\n"," 229888/300000: episode: 1150, duration: 1.910s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.858912, mae: 26.537266, mean_q: -39.011272\n"," 230088/300000: episode: 1151, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.439865, mae: 26.656544, mean_q: -39.198994\n"," 230288/300000: episode: 1152, duration: 1.961s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 1.516629, mae: 26.811207, mean_q: -39.410343\n"," 230488/300000: episode: 1153, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 1.393626, mae: 26.615444, mean_q: -39.067570\n"," 230688/300000: episode: 1154, duration: 1.989s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.241387, mae: 26.646450, mean_q: -39.191856\n"," 230888/300000: episode: 1155, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.136357, mae: 26.991055, mean_q: -39.699413\n"," 231088/300000: episode: 1156, duration: 1.955s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.643728, mae: 26.736025, mean_q: -39.284382\n"," 231288/300000: episode: 1157, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.371703, mae: 26.959011, mean_q: -39.620441\n"," 231488/300000: episode: 1158, duration: 1.916s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.261807, mae: 26.800894, mean_q: -39.367298\n"," 231688/300000: episode: 1159, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 1.588412, mae: 26.878407, mean_q: -39.459400\n"," 231888/300000: episode: 1160, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.808833, mae: 26.938616, mean_q: -39.428558\n"," 232058/300000: episode: 1161, duration: 1.645s, episode steps: 170, steps per second: 103, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.029 [0.000, 2.000],  loss: 1.415153, mae: 26.570164, mean_q: -38.994820\n"," 232258/300000: episode: 1162, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 1.250461, mae: 26.583715, mean_q: -39.032803\n"," 232458/300000: episode: 1163, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.867084, mae: 26.622208, mean_q: -39.110233\n"," 232658/300000: episode: 1164, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.897958, mae: 26.713003, mean_q: -39.221489\n"," 232858/300000: episode: 1165, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.425735, mae: 26.641108, mean_q: -39.066769\n"," 233058/300000: episode: 1166, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 1.758945, mae: 26.349066, mean_q: -38.629673\n"," 233258/300000: episode: 1167, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.045702, mae: 26.364370, mean_q: -38.720783\n"," 233458/300000: episode: 1168, duration: 1.944s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.034435, mae: 26.326632, mean_q: -38.651649\n"," 233658/300000: episode: 1169, duration: 1.970s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.405214, mae: 26.390881, mean_q: -38.729076\n"," 233858/300000: episode: 1170, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.698622, mae: 26.371922, mean_q: -38.621864\n"," 234058/300000: episode: 1171, duration: 1.994s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.323488, mae: 26.405457, mean_q: -38.747677\n"," 234258/300000: episode: 1172, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.085437, mae: 26.240362, mean_q: -38.475868\n"," 234458/300000: episode: 1173, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.905478, mae: 26.198315, mean_q: -38.468876\n"," 234658/300000: episode: 1174, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.216854, mae: 26.243374, mean_q: -38.534180\n"," 234858/300000: episode: 1175, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.950989, mae: 26.171598, mean_q: -38.454769\n"," 235058/300000: episode: 1176, duration: 1.931s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.091623, mae: 26.268579, mean_q: -38.567429\n"," 235258/300000: episode: 1177, duration: 2.004s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.699856, mae: 26.115150, mean_q: -38.356003\n"," 235458/300000: episode: 1178, duration: 1.976s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.109571, mae: 26.085400, mean_q: -38.275284\n"," 235658/300000: episode: 1179, duration: 1.970s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.950655, mae: 26.365185, mean_q: -38.687057\n"," 235858/300000: episode: 1180, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.210282, mae: 26.081413, mean_q: -38.286572\n"," 236058/300000: episode: 1181, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.088962, mae: 26.183666, mean_q: -38.424706\n"," 236258/300000: episode: 1182, duration: 1.926s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.155518, mae: 26.193253, mean_q: -38.431305\n"," 236458/300000: episode: 1183, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 1.087480, mae: 26.337915, mean_q: -38.626003\n"," 236658/300000: episode: 1184, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 1.118297, mae: 26.061190, mean_q: -38.209225\n"," 236858/300000: episode: 1185, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.896300, mae: 26.148201, mean_q: -38.356895\n"," 237058/300000: episode: 1186, duration: 1.974s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.603383, mae: 26.333782, mean_q: -38.659332\n"," 237258/300000: episode: 1187, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 1.294119, mae: 26.250704, mean_q: -38.467949\n"," 237458/300000: episode: 1188, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.726123, mae: 25.799171, mean_q: -37.876472\n"," 237658/300000: episode: 1189, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.736356, mae: 25.956890, mean_q: -38.066463\n"," 237857/300000: episode: 1190, duration: 1.901s, episode steps: 199, steps per second: 105, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.783670, mae: 26.178581, mean_q: -38.422134\n"," 238051/300000: episode: 1191, duration: 1.839s, episode steps: 194, steps per second: 106, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.119 [0.000, 2.000],  loss: 0.856506, mae: 25.751368, mean_q: -37.769657\n"," 238251/300000: episode: 1192, duration: 1.916s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.981887, mae: 25.907831, mean_q: -37.946911\n"," 238451/300000: episode: 1193, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.226433, mae: 25.930092, mean_q: -38.021362\n"," 238651/300000: episode: 1194, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.285516, mae: 26.184311, mean_q: -38.312321\n"," 238851/300000: episode: 1195, duration: 1.965s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.957417, mae: 25.991426, mean_q: -38.071396\n"," 239051/300000: episode: 1196, duration: 1.979s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.249357, mae: 25.844698, mean_q: -37.838123\n"," 239214/300000: episode: 1197, duration: 1.598s, episode steps: 163, steps per second: 102, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.080443, mae: 25.849588, mean_q: -37.850174\n"," 239414/300000: episode: 1198, duration: 1.950s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.734388, mae: 25.813711, mean_q: -37.879902\n"," 239614/300000: episode: 1199, duration: 1.952s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.717447, mae: 25.876654, mean_q: -38.011829\n"," 239814/300000: episode: 1200, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.722366, mae: 25.874086, mean_q: -37.957588\n"," 239999/300000: episode: 1201, duration: 1.815s, episode steps: 185, steps per second: 102, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.043 [0.000, 2.000],  loss: 0.569282, mae: 25.941730, mean_q: -38.105274\n"," 240199/300000: episode: 1202, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.078192, mae: 25.760757, mean_q: -37.757835\n"," 240399/300000: episode: 1203, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.842169, mae: 25.747768, mean_q: -37.775352\n"," 240599/300000: episode: 1204, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.959325, mae: 25.764702, mean_q: -37.759106\n"," 240767/300000: episode: 1205, duration: 1.607s, episode steps: 168, steps per second: 105, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 1.177806, mae: 25.858688, mean_q: -37.913605\n"," 240967/300000: episode: 1206, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.995044, mae: 25.851303, mean_q: -37.882370\n"," 241134/300000: episode: 1207, duration: 1.594s, episode steps: 167, steps per second: 105, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.126 [0.000, 2.000],  loss: 1.243740, mae: 25.913431, mean_q: -37.934044\n"," 241334/300000: episode: 1208, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.988963, mae: 25.588711, mean_q: -37.504581\n"," 241534/300000: episode: 1209, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.051440, mae: 25.957548, mean_q: -38.049488\n"," 241734/300000: episode: 1210, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.696584, mae: 25.673113, mean_q: -37.670094\n"," 241934/300000: episode: 1211, duration: 1.950s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.659060, mae: 25.743950, mean_q: -37.775536\n"," 242134/300000: episode: 1212, duration: 1.921s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.788222, mae: 25.623718, mean_q: -37.601002\n"," 242334/300000: episode: 1213, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.328630, mae: 25.849468, mean_q: -37.874722\n"," 242534/300000: episode: 1214, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 1.400193, mae: 25.793669, mean_q: -37.778053\n"," 242734/300000: episode: 1215, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.924645, mae: 25.682442, mean_q: -37.645782\n"," 242934/300000: episode: 1216, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.445841, mae: 26.007086, mean_q: -38.190907\n"," 243134/300000: episode: 1217, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.144948, mae: 25.956196, mean_q: -38.047359\n"," 243334/300000: episode: 1218, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 1.014495, mae: 25.743584, mean_q: -37.739777\n"," 243534/300000: episode: 1219, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.878858, mae: 25.930134, mean_q: -38.039253\n"," 243734/300000: episode: 1220, duration: 1.818s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.672660, mae: 25.682301, mean_q: -37.676968\n"," 243934/300000: episode: 1221, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.589801, mae: 25.685406, mean_q: -37.705505\n"," 244098/300000: episode: 1222, duration: 1.493s, episode steps: 164, steps per second: 110, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.986010, mae: 25.673363, mean_q: -37.606888\n"," 244298/300000: episode: 1223, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.249227, mae: 25.943560, mean_q: -38.007862\n"," 244498/300000: episode: 1224, duration: 1.799s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 1.314779, mae: 25.667173, mean_q: -37.559162\n"," 244698/300000: episode: 1225, duration: 1.811s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 1.210764, mae: 25.644041, mean_q: -37.577492\n"," 244898/300000: episode: 1226, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.918664, mae: 25.510654, mean_q: -37.376011\n"," 245098/300000: episode: 1227, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.074263, mae: 25.728409, mean_q: -37.679798\n"," 245297/300000: episode: 1228, duration: 1.866s, episode steps: 199, steps per second: 107, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.818006, mae: 25.416164, mean_q: -37.227112\n"," 245497/300000: episode: 1229, duration: 1.902s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.710544, mae: 25.418364, mean_q: -37.255550\n"," 245697/300000: episode: 1230, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.024180, mae: 25.285957, mean_q: -37.042660\n"," 245897/300000: episode: 1231, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.785011, mae: 25.483232, mean_q: -37.351719\n"," 246097/300000: episode: 1232, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.709030, mae: 25.471403, mean_q: -37.371056\n"," 246297/300000: episode: 1233, duration: 1.886s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.865133, mae: 25.498150, mean_q: -37.399597\n"," 246497/300000: episode: 1234, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.721276, mae: 25.657749, mean_q: -37.665268\n"," 246697/300000: episode: 1235, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.300514, mae: 25.563765, mean_q: -37.460854\n"," 246897/300000: episode: 1236, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.537753, mae: 25.581490, mean_q: -37.532806\n"," 247097/300000: episode: 1237, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.511345, mae: 25.736012, mean_q: -37.801666\n"," 247269/300000: episode: 1238, duration: 1.618s, episode steps: 172, steps per second: 106, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 1.034412, mae: 25.655773, mean_q: -37.643452\n"," 247469/300000: episode: 1239, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.728589, mae: 25.826431, mean_q: -37.923058\n"," 247669/300000: episode: 1240, duration: 1.929s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 1.222593, mae: 25.594509, mean_q: -37.491711\n"," 247834/300000: episode: 1241, duration: 1.574s, episode steps: 165, steps per second: 105, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.605546, mae: 25.527462, mean_q: -37.451115\n"," 248034/300000: episode: 1242, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.892331, mae: 25.466726, mean_q: -37.358799\n"," 248234/300000: episode: 1243, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.187886, mae: 25.572851, mean_q: -37.422909\n"," 248434/300000: episode: 1244, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.004663, mae: 25.327721, mean_q: -37.085278\n"," 248634/300000: episode: 1245, duration: 1.812s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.133802, mae: 25.811607, mean_q: -37.795609\n"," 248822/300000: episode: 1246, duration: 1.690s, episode steps: 188, steps per second: 111, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.995457, mae: 25.568396, mean_q: -37.414455\n"," 249022/300000: episode: 1247, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.064798, mae: 25.391184, mean_q: -37.213753\n"," 249222/300000: episode: 1248, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.608187, mae: 25.494207, mean_q: -37.385395\n"," 249422/300000: episode: 1249, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.813255, mae: 25.377522, mean_q: -37.223133\n"," 249622/300000: episode: 1250, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.706819, mae: 25.464218, mean_q: -37.358074\n"," 249814/300000: episode: 1251, duration: 1.758s, episode steps: 192, steps per second: 109, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.172 [0.000, 2.000],  loss: 0.941238, mae: 25.468992, mean_q: -37.320019\n"," 250014/300000: episode: 1252, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.787264, mae: 25.500305, mean_q: -37.394821\n"," 250214/300000: episode: 1253, duration: 1.798s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.350934, mae: 25.378395, mean_q: -37.130150\n"," 250414/300000: episode: 1254, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.084927, mae: 25.373369, mean_q: -37.146473\n"," 250614/300000: episode: 1255, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.787017, mae: 25.411812, mean_q: -37.240368\n"," 250814/300000: episode: 1256, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.772948, mae: 25.476080, mean_q: -37.369564\n"," 250995/300000: episode: 1257, duration: 1.666s, episode steps: 181, steps per second: 109, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.077 [0.000, 2.000],  loss: 0.759502, mae: 25.548267, mean_q: -37.492310\n"," 251195/300000: episode: 1258, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.926070, mae: 25.607935, mean_q: -37.556351\n"," 251395/300000: episode: 1259, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.895882, mae: 25.459810, mean_q: -37.372211\n"," 251595/300000: episode: 1260, duration: 1.866s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.877387, mae: 25.673681, mean_q: -37.681797\n"," 251795/300000: episode: 1261, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.840733, mae: 25.734699, mean_q: -37.781513\n"," 251995/300000: episode: 1262, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.779736, mae: 25.712238, mean_q: -37.755833\n"," 252195/300000: episode: 1263, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.957565, mae: 25.766973, mean_q: -37.786686\n"," 252395/300000: episode: 1264, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.126747, mae: 25.725796, mean_q: -37.717197\n"," 252595/300000: episode: 1265, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.877858, mae: 25.732407, mean_q: -37.753433\n"," 252795/300000: episode: 1266, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.805156, mae: 25.721064, mean_q: -37.739834\n"," 252995/300000: episode: 1267, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.356392, mae: 25.726562, mean_q: -37.683006\n"," 253195/300000: episode: 1268, duration: 1.945s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.102820, mae: 25.527187, mean_q: -37.432301\n"," 253395/300000: episode: 1269, duration: 1.902s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.683128, mae: 25.899223, mean_q: -38.004478\n"," 253593/300000: episode: 1270, duration: 1.886s, episode steps: 198, steps per second: 105, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000],  loss: 0.855897, mae: 25.784851, mean_q: -37.841541\n"," 253793/300000: episode: 1271, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.813365, mae: 25.838682, mean_q: -37.909168\n"," 253993/300000: episode: 1272, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.497367, mae: 25.527050, mean_q: -37.466358\n"," 254193/300000: episode: 1273, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.823035, mae: 25.814878, mean_q: -37.865288\n"," 254388/300000: episode: 1274, duration: 1.835s, episode steps: 195, steps per second: 106, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.226 [0.000, 2.000],  loss: 0.607912, mae: 25.754793, mean_q: -37.807220\n"," 254588/300000: episode: 1275, duration: 1.902s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.807390, mae: 25.616140, mean_q: -37.579468\n"," 254788/300000: episode: 1276, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.873477, mae: 25.851374, mean_q: -37.933681\n"," 254988/300000: episode: 1277, duration: 1.848s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.649913, mae: 25.951502, mean_q: -38.074589\n"," 255188/300000: episode: 1278, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.854212, mae: 25.873108, mean_q: -37.917465\n"," 255388/300000: episode: 1279, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.681344, mae: 26.005487, mean_q: -38.207832\n"," 255588/300000: episode: 1280, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.695535, mae: 26.125645, mean_q: -38.375629\n"," 255773/300000: episode: 1281, duration: 1.705s, episode steps: 185, steps per second: 109, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.092 [0.000, 2.000],  loss: 1.098456, mae: 26.189056, mean_q: -38.390518\n"," 255973/300000: episode: 1282, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.834829, mae: 26.033354, mean_q: -38.214306\n"," 256173/300000: episode: 1283, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.227514, mae: 26.238531, mean_q: -38.460140\n"," 256373/300000: episode: 1284, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.704184, mae: 26.012758, mean_q: -38.195377\n"," 256571/300000: episode: 1285, duration: 1.888s, episode steps: 198, steps per second: 105, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000],  loss: 1.041839, mae: 25.814405, mean_q: -37.823795\n"," 256771/300000: episode: 1286, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.891936, mae: 25.675756, mean_q: -37.652283\n"," 256971/300000: episode: 1287, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.882434, mae: 25.920298, mean_q: -38.025143\n"," 257171/300000: episode: 1288, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.771745, mae: 26.023169, mean_q: -38.217941\n"," 257371/300000: episode: 1289, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.043233, mae: 26.062128, mean_q: -38.243851\n"," 257571/300000: episode: 1290, duration: 1.950s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.923607, mae: 25.922226, mean_q: -38.017639\n"," 257771/300000: episode: 1291, duration: 1.925s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.941506, mae: 26.106195, mean_q: -38.317562\n"," 257942/300000: episode: 1292, duration: 1.660s, episode steps: 171, steps per second: 103, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000],  loss: 1.553000, mae: 25.981783, mean_q: -38.022724\n"," 258142/300000: episode: 1293, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.850849, mae: 26.217262, mean_q: -38.480686\n"," 258342/300000: episode: 1294, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.022355, mae: 26.120464, mean_q: -38.311619\n"," 258542/300000: episode: 1295, duration: 1.962s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.759560, mae: 26.182781, mean_q: -38.471943\n"," 258742/300000: episode: 1296, duration: 1.878s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.798054, mae: 26.237507, mean_q: -38.487438\n"," 258942/300000: episode: 1297, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.961995, mae: 26.254770, mean_q: -38.532055\n"," 259142/300000: episode: 1298, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.759334, mae: 26.194260, mean_q: -38.457565\n"," 259342/300000: episode: 1299, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.939625, mae: 25.947090, mean_q: -38.076405\n"," 259542/300000: episode: 1300, duration: 1.811s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.963330, mae: 26.247736, mean_q: -38.522972\n"," 259742/300000: episode: 1301, duration: 1.798s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.868272, mae: 26.080700, mean_q: -38.268356\n"," 259942/300000: episode: 1302, duration: 1.825s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.868452, mae: 26.037987, mean_q: -38.203609\n"," 260142/300000: episode: 1303, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.975535, mae: 25.994692, mean_q: -38.137249\n"," 260342/300000: episode: 1304, duration: 1.798s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.985756, mae: 26.137539, mean_q: -38.323387\n"," 260542/300000: episode: 1305, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 0.955080, mae: 26.058462, mean_q: -38.261948\n"," 260742/300000: episode: 1306, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.995893, mae: 25.986721, mean_q: -38.078583\n"," 260942/300000: episode: 1307, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.716075, mae: 25.894329, mean_q: -38.008888\n"," 261142/300000: episode: 1308, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.635494, mae: 25.900566, mean_q: -37.986488\n"," 261342/300000: episode: 1309, duration: 1.820s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.596438, mae: 26.194395, mean_q: -38.466827\n"," 261542/300000: episode: 1310, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.918735, mae: 26.178623, mean_q: -38.434826\n"," 261742/300000: episode: 1311, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.921185, mae: 26.275846, mean_q: -38.564972\n"," 261942/300000: episode: 1312, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.816065, mae: 26.353594, mean_q: -38.670166\n"," 262142/300000: episode: 1313, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.785272, mae: 26.172857, mean_q: -38.431763\n"," 262342/300000: episode: 1314, duration: 1.844s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.942904, mae: 26.227745, mean_q: -38.459789\n"," 262542/300000: episode: 1315, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.619014, mae: 26.452324, mean_q: -38.865990\n"," 262704/300000: episode: 1316, duration: 1.511s, episode steps: 162, steps per second: 107, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.957 [0.000, 2.000],  loss: 0.524955, mae: 26.219070, mean_q: -38.557579\n"," 262904/300000: episode: 1317, duration: 1.850s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.056393, mae: 26.460350, mean_q: -38.814156\n"," 263104/300000: episode: 1318, duration: 1.897s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.817683, mae: 26.419029, mean_q: -38.783161\n"," 263304/300000: episode: 1319, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 1.506446, mae: 26.621931, mean_q: -39.028530\n"," 263504/300000: episode: 1320, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.775610, mae: 26.612032, mean_q: -39.053974\n"," 263700/300000: episode: 1321, duration: 1.827s, episode steps: 196, steps per second: 107, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.077 [0.000, 2.000],  loss: 0.589184, mae: 26.393967, mean_q: -38.743946\n"," 263900/300000: episode: 1322, duration: 1.811s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.718883, mae: 26.532061, mean_q: -38.942642\n"," 264100/300000: episode: 1323, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.566838, mae: 26.346664, mean_q: -38.664482\n"," 264300/300000: episode: 1324, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.616548, mae: 26.249596, mean_q: -38.525742\n"," 264500/300000: episode: 1325, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.807607, mae: 26.397188, mean_q: -38.757992\n"," 264700/300000: episode: 1326, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 0.951918, mae: 26.486919, mean_q: -38.895462\n"," 264900/300000: episode: 1327, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.551604, mae: 26.409678, mean_q: -38.785194\n"," 265100/300000: episode: 1328, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 0.647086, mae: 26.511841, mean_q: -38.952831\n"," 265300/300000: episode: 1329, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.911802, mae: 26.398350, mean_q: -38.722561\n"," 265500/300000: episode: 1330, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.663468, mae: 26.571724, mean_q: -39.031303\n"," 265700/300000: episode: 1331, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.989233, mae: 26.533842, mean_q: -38.961464\n"," 265900/300000: episode: 1332, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.198990, mae: 26.661552, mean_q: -39.125416\n"," 266100/300000: episode: 1333, duration: 1.842s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.690759, mae: 26.704247, mean_q: -39.219585\n"," 266300/300000: episode: 1334, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.649962, mae: 26.467310, mean_q: -38.889862\n"," 266500/300000: episode: 1335, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.803693, mae: 26.634493, mean_q: -39.104538\n"," 266700/300000: episode: 1336, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.059571, mae: 26.655537, mean_q: -39.125397\n"," 266900/300000: episode: 1337, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.607412, mae: 26.659130, mean_q: -39.183887\n"," 267100/300000: episode: 1338, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.918411, mae: 26.700195, mean_q: -39.259678\n"," 267300/300000: episode: 1339, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.124514, mae: 26.798939, mean_q: -39.331459\n"," 267500/300000: episode: 1340, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.125593, mae: 26.986778, mean_q: -39.609554\n"," 267700/300000: episode: 1341, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.611806, mae: 26.802065, mean_q: -39.384838\n"," 267900/300000: episode: 1342, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.028868, mae: 26.777332, mean_q: -39.285091\n"," 268100/300000: episode: 1343, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.902806, mae: 26.723345, mean_q: -39.249672\n"," 268300/300000: episode: 1344, duration: 1.826s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.994295, mae: 26.734026, mean_q: -39.250156\n"," 268500/300000: episode: 1345, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.867199, mae: 26.867622, mean_q: -39.452023\n"," 268700/300000: episode: 1346, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.934940, mae: 26.762737, mean_q: -39.297989\n"," 268900/300000: episode: 1347, duration: 1.873s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.730068, mae: 26.854734, mean_q: -39.444977\n"," 269100/300000: episode: 1348, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.065475, mae: 26.708534, mean_q: -39.158886\n"," 269300/300000: episode: 1349, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.080158, mae: 26.575848, mean_q: -38.956329\n"," 269500/300000: episode: 1350, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.867332, mae: 26.770226, mean_q: -39.279831\n"," 269700/300000: episode: 1351, duration: 1.951s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.712970, mae: 26.843418, mean_q: -39.430645\n"," 269900/300000: episode: 1352, duration: 1.917s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.625725, mae: 26.738039, mean_q: -39.269238\n"," 270100/300000: episode: 1353, duration: 1.914s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.217834, mae: 26.874043, mean_q: -39.395351\n"," 270300/300000: episode: 1354, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000],  loss: 0.642092, mae: 26.871695, mean_q: -39.450436\n"," 270500/300000: episode: 1355, duration: 1.830s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.782721, mae: 26.773937, mean_q: -39.230354\n"," 270700/300000: episode: 1356, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.128159, mae: 26.822773, mean_q: -39.351093\n"," 270900/300000: episode: 1357, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.699517, mae: 26.741583, mean_q: -39.284016\n"," 271100/300000: episode: 1358, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.859365, mae: 26.954849, mean_q: -39.582214\n"," 271300/300000: episode: 1359, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.860579, mae: 26.825600, mean_q: -39.356606\n"," 271464/300000: episode: 1360, duration: 1.517s, episode steps: 164, steps per second: 108, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000],  loss: 0.941832, mae: 26.776966, mean_q: -39.269722\n"," 271664/300000: episode: 1361, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.884104, mae: 26.843311, mean_q: -39.387604\n"," 271864/300000: episode: 1362, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.880194, mae: 26.637690, mean_q: -39.071533\n"," 272064/300000: episode: 1363, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.757535, mae: 26.866079, mean_q: -39.416088\n"," 272261/300000: episode: 1364, duration: 1.789s, episode steps: 197, steps per second: 110, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.883542, mae: 26.754156, mean_q: -39.248291\n"," 272461/300000: episode: 1365, duration: 1.810s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.051436, mae: 26.771011, mean_q: -39.259056\n"," 272661/300000: episode: 1366, duration: 1.810s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.806926, mae: 26.626947, mean_q: -39.057877\n"," 272861/300000: episode: 1367, duration: 1.750s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.658777, mae: 26.864569, mean_q: -39.440994\n"," 273061/300000: episode: 1368, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.469007, mae: 26.755264, mean_q: -39.325619\n"," 273223/300000: episode: 1369, duration: 1.480s, episode steps: 162, steps per second: 109, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.117 [0.000, 2.000],  loss: 1.331325, mae: 26.799576, mean_q: -39.262070\n"," 273423/300000: episode: 1370, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.891872, mae: 26.870596, mean_q: -39.410164\n"," 273623/300000: episode: 1371, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.869278, mae: 26.603243, mean_q: -39.015522\n"," 273823/300000: episode: 1372, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.998602, mae: 26.843699, mean_q: -39.407516\n"," 274023/300000: episode: 1373, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.649608, mae: 26.780876, mean_q: -39.297157\n"," 274189/300000: episode: 1374, duration: 1.555s, episode steps: 166, steps per second: 107, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.102 [0.000, 2.000],  loss: 1.037011, mae: 26.773703, mean_q: -39.280464\n"," 274389/300000: episode: 1375, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.055532, mae: 26.622086, mean_q: -39.046177\n"," 274589/300000: episode: 1376, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.067230, mae: 26.929571, mean_q: -39.474640\n"," 274789/300000: episode: 1377, duration: 1.771s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.814175, mae: 26.800581, mean_q: -39.327011\n"," 274989/300000: episode: 1378, duration: 1.793s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.672831, mae: 26.745766, mean_q: -39.254967\n"," 275189/300000: episode: 1379, duration: 1.809s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.759269, mae: 26.438135, mean_q: -38.770515\n"," 275389/300000: episode: 1380, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.202092, mae: 26.703754, mean_q: -39.130615\n"," 275589/300000: episode: 1381, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 0.979105, mae: 26.412518, mean_q: -38.692616\n"," 275761/300000: episode: 1382, duration: 1.585s, episode steps: 172, steps per second: 109, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.505722, mae: 26.551878, mean_q: -38.962753\n"," 275951/300000: episode: 1383, duration: 1.727s, episode steps: 190, steps per second: 110, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.058 [0.000, 2.000],  loss: 0.562553, mae: 26.436148, mean_q: -38.770924\n"," 276151/300000: episode: 1384, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.758782, mae: 26.574848, mean_q: -38.973194\n"," 276323/300000: episode: 1385, duration: 1.587s, episode steps: 172, steps per second: 108, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.934001, mae: 26.286917, mean_q: -38.511837\n"," 276523/300000: episode: 1386, duration: 1.846s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.453859, mae: 26.459343, mean_q: -38.830914\n"," 276723/300000: episode: 1387, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.489374, mae: 26.253033, mean_q: -38.536991\n"," 276923/300000: episode: 1388, duration: 1.860s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.883455, mae: 26.324713, mean_q: -38.617245\n"," 277093/300000: episode: 1389, duration: 1.613s, episode steps: 170, steps per second: 105, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.565708, mae: 26.291430, mean_q: -38.581062\n"," 277293/300000: episode: 1390, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.666985, mae: 26.298948, mean_q: -38.595394\n"," 277493/300000: episode: 1391, duration: 1.896s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.742173, mae: 26.433382, mean_q: -38.778866\n"," 277693/300000: episode: 1392, duration: 1.860s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.990155, mae: 26.165382, mean_q: -38.322094\n"," 277893/300000: episode: 1393, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.995903, mae: 26.306190, mean_q: -38.550922\n"," 278093/300000: episode: 1394, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.606819, mae: 26.043760, mean_q: -38.172222\n"," 278293/300000: episode: 1395, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.576797, mae: 26.238035, mean_q: -38.511520\n"," 278493/300000: episode: 1396, duration: 1.858s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.992281, mae: 26.124434, mean_q: -38.267555\n"," 278693/300000: episode: 1397, duration: 1.836s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.652491, mae: 26.131126, mean_q: -38.324486\n"," 278893/300000: episode: 1398, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.154111, mae: 26.284536, mean_q: -38.496925\n"," 279093/300000: episode: 1399, duration: 1.878s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.861408, mae: 26.229849, mean_q: -38.438610\n"," 279292/300000: episode: 1400, duration: 1.865s, episode steps: 199, steps per second: 107, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.486037, mae: 26.137671, mean_q: -38.335258\n"," 279492/300000: episode: 1401, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.569384, mae: 26.056812, mean_q: -38.206772\n"," 279692/300000: episode: 1402, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.683378, mae: 25.929255, mean_q: -38.001358\n"," 279857/300000: episode: 1403, duration: 1.550s, episode steps: 165, steps per second: 106, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.079 [0.000, 2.000],  loss: 0.696117, mae: 25.888807, mean_q: -37.938923\n"," 280057/300000: episode: 1404, duration: 1.849s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.741312, mae: 26.044529, mean_q: -38.158508\n"," 280217/300000: episode: 1405, duration: 1.486s, episode steps: 160, steps per second: 108, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.903183, mae: 25.994757, mean_q: -38.071262\n"," 280392/300000: episode: 1406, duration: 1.643s, episode steps: 175, steps per second: 107, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.040305, mae: 26.075006, mean_q: -38.151203\n"," 280583/300000: episode: 1407, duration: 1.767s, episode steps: 191, steps per second: 108, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000],  loss: 0.634852, mae: 26.004972, mean_q: -38.088036\n"," 280738/300000: episode: 1408, duration: 1.425s, episode steps: 155, steps per second: 109, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.032 [0.000, 2.000],  loss: 0.350708, mae: 25.897234, mean_q: -37.976120\n"," 280938/300000: episode: 1409, duration: 1.843s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.639448, mae: 25.687433, mean_q: -37.636524\n"," 281132/300000: episode: 1410, duration: 1.755s, episode steps: 194, steps per second: 111, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000],  loss: 0.508406, mae: 25.606848, mean_q: -37.528893\n"," 281332/300000: episode: 1411, duration: 1.819s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.492988, mae: 25.802462, mean_q: -37.847149\n"," 281532/300000: episode: 1412, duration: 1.796s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.663400, mae: 25.865942, mean_q: -37.910660\n"," 281732/300000: episode: 1413, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.471438, mae: 25.753052, mean_q: -37.775589\n"," 281921/300000: episode: 1414, duration: 1.795s, episode steps: 189, steps per second: 105, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.069 [0.000, 2.000],  loss: 0.569914, mae: 25.719685, mean_q: -37.732754\n"," 282121/300000: episode: 1415, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.622141, mae: 26.001886, mean_q: -38.127460\n"," 282321/300000: episode: 1416, duration: 1.837s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.819451, mae: 25.605450, mean_q: -37.543083\n"," 282521/300000: episode: 1417, duration: 1.845s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.641229, mae: 25.804842, mean_q: -37.857475\n"," 282720/300000: episode: 1418, duration: 1.838s, episode steps: 199, steps per second: 108, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.762989, mae: 25.602125, mean_q: -37.512871\n"," 282920/300000: episode: 1419, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.937456, mae: 25.926903, mean_q: -38.003754\n"," 283120/300000: episode: 1420, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.393378, mae: 25.707844, mean_q: -37.747715\n"," 283320/300000: episode: 1421, duration: 1.853s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.175133, mae: 25.734503, mean_q: -37.655094\n"," 283520/300000: episode: 1422, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.863853, mae: 25.677919, mean_q: -37.627728\n"," 283720/300000: episode: 1423, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.406991, mae: 25.591562, mean_q: -37.542015\n"," 283920/300000: episode: 1424, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.865930, mae: 25.679375, mean_q: -37.633423\n"," 284120/300000: episode: 1425, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.751353, mae: 25.838291, mean_q: -37.875256\n"," 284315/300000: episode: 1426, duration: 1.786s, episode steps: 195, steps per second: 109, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.103 [0.000, 2.000],  loss: 0.428456, mae: 25.984520, mean_q: -38.129288\n"," 284479/300000: episode: 1427, duration: 1.487s, episode steps: 164, steps per second: 110, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.719594, mae: 25.574759, mean_q: -37.490757\n"," 284679/300000: episode: 1428, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.906120, mae: 25.833584, mean_q: -37.850380\n"," 284879/300000: episode: 1429, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.642209, mae: 25.833157, mean_q: -37.908092\n"," 285074/300000: episode: 1430, duration: 1.852s, episode steps: 195, steps per second: 105, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.123 [0.000, 2.000],  loss: 0.543564, mae: 25.917099, mean_q: -38.010891\n"," 285274/300000: episode: 1431, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.720564, mae: 25.872829, mean_q: -37.941402\n"," 285474/300000: episode: 1432, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 1.168717, mae: 25.972284, mean_q: -38.065426\n"," 285674/300000: episode: 1433, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.949717, mae: 25.943666, mean_q: -38.016811\n"," 285874/300000: episode: 1434, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.828448, mae: 25.969690, mean_q: -38.107338\n"," 286054/300000: episode: 1435, duration: 1.627s, episode steps: 180, steps per second: 111, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.995611, mae: 26.139540, mean_q: -38.303391\n"," 286254/300000: episode: 1436, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.640777, mae: 25.976669, mean_q: -38.105484\n"," 286454/300000: episode: 1437, duration: 1.852s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.173225, mae: 25.905140, mean_q: -37.937080\n"," 286630/300000: episode: 1438, duration: 1.665s, episode steps: 176, steps per second: 106, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.383266, mae: 25.856951, mean_q: -37.952755\n"," 286828/300000: episode: 1439, duration: 1.874s, episode steps: 198, steps per second: 106, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000],  loss: 0.757052, mae: 25.987329, mean_q: -38.099083\n"," 286994/300000: episode: 1440, duration: 1.523s, episode steps: 166, steps per second: 109, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.927907, mae: 25.988888, mean_q: -38.120617\n"," 287194/300000: episode: 1441, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.712811, mae: 25.942717, mean_q: -38.074688\n"," 287394/300000: episode: 1442, duration: 1.891s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.925474, mae: 25.921883, mean_q: -38.034103\n"," 287594/300000: episode: 1443, duration: 1.854s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.678395, mae: 25.997942, mean_q: -38.166618\n"," 287794/300000: episode: 1444, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.668077, mae: 26.207542, mean_q: -38.458721\n"," 287994/300000: episode: 1445, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 1.112686, mae: 26.273012, mean_q: -38.466984\n"," 288194/300000: episode: 1446, duration: 1.822s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.739819, mae: 26.253393, mean_q: -38.535179\n"," 288394/300000: episode: 1447, duration: 1.862s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.440451, mae: 26.227606, mean_q: -38.513683\n"," 288594/300000: episode: 1448, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.968702, mae: 26.143953, mean_q: -38.294624\n"," 288794/300000: episode: 1449, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.552723, mae: 26.084524, mean_q: -38.260677\n"," 288994/300000: episode: 1450, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.630803, mae: 26.156275, mean_q: -38.363663\n"," 289194/300000: episode: 1451, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.768568, mae: 26.290138, mean_q: -38.557339\n"," 289394/300000: episode: 1452, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.615453, mae: 26.207363, mean_q: -38.445446\n"," 289594/300000: episode: 1453, duration: 1.810s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.808848, mae: 26.340630, mean_q: -38.646473\n"," 289794/300000: episode: 1454, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.770828, mae: 26.292744, mean_q: -38.608963\n"," 289994/300000: episode: 1455, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.725191, mae: 26.336098, mean_q: -38.639355\n"," 290194/300000: episode: 1456, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.676339, mae: 26.122124, mean_q: -38.320072\n"," 290394/300000: episode: 1457, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.749772, mae: 26.052908, mean_q: -38.215496\n"," 290594/300000: episode: 1458, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.683012, mae: 26.303955, mean_q: -38.612720\n"," 290794/300000: episode: 1459, duration: 2.003s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.948671, mae: 26.783646, mean_q: -39.296230\n"," 290953/300000: episode: 1460, duration: 1.572s, episode steps: 159, steps per second: 101, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 1.051477, mae: 26.339973, mean_q: -38.614864\n"," 291153/300000: episode: 1461, duration: 1.994s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.799827, mae: 26.464022, mean_q: -38.844360\n"," 291353/300000: episode: 1462, duration: 1.906s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.715527, mae: 25.990318, mean_q: -38.117847\n"," 291553/300000: episode: 1463, duration: 1.965s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.901453, mae: 26.189060, mean_q: -38.401703\n"," 291753/300000: episode: 1464, duration: 1.973s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.876134, mae: 26.264122, mean_q: -38.531059\n"," 291953/300000: episode: 1465, duration: 1.935s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.971340, mae: 26.387178, mean_q: -38.705116\n"," 292153/300000: episode: 1466, duration: 1.951s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.614709, mae: 26.333843, mean_q: -38.666504\n"," 292353/300000: episode: 1467, duration: 1.941s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.614413, mae: 26.223536, mean_q: -38.513805\n"," 292545/300000: episode: 1468, duration: 1.848s, episode steps: 192, steps per second: 104, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000],  loss: 0.775314, mae: 26.255941, mean_q: -38.512550\n"," 292745/300000: episode: 1469, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.583277, mae: 26.206511, mean_q: -38.470772\n"," 292945/300000: episode: 1470, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.684136, mae: 26.128651, mean_q: -38.313984\n"," 293145/300000: episode: 1471, duration: 1.935s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 0.538544, mae: 26.292917, mean_q: -38.605431\n"," 293345/300000: episode: 1472, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.838236, mae: 26.243042, mean_q: -38.465004\n"," 293545/300000: episode: 1473, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.882500, mae: 26.116837, mean_q: -38.298744\n"," 293745/300000: episode: 1474, duration: 1.970s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.666997, mae: 26.321218, mean_q: -38.610664\n"," 293945/300000: episode: 1475, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.345181, mae: 26.211424, mean_q: -38.443432\n"," 294145/300000: episode: 1476, duration: 1.905s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.575153, mae: 26.402054, mean_q: -38.731949\n"," 294345/300000: episode: 1477, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.624937, mae: 26.246689, mean_q: -38.513222\n"," 294545/300000: episode: 1478, duration: 1.949s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.739383, mae: 26.379311, mean_q: -38.709248\n"," 294745/300000: episode: 1479, duration: 1.969s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.757182, mae: 26.365538, mean_q: -38.665760\n"," 294945/300000: episode: 1480, duration: 1.923s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.795632, mae: 26.271278, mean_q: -38.535019\n"," 295145/300000: episode: 1481, duration: 1.869s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.527954, mae: 26.265606, mean_q: -38.530052\n"," 295345/300000: episode: 1482, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.124686, mae: 26.315947, mean_q: -38.575417\n"," 295545/300000: episode: 1483, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 1.039070, mae: 26.134993, mean_q: -38.289013\n"," 295745/300000: episode: 1484, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.887040, mae: 26.375015, mean_q: -38.681602\n"," 295945/300000: episode: 1485, duration: 1.918s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.723912, mae: 26.354698, mean_q: -38.657494\n"," 296118/300000: episode: 1486, duration: 1.654s, episode steps: 173, steps per second: 105, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.092 [0.000, 2.000],  loss: 0.757849, mae: 26.399307, mean_q: -38.750568\n"," 296318/300000: episode: 1487, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.729308, mae: 26.389812, mean_q: -38.740288\n"," 296518/300000: episode: 1488, duration: 1.855s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.780017, mae: 26.204941, mean_q: -38.429657\n"," 296718/300000: episode: 1489, duration: 1.944s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.881071, mae: 26.346107, mean_q: -38.632618\n"," 296918/300000: episode: 1490, duration: 1.900s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.847112, mae: 26.220564, mean_q: -38.484238\n"," 297118/300000: episode: 1491, duration: 1.859s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.768737, mae: 26.150375, mean_q: -38.380253\n"," 297318/300000: episode: 1492, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.736523, mae: 26.259575, mean_q: -38.524143\n"," 297518/300000: episode: 1493, duration: 2.052s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.639033, mae: 26.449045, mean_q: -38.814831\n"," 297718/300000: episode: 1494, duration: 1.920s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.975768, mae: 26.204765, mean_q: -38.393208\n"," 297918/300000: episode: 1495, duration: 1.982s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.091232, mae: 26.192192, mean_q: -38.353004\n"," 298118/300000: episode: 1496, duration: 1.963s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.613336, mae: 26.230152, mean_q: -38.445473\n"," 298300/300000: episode: 1497, duration: 1.732s, episode steps: 182, steps per second: 105, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000],  loss: 0.478787, mae: 26.308777, mean_q: -38.633305\n"," 298500/300000: episode: 1498, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.438099, mae: 26.261982, mean_q: -38.552284\n"," 298700/300000: episode: 1499, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.437368, mae: 26.335152, mean_q: -38.646103\n"," 298900/300000: episode: 1500, duration: 1.941s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.662012, mae: 26.291990, mean_q: -38.534225\n"," 299100/300000: episode: 1501, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.904511, mae: 26.247368, mean_q: -38.488564\n"," 299300/300000: episode: 1502, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.826760, mae: 26.330793, mean_q: -38.593632\n"," 299500/300000: episode: 1503, duration: 1.954s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.520127, mae: 26.305838, mean_q: -38.621517\n"," 299700/300000: episode: 1504, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.520899, mae: 26.395908, mean_q: -38.754688\n"," 299900/300000: episode: 1505, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.678871, mae: 26.289021, mean_q: -38.594704\n","done, took 2800.492 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fV1i3vj2-YlV","executionInfo":{"status":"ok","timestamp":1620587939678,"user_tz":-60,"elapsed":6070924,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"831ba6d3-e5d2-496d-fb80-ac9f616e9b69"},"source":["model = Sequential()\n","model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n","model.add(Dense(128, activation=\"relu\"))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dense(32, activation=\"relu\"))\n","model.add(Dense(env.action_space.n, activation=\"linear\"))\n","\n","dqn = DQNAgent(\n","    model=model, \n","    nb_actions=env.action_space.n, \n","    memory=SequentialMemory(limit=50000, window_length=1), \n","    nb_steps_warmup=10,\n","    target_model_update=1e-2, \n","    policy=BoltzmannQPolicy(),\n","    enable_double_dqn=True)\n","\n","dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n","history_double = dqn.fit(env, nb_steps=300000, visualize=False, verbose=2)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Training for 300000 steps ...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  warnings.warn('`Model.state_updates` will be removed in a future version. '\n","/usr/local/lib/python3.7/dist-packages/rl/memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n","  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"],"name":"stderr"},{"output_type":"stream","text":["    200/300000: episode: 1, duration: 2.509s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.077266, mae: 0.829189, mean_q: -1.079105\n","    400/300000: episode: 2, duration: 1.948s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.006287, mae: 1.821189, mean_q: -2.685626\n","    600/300000: episode: 3, duration: 1.908s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.020334, mae: 2.941657, mean_q: -4.344371\n","    800/300000: episode: 4, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.039558, mae: 4.094830, mean_q: -6.060324\n","   1000/300000: episode: 5, duration: 1.989s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.094880, mae: 5.262471, mean_q: -7.763782\n","   1200/300000: episode: 6, duration: 1.953s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.170987, mae: 6.399232, mean_q: -9.446150\n","   1400/300000: episode: 7, duration: 1.975s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.192441, mae: 7.494828, mean_q: -11.092888\n","   1600/300000: episode: 8, duration: 2.046s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.239515, mae: 8.573443, mean_q: -12.688968\n","   1800/300000: episode: 9, duration: 2.014s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.484480, mae: 9.601322, mean_q: -14.175066\n","   2000/300000: episode: 10, duration: 2.027s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.445837, mae: 10.601826, mean_q: -15.696599\n","   2200/300000: episode: 11, duration: 2.007s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.578429, mae: 11.588234, mean_q: -17.104580\n","   2400/300000: episode: 12, duration: 1.989s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.852247, mae: 12.515650, mean_q: -18.519005\n","   2600/300000: episode: 13, duration: 2.030s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.954484, mae: 13.401155, mean_q: -19.777277\n","   2800/300000: episode: 14, duration: 2.092s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 1.084534, mae: 14.264936, mean_q: -21.053278\n","   3000/300000: episode: 15, duration: 2.019s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.379594, mae: 15.087607, mean_q: -22.257469\n","   3200/300000: episode: 16, duration: 2.009s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 1.053203, mae: 15.938842, mean_q: -23.572716\n","   3400/300000: episode: 17, duration: 1.969s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.195481, mae: 16.738096, mean_q: -24.748491\n","   3600/300000: episode: 18, duration: 1.959s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.629423, mae: 17.492413, mean_q: -25.833855\n","   3800/300000: episode: 19, duration: 2.023s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.887249, mae: 18.269922, mean_q: -26.955494\n","   4000/300000: episode: 20, duration: 2.048s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 2.234129, mae: 18.981184, mean_q: -28.033358\n","   4200/300000: episode: 21, duration: 1.976s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.802494, mae: 19.608572, mean_q: -28.947548\n","   4400/300000: episode: 22, duration: 1.964s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 2.251675, mae: 20.171865, mean_q: -29.827333\n","   4600/300000: episode: 23, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 2.391518, mae: 20.789698, mean_q: -30.780413\n","   4800/300000: episode: 24, duration: 1.941s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 3.462400, mae: 21.326731, mean_q: -31.437969\n","   5000/300000: episode: 25, duration: 1.940s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.389287, mae: 21.945332, mean_q: -32.519550\n","   5200/300000: episode: 26, duration: 2.041s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.400354, mae: 22.632256, mean_q: -33.533169\n","   5400/300000: episode: 27, duration: 2.082s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 3.551391, mae: 23.179642, mean_q: -34.292553\n","   5600/300000: episode: 28, duration: 2.032s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 3.504760, mae: 23.716768, mean_q: -35.034004\n","   5800/300000: episode: 29, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 2.367049, mae: 24.270527, mean_q: -35.949516\n","   6000/300000: episode: 30, duration: 2.073s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 4.198361, mae: 24.855503, mean_q: -36.690952\n","   6200/300000: episode: 31, duration: 2.083s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 3.820565, mae: 25.270090, mean_q: -37.363914\n","   6400/300000: episode: 32, duration: 2.021s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 4.734381, mae: 25.629753, mean_q: -37.790943\n","   6600/300000: episode: 33, duration: 2.003s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.280244, mae: 26.052118, mean_q: -38.566273\n","   6800/300000: episode: 34, duration: 1.941s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 4.169545, mae: 26.535498, mean_q: -39.263828\n","   7000/300000: episode: 35, duration: 1.933s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 4.850277, mae: 26.978880, mean_q: -39.848942\n","   7200/300000: episode: 36, duration: 1.919s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 4.037831, mae: 27.427769, mean_q: -40.726105\n","   7400/300000: episode: 37, duration: 1.918s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 4.398065, mae: 27.858126, mean_q: -41.246944\n","   7600/300000: episode: 38, duration: 2.005s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 4.378275, mae: 28.296255, mean_q: -41.810715\n","   7800/300000: episode: 39, duration: 2.011s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 4.596928, mae: 28.620546, mean_q: -42.377724\n","   8000/300000: episode: 40, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 4.518944, mae: 29.073292, mean_q: -42.987770\n","   8200/300000: episode: 41, duration: 2.101s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 3.180969, mae: 29.395226, mean_q: -43.571659\n","   8400/300000: episode: 42, duration: 2.074s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 5.447557, mae: 29.837757, mean_q: -44.170666\n","   8600/300000: episode: 43, duration: 2.046s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 4.875138, mae: 30.187500, mean_q: -44.710281\n","   8800/300000: episode: 44, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 4.408067, mae: 30.573467, mean_q: -45.328739\n","   9000/300000: episode: 45, duration: 2.093s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 4.924685, mae: 30.887730, mean_q: -45.702560\n","   9200/300000: episode: 46, duration: 2.066s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.261306, mae: 31.189514, mean_q: -46.159901\n","   9400/300000: episode: 47, duration: 2.064s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.576137, mae: 31.525740, mean_q: -46.817802\n","   9600/300000: episode: 48, duration: 2.054s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 4.758031, mae: 31.915199, mean_q: -47.252201\n","   9800/300000: episode: 49, duration: 2.060s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 6.297899, mae: 32.166954, mean_q: -47.636776\n","  10000/300000: episode: 50, duration: 2.048s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 5.686101, mae: 32.471195, mean_q: -47.918530\n","  10200/300000: episode: 51, duration: 2.037s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.416393, mae: 32.631786, mean_q: -48.280029\n","  10400/300000: episode: 52, duration: 2.116s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 6.769559, mae: 32.930824, mean_q: -48.761307\n","  10600/300000: episode: 53, duration: 2.129s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 4.344339, mae: 33.169479, mean_q: -49.283546\n","  10800/300000: episode: 54, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 5.870260, mae: 33.502808, mean_q: -49.592369\n","  11000/300000: episode: 55, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.318828, mae: 33.725933, mean_q: -49.927967\n","  11200/300000: episode: 56, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 5.232802, mae: 34.049141, mean_q: -50.498165\n","  11400/300000: episode: 57, duration: 2.106s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.761596, mae: 34.298809, mean_q: -50.692516\n","  11600/300000: episode: 58, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 5.482752, mae: 34.427807, mean_q: -51.002247\n","  11800/300000: episode: 59, duration: 2.101s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.169217, mae: 34.641125, mean_q: -51.206005\n","  12000/300000: episode: 60, duration: 2.086s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.213384, mae: 34.649715, mean_q: -51.249912\n","  12200/300000: episode: 61, duration: 2.089s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.254013, mae: 34.813438, mean_q: -51.430824\n","  12400/300000: episode: 62, duration: 2.021s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 7.900386, mae: 34.929230, mean_q: -51.685810\n","  12600/300000: episode: 63, duration: 2.080s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 6.163942, mae: 35.081070, mean_q: -51.952274\n","  12800/300000: episode: 64, duration: 2.076s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 7.757732, mae: 35.163403, mean_q: -52.028336\n","  13000/300000: episode: 65, duration: 2.051s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 5.213240, mae: 35.416210, mean_q: -52.660694\n","  13200/300000: episode: 66, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.975883, mae: 35.619148, mean_q: -52.743484\n","  13400/300000: episode: 67, duration: 2.072s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.673229, mae: 35.776257, mean_q: -52.962032\n","  13600/300000: episode: 68, duration: 2.065s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.813852, mae: 35.982914, mean_q: -53.288662\n","  13800/300000: episode: 69, duration: 2.007s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.173986, mae: 36.048801, mean_q: -53.261837\n","  14000/300000: episode: 70, duration: 2.052s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.278757, mae: 36.277939, mean_q: -53.741474\n","  14200/300000: episode: 71, duration: 2.042s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 6.399075, mae: 36.518036, mean_q: -54.158379\n","  14400/300000: episode: 72, duration: 2.083s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 5.831717, mae: 36.771912, mean_q: -54.618687\n","  14600/300000: episode: 73, duration: 2.100s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 4.912280, mae: 36.949783, mean_q: -54.825245\n","  14800/300000: episode: 74, duration: 2.075s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 6.649363, mae: 37.278542, mean_q: -55.343800\n","  15000/300000: episode: 75, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 6.158551, mae: 37.570301, mean_q: -55.685036\n","  15200/300000: episode: 76, duration: 2.085s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 7.495481, mae: 37.740280, mean_q: -55.929897\n","  15400/300000: episode: 77, duration: 2.091s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 6.975862, mae: 37.979713, mean_q: -56.354618\n","  15600/300000: episode: 78, duration: 2.056s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 7.776717, mae: 38.077404, mean_q: -56.440704\n","  15800/300000: episode: 79, duration: 2.064s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.149021, mae: 38.330887, mean_q: -56.870274\n","  16000/300000: episode: 80, duration: 2.036s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.213141, mae: 38.480614, mean_q: -57.045731\n","  16200/300000: episode: 81, duration: 2.076s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 6.368193, mae: 38.589577, mean_q: -57.278198\n","  16400/300000: episode: 82, duration: 2.113s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.618788, mae: 38.854584, mean_q: -57.518410\n","  16600/300000: episode: 83, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.485438, mae: 38.941456, mean_q: -57.722511\n","  16800/300000: episode: 84, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 7.018203, mae: 39.024738, mean_q: -57.961601\n","  17000/300000: episode: 85, duration: 2.059s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.626356, mae: 39.173363, mean_q: -58.210117\n","  17200/300000: episode: 86, duration: 2.012s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 5.435377, mae: 39.432079, mean_q: -58.582211\n","  17400/300000: episode: 87, duration: 2.002s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.452590, mae: 39.553284, mean_q: -58.695244\n","  17600/300000: episode: 88, duration: 2.036s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 6.705932, mae: 39.795731, mean_q: -59.113907\n","  17800/300000: episode: 89, duration: 2.080s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.728436, mae: 40.018631, mean_q: -59.370827\n","  18000/300000: episode: 90, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 10.543078, mae: 39.988514, mean_q: -59.220047\n","  18200/300000: episode: 91, duration: 2.036s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 10.883872, mae: 40.103333, mean_q: -59.351685\n","  18400/300000: episode: 92, duration: 2.031s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.890100, mae: 40.077423, mean_q: -59.456074\n","  18600/300000: episode: 93, duration: 1.999s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.844327, mae: 40.128239, mean_q: -59.425694\n","  18800/300000: episode: 94, duration: 2.008s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.759589, mae: 40.133881, mean_q: -59.574970\n","  19000/300000: episode: 95, duration: 2.030s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 8.128025, mae: 40.240250, mean_q: -59.678272\n","  19200/300000: episode: 96, duration: 2.039s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 10.814785, mae: 40.354877, mean_q: -59.768448\n","  19400/300000: episode: 97, duration: 2.048s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.043467, mae: 40.379078, mean_q: -59.905994\n","  19600/300000: episode: 98, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.427478, mae: 40.503841, mean_q: -59.974110\n","  19800/300000: episode: 99, duration: 2.047s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.321762, mae: 40.555229, mean_q: -60.017242\n","  20000/300000: episode: 100, duration: 2.061s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.249291, mae: 40.624523, mean_q: -60.340839\n","  20200/300000: episode: 101, duration: 2.084s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.407365, mae: 40.835457, mean_q: -60.730419\n","  20400/300000: episode: 102, duration: 2.051s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 10.252594, mae: 40.946545, mean_q: -60.646465\n","  20600/300000: episode: 103, duration: 2.040s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 10.313423, mae: 40.962158, mean_q: -60.711349\n","  20800/300000: episode: 104, duration: 2.069s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.631252, mae: 40.943733, mean_q: -60.690098\n","  21000/300000: episode: 105, duration: 2.071s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 6.076814, mae: 41.087048, mean_q: -60.988163\n","  21200/300000: episode: 106, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 9.558913, mae: 41.246532, mean_q: -61.201950\n","  21400/300000: episode: 107, duration: 2.081s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 6.105913, mae: 41.276924, mean_q: -61.349239\n","  21600/300000: episode: 108, duration: 2.023s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 6.383807, mae: 41.455273, mean_q: -61.596500\n","  21800/300000: episode: 109, duration: 2.071s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.824636, mae: 41.579502, mean_q: -61.713261\n","  22000/300000: episode: 110, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.611318, mae: 41.690781, mean_q: -61.916080\n","  22200/300000: episode: 111, duration: 2.129s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 11.418674, mae: 41.730698, mean_q: -61.799767\n","  22400/300000: episode: 112, duration: 2.172s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 7.621807, mae: 41.698776, mean_q: -61.938393\n","  22600/300000: episode: 113, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 11.274278, mae: 41.681339, mean_q: -61.620468\n","  22800/300000: episode: 114, duration: 2.147s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.408779, mae: 41.717148, mean_q: -61.901443\n","  23000/300000: episode: 115, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000],  loss: 10.203309, mae: 41.490372, mean_q: -61.463490\n","  23200/300000: episode: 116, duration: 2.203s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 6.832069, mae: 41.593224, mean_q: -61.818687\n","  23400/300000: episode: 117, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.714700, mae: 41.746902, mean_q: -61.946114\n","  23600/300000: episode: 118, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.737744, mae: 41.793301, mean_q: -62.019428\n","  23800/300000: episode: 119, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 10.195995, mae: 41.900440, mean_q: -62.117149\n","  24000/300000: episode: 120, duration: 2.029s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 6.756000, mae: 42.030331, mean_q: -62.414238\n","  24200/300000: episode: 121, duration: 2.041s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 9.435144, mae: 42.153149, mean_q: -62.539658\n","  24400/300000: episode: 122, duration: 2.078s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 6.510889, mae: 42.229931, mean_q: -62.755157\n","  24600/300000: episode: 123, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.463484, mae: 42.277756, mean_q: -62.669899\n","  24800/300000: episode: 124, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.426201, mae: 42.370869, mean_q: -62.934795\n","  25000/300000: episode: 125, duration: 2.167s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 9.116859, mae: 42.437569, mean_q: -62.858150\n","  25200/300000: episode: 126, duration: 2.189s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 12.642473, mae: 42.375088, mean_q: -62.756382\n","  25400/300000: episode: 127, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 11.927343, mae: 42.271542, mean_q: -62.662441\n","  25600/300000: episode: 128, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.739606, mae: 42.279491, mean_q: -62.818008\n","  25800/300000: episode: 129, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 11.413374, mae: 42.159496, mean_q: -62.349178\n","  26000/300000: episode: 130, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 13.285969, mae: 42.067329, mean_q: -62.261444\n","  26200/300000: episode: 131, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 13.189636, mae: 42.033344, mean_q: -62.229855\n","  26400/300000: episode: 132, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 10.935648, mae: 42.013432, mean_q: -62.229660\n","  26600/300000: episode: 133, duration: 2.061s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.659245, mae: 42.023926, mean_q: -62.311069\n","  26800/300000: episode: 134, duration: 2.068s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 9.512605, mae: 41.994614, mean_q: -62.347912\n","  27000/300000: episode: 135, duration: 2.079s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 9.872760, mae: 42.106434, mean_q: -62.398788\n","  27200/300000: episode: 136, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.396402, mae: 42.186798, mean_q: -62.641125\n","  27400/300000: episode: 137, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.862306, mae: 42.332973, mean_q: -62.890881\n","  27600/300000: episode: 138, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.944254, mae: 42.267860, mean_q: -62.709305\n","  27800/300000: episode: 139, duration: 2.205s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 11.262801, mae: 42.359859, mean_q: -62.699463\n","  28000/300000: episode: 140, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.731074, mae: 42.477345, mean_q: -62.942383\n","  28200/300000: episode: 141, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 10.438620, mae: 42.420521, mean_q: -62.905731\n","  28400/300000: episode: 142, duration: 2.139s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 5.728883, mae: 42.553001, mean_q: -63.328468\n","  28600/300000: episode: 143, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 12.354010, mae: 42.598175, mean_q: -63.050034\n","  28800/300000: episode: 144, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 12.019068, mae: 42.560806, mean_q: -63.071835\n","  29000/300000: episode: 145, duration: 2.109s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.498157, mae: 42.490093, mean_q: -62.862503\n","  29200/300000: episode: 146, duration: 2.085s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 8.866416, mae: 42.488312, mean_q: -63.031773\n","  29400/300000: episode: 147, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 11.888198, mae: 42.521523, mean_q: -62.881222\n","  29600/300000: episode: 148, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 10.296072, mae: 42.534599, mean_q: -63.132801\n","  29800/300000: episode: 149, duration: 2.076s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 9.668409, mae: 42.547211, mean_q: -63.170151\n","  30000/300000: episode: 150, duration: 2.085s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 11.611721, mae: 42.484318, mean_q: -62.976376\n","  30200/300000: episode: 151, duration: 2.052s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 11.960624, mae: 42.448925, mean_q: -62.862900\n","  30400/300000: episode: 152, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 11.431348, mae: 42.424938, mean_q: -62.796486\n","  30600/300000: episode: 153, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 8.604941, mae: 42.414154, mean_q: -62.972023\n","  30800/300000: episode: 154, duration: 2.190s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.312664, mae: 42.505074, mean_q: -63.115673\n","  31000/300000: episode: 155, duration: 2.189s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 11.419526, mae: 42.479404, mean_q: -63.010468\n","  31200/300000: episode: 156, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 8.440821, mae: 42.548946, mean_q: -63.169014\n","  31400/300000: episode: 157, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 9.185959, mae: 42.540627, mean_q: -63.150185\n","  31600/300000: episode: 158, duration: 2.153s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 10.757459, mae: 42.530346, mean_q: -63.066212\n","  31800/300000: episode: 159, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.169520, mae: 42.515518, mean_q: -63.001904\n","  32000/300000: episode: 160, duration: 2.051s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.346183, mae: 42.601128, mean_q: -63.257431\n","  32200/300000: episode: 161, duration: 2.117s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 9.766396, mae: 42.670956, mean_q: -63.361847\n","  32400/300000: episode: 162, duration: 2.030s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.888586, mae: 42.701561, mean_q: -63.302773\n","  32600/300000: episode: 163, duration: 2.028s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 11.209570, mae: 42.671356, mean_q: -63.263500\n","  32800/300000: episode: 164, duration: 2.007s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 9.822489, mae: 42.730202, mean_q: -63.278961\n","  33000/300000: episode: 165, duration: 2.078s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.475110, mae: 42.726147, mean_q: -63.423203\n","  33200/300000: episode: 166, duration: 2.117s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.557229, mae: 42.893692, mean_q: -63.705936\n","  33400/300000: episode: 167, duration: 2.084s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.133467, mae: 43.030994, mean_q: -63.835316\n","  33600/300000: episode: 168, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.038177, mae: 42.982002, mean_q: -63.755806\n","  33800/300000: episode: 169, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 10.667017, mae: 43.091515, mean_q: -63.899017\n","  34000/300000: episode: 170, duration: 2.086s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 10.545363, mae: 43.093872, mean_q: -63.955967\n","  34200/300000: episode: 171, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 11.248789, mae: 42.971905, mean_q: -63.705059\n","  34400/300000: episode: 172, duration: 2.069s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 13.019667, mae: 43.014313, mean_q: -63.637516\n","  34600/300000: episode: 173, duration: 2.063s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.490876, mae: 42.816948, mean_q: -63.506397\n","  34800/300000: episode: 174, duration: 2.110s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.747536, mae: 42.889736, mean_q: -63.592789\n","  35000/300000: episode: 175, duration: 2.108s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.851196, mae: 42.734375, mean_q: -63.392548\n","  35200/300000: episode: 176, duration: 2.090s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.056973, mae: 42.799229, mean_q: -63.611298\n","  35400/300000: episode: 177, duration: 2.060s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 10.242293, mae: 42.791771, mean_q: -63.339893\n","  35600/300000: episode: 178, duration: 2.094s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.343414, mae: 42.819210, mean_q: -63.624317\n","  35800/300000: episode: 179, duration: 2.022s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 10.615064, mae: 42.920563, mean_q: -63.623802\n","  36000/300000: episode: 180, duration: 2.080s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 13.036086, mae: 42.806171, mean_q: -63.319542\n","  36200/300000: episode: 181, duration: 2.071s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.288570, mae: 42.689579, mean_q: -63.382885\n","  36400/300000: episode: 182, duration: 2.046s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.068228, mae: 42.867157, mean_q: -63.652035\n","  36600/300000: episode: 183, duration: 2.039s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.770018, mae: 42.847977, mean_q: -63.595810\n","  36800/300000: episode: 184, duration: 2.002s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 10.326726, mae: 42.918682, mean_q: -63.701836\n","  37000/300000: episode: 185, duration: 2.067s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 11.333724, mae: 42.893536, mean_q: -63.440010\n","  37200/300000: episode: 186, duration: 2.055s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.683982, mae: 42.889561, mean_q: -63.646873\n","  37400/300000: episode: 187, duration: 2.075s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 8.830456, mae: 42.977638, mean_q: -63.779022\n","  37600/300000: episode: 188, duration: 2.082s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 10.217049, mae: 43.059917, mean_q: -63.927017\n","  37800/300000: episode: 189, duration: 2.077s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.486978, mae: 43.088150, mean_q: -63.991592\n","  38000/300000: episode: 190, duration: 2.094s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 10.517681, mae: 43.109917, mean_q: -63.914936\n","  38200/300000: episode: 191, duration: 2.081s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 9.545010, mae: 43.222488, mean_q: -64.116508\n","  38400/300000: episode: 192, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 11.560009, mae: 43.154491, mean_q: -63.889259\n","  38600/300000: episode: 193, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 7.674898, mae: 43.270126, mean_q: -64.267693\n","  38800/300000: episode: 194, duration: 2.145s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.740770, mae: 43.385391, mean_q: -64.308365\n","  39000/300000: episode: 195, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.089978, mae: 43.343796, mean_q: -64.354729\n","  39200/300000: episode: 196, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 10.308098, mae: 43.365337, mean_q: -64.310600\n","  39400/300000: episode: 197, duration: 2.123s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 12.892660, mae: 43.280155, mean_q: -64.024773\n","  39600/300000: episode: 198, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 9.149943, mae: 43.291973, mean_q: -64.324432\n","  39800/300000: episode: 199, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.893764, mae: 43.402287, mean_q: -64.452736\n","  40000/300000: episode: 200, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 11.698904, mae: 43.441284, mean_q: -64.368256\n","  40200/300000: episode: 201, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 10.669655, mae: 43.352695, mean_q: -64.261955\n","  40400/300000: episode: 202, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.628740, mae: 43.282616, mean_q: -64.116722\n","  40600/300000: episode: 203, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 12.720907, mae: 43.332916, mean_q: -64.173607\n","  40800/300000: episode: 204, duration: 2.172s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 8.682126, mae: 43.183712, mean_q: -64.105217\n","  41000/300000: episode: 205, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 10.140532, mae: 43.123734, mean_q: -63.978146\n","  41200/300000: episode: 206, duration: 2.139s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.579893, mae: 43.178436, mean_q: -64.082787\n","  41400/300000: episode: 207, duration: 2.084s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 10.208451, mae: 43.188568, mean_q: -64.026863\n","  41600/300000: episode: 208, duration: 2.108s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 10.169582, mae: 43.216209, mean_q: -64.151955\n","  41800/300000: episode: 209, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 8.884972, mae: 43.280331, mean_q: -64.186272\n","  42000/300000: episode: 210, duration: 2.106s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.179557, mae: 43.263317, mean_q: -64.223610\n","  42200/300000: episode: 211, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.467296, mae: 43.368752, mean_q: -64.299034\n","  42400/300000: episode: 212, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.954018, mae: 43.408321, mean_q: -64.372643\n","  42600/300000: episode: 213, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 10.601890, mae: 43.446213, mean_q: -64.383034\n","  42800/300000: episode: 214, duration: 2.113s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 7.155613, mae: 43.453617, mean_q: -64.604500\n","  43000/300000: episode: 215, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 12.060230, mae: 43.516605, mean_q: -64.448914\n","  43200/300000: episode: 216, duration: 2.117s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 13.946054, mae: 43.430458, mean_q: -64.302429\n","  43400/300000: episode: 217, duration: 2.078s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 13.480638, mae: 43.297794, mean_q: -64.079407\n","  43600/300000: episode: 218, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.566517, mae: 43.308605, mean_q: -64.317665\n","  43800/300000: episode: 219, duration: 2.105s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.856568, mae: 43.320572, mean_q: -64.259094\n","  44000/300000: episode: 220, duration: 2.097s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.759146, mae: 43.400391, mean_q: -64.431175\n","  44200/300000: episode: 221, duration: 2.056s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 6.972150, mae: 43.502045, mean_q: -64.661911\n","  44400/300000: episode: 222, duration: 2.123s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 10.540182, mae: 43.534233, mean_q: -64.468552\n","  44600/300000: episode: 223, duration: 2.104s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 11.233272, mae: 43.407257, mean_q: -64.210228\n","  44800/300000: episode: 224, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.339126, mae: 43.420956, mean_q: -64.501335\n","  45000/300000: episode: 225, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 10.986233, mae: 43.499771, mean_q: -64.418678\n","  45200/300000: episode: 226, duration: 2.039s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 9.420889, mae: 43.499981, mean_q: -64.590126\n","  45400/300000: episode: 227, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 10.257853, mae: 43.530975, mean_q: -64.570847\n","  45600/300000: episode: 228, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.169710, mae: 43.604733, mean_q: -64.746887\n","  45800/300000: episode: 229, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 10.253085, mae: 43.570114, mean_q: -64.647995\n","  46000/300000: episode: 230, duration: 2.108s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 12.303684, mae: 43.497158, mean_q: -64.397552\n","  46200/300000: episode: 231, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.366488, mae: 43.487186, mean_q: -64.476692\n","  46400/300000: episode: 232, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 13.532657, mae: 43.490726, mean_q: -64.289795\n","  46600/300000: episode: 233, duration: 2.116s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 10.486031, mae: 43.397118, mean_q: -64.293983\n","  46800/300000: episode: 234, duration: 2.115s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.221743, mae: 43.543545, mean_q: -64.703789\n","  47000/300000: episode: 235, duration: 2.092s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 11.241283, mae: 43.480148, mean_q: -64.511429\n","  47200/300000: episode: 236, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 10.030890, mae: 43.531036, mean_q: -64.530098\n","  47400/300000: episode: 237, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 13.584933, mae: 43.442703, mean_q: -64.323723\n","  47600/300000: episode: 238, duration: 2.173s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 11.804565, mae: 43.398312, mean_q: -64.333466\n","  47800/300000: episode: 239, duration: 2.091s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 7.446902, mae: 43.415508, mean_q: -64.546082\n","  48000/300000: episode: 240, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 11.873242, mae: 43.372715, mean_q: -64.298004\n","  48200/300000: episode: 241, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.498551, mae: 43.424095, mean_q: -64.455872\n","  48400/300000: episode: 242, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 12.199999, mae: 43.355156, mean_q: -64.237862\n","  48600/300000: episode: 243, duration: 2.109s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.144062, mae: 43.272388, mean_q: -64.136200\n","  48800/300000: episode: 244, duration: 2.038s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 7.632759, mae: 43.310162, mean_q: -64.374657\n","  49000/300000: episode: 245, duration: 2.039s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 12.138271, mae: 43.360600, mean_q: -64.273148\n","  49200/300000: episode: 246, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 10.062307, mae: 43.262676, mean_q: -64.138840\n","  49400/300000: episode: 247, duration: 2.031s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 11.979798, mae: 43.289547, mean_q: -64.102463\n","  49600/300000: episode: 248, duration: 2.032s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.310664, mae: 43.235981, mean_q: -64.219582\n","  49800/300000: episode: 249, duration: 2.046s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 11.101331, mae: 43.263134, mean_q: -64.107277\n","  50000/300000: episode: 250, duration: 2.013s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 10.392268, mae: 43.213776, mean_q: -64.079491\n","  50200/300000: episode: 251, duration: 2.027s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.277484, mae: 43.183769, mean_q: -64.079788\n","  50400/300000: episode: 252, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.171930, mae: 43.342289, mean_q: -64.172180\n","  50600/300000: episode: 253, duration: 2.224s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 13.571184, mae: 43.264786, mean_q: -64.051003\n","  50800/300000: episode: 254, duration: 2.259s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 9.646787, mae: 43.149433, mean_q: -64.026154\n","  51000/300000: episode: 255, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 11.856263, mae: 43.092934, mean_q: -63.841354\n","  51200/300000: episode: 256, duration: 2.241s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 11.833244, mae: 43.009247, mean_q: -63.721104\n","  51400/300000: episode: 257, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 11.304534, mae: 42.947567, mean_q: -63.703770\n","  51600/300000: episode: 258, duration: 2.203s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 10.841966, mae: 42.950752, mean_q: -63.652519\n","  51800/300000: episode: 259, duration: 2.217s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 9.635973, mae: 42.994141, mean_q: -63.854053\n","  52000/300000: episode: 260, duration: 2.228s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 10.232349, mae: 42.982140, mean_q: -63.776878\n","  52200/300000: episode: 261, duration: 2.261s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 12.399393, mae: 42.933853, mean_q: -63.624561\n","  52400/300000: episode: 262, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 10.941709, mae: 42.899101, mean_q: -63.552608\n","  52600/300000: episode: 263, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 7.602241, mae: 42.879532, mean_q: -63.693707\n","  52800/300000: episode: 264, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 11.212157, mae: 42.854389, mean_q: -63.468327\n","  53000/300000: episode: 265, duration: 2.225s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 7.744184, mae: 42.815575, mean_q: -63.581337\n","  53200/300000: episode: 266, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 7.900434, mae: 42.993839, mean_q: -63.919228\n","  53400/300000: episode: 267, duration: 2.199s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 10.606697, mae: 43.050468, mean_q: -63.787148\n","  53600/300000: episode: 268, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 9.365521, mae: 43.029316, mean_q: -63.761929\n","  53800/300000: episode: 269, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 7.026885, mae: 43.126877, mean_q: -64.169861\n","  54000/300000: episode: 270, duration: 2.229s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 10.575253, mae: 43.088593, mean_q: -63.935478\n","  54200/300000: episode: 271, duration: 2.228s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 6.952495, mae: 43.282784, mean_q: -64.369644\n","  54400/300000: episode: 272, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 8.703121, mae: 43.360672, mean_q: -64.333984\n","  54600/300000: episode: 273, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.852626, mae: 43.374004, mean_q: -64.455421\n","  54800/300000: episode: 274, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.108188, mae: 43.464298, mean_q: -64.376534\n","  55000/300000: episode: 275, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.852126, mae: 43.395763, mean_q: -64.451286\n","  55200/300000: episode: 276, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.398945, mae: 43.424576, mean_q: -64.428253\n","  55400/300000: episode: 277, duration: 2.097s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 8.525150, mae: 43.516098, mean_q: -64.674881\n","  55600/300000: episode: 278, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.408273, mae: 43.525078, mean_q: -64.558586\n","  55800/300000: episode: 279, duration: 2.187s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 11.144881, mae: 43.536587, mean_q: -64.502296\n","  56000/300000: episode: 280, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.530800, mae: 43.573910, mean_q: -64.751076\n","  56200/300000: episode: 281, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.915679, mae: 43.670303, mean_q: -64.844521\n","  56400/300000: episode: 282, duration: 2.167s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 11.347342, mae: 43.607040, mean_q: -64.678810\n","  56600/300000: episode: 283, duration: 2.271s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 8.928040, mae: 43.674549, mean_q: -64.880714\n","  56800/300000: episode: 284, duration: 2.243s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.846828, mae: 43.691586, mean_q: -64.804405\n","  57000/300000: episode: 285, duration: 2.281s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 9.135654, mae: 43.583282, mean_q: -64.726662\n","  57200/300000: episode: 286, duration: 2.220s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 9.508307, mae: 43.736984, mean_q: -64.970955\n","  57400/300000: episode: 287, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 9.696356, mae: 43.717682, mean_q: -64.791405\n","  57600/300000: episode: 288, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 12.773782, mae: 43.645626, mean_q: -64.487251\n","  57800/300000: episode: 289, duration: 2.256s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.369033, mae: 43.655727, mean_q: -64.742813\n","  58000/300000: episode: 290, duration: 2.249s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 11.156075, mae: 43.657467, mean_q: -64.597504\n","  58200/300000: episode: 291, duration: 2.250s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 10.048719, mae: 43.590031, mean_q: -64.495811\n","  58400/300000: episode: 292, duration: 2.249s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 12.153704, mae: 43.590889, mean_q: -64.541008\n","  58600/300000: episode: 293, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.698621, mae: 43.569283, mean_q: -64.541748\n","  58800/300000: episode: 294, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.650941, mae: 43.563564, mean_q: -64.546234\n","  59000/300000: episode: 295, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 10.448365, mae: 43.382168, mean_q: -64.245743\n","  59200/300000: episode: 296, duration: 2.264s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.248613, mae: 43.404942, mean_q: -64.439987\n","  59400/300000: episode: 297, duration: 2.229s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 10.526353, mae: 43.443436, mean_q: -64.252716\n","  59600/300000: episode: 298, duration: 2.312s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 8.821929, mae: 43.316925, mean_q: -64.243782\n","  59800/300000: episode: 299, duration: 2.198s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 10.146644, mae: 43.249435, mean_q: -64.076111\n","  60000/300000: episode: 300, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.993855, mae: 43.362099, mean_q: -64.390823\n","  60200/300000: episode: 301, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 12.634533, mae: 43.348339, mean_q: -64.218605\n","  60400/300000: episode: 302, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 10.174230, mae: 43.200104, mean_q: -63.967178\n","  60600/300000: episode: 303, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 7.485489, mae: 43.129498, mean_q: -64.075233\n","  60800/300000: episode: 304, duration: 2.130s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 11.291769, mae: 43.127968, mean_q: -63.950859\n","  61000/300000: episode: 305, duration: 2.106s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 12.056465, mae: 43.120758, mean_q: -63.892284\n","  61200/300000: episode: 306, duration: 2.200s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 9.604635, mae: 43.017124, mean_q: -63.848644\n","  61400/300000: episode: 307, duration: 2.136s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.273896, mae: 43.040665, mean_q: -63.856289\n","  61600/300000: episode: 308, duration: 2.211s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.383728, mae: 43.017071, mean_q: -63.893097\n","  61800/300000: episode: 309, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 5.734891, mae: 43.140556, mean_q: -64.215111\n","  62000/300000: episode: 310, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 9.307742, mae: 43.273052, mean_q: -64.100357\n","  62200/300000: episode: 311, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 4.931016, mae: 43.365536, mean_q: -64.452591\n","  62400/300000: episode: 312, duration: 2.222s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 11.193979, mae: 43.481483, mean_q: -64.340828\n","  62600/300000: episode: 313, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.723360, mae: 43.540249, mean_q: -64.742287\n","  62800/300000: episode: 314, duration: 2.247s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 13.558790, mae: 43.437119, mean_q: -64.231270\n","  63000/300000: episode: 315, duration: 2.243s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 7.396469, mae: 43.496971, mean_q: -64.593811\n","  63200/300000: episode: 316, duration: 2.254s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 8.754107, mae: 43.573299, mean_q: -64.628456\n","  63400/300000: episode: 317, duration: 2.311s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 10.041729, mae: 43.586842, mean_q: -64.659546\n","  63600/300000: episode: 318, duration: 2.221s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 9.110346, mae: 43.581612, mean_q: -64.678406\n","  63800/300000: episode: 319, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 10.379049, mae: 43.603378, mean_q: -64.672737\n","  64000/300000: episode: 320, duration: 2.198s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 12.118483, mae: 43.446800, mean_q: -64.303223\n","  64200/300000: episode: 321, duration: 2.199s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.549170, mae: 43.414135, mean_q: -64.513824\n","  64400/300000: episode: 322, duration: 2.274s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 12.345447, mae: 43.419903, mean_q: -64.263481\n","  64600/300000: episode: 323, duration: 2.153s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 11.125978, mae: 43.268734, mean_q: -64.083702\n","  64800/300000: episode: 324, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 12.377148, mae: 43.153927, mean_q: -63.908215\n","  65000/300000: episode: 325, duration: 2.287s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 8.954310, mae: 43.079624, mean_q: -63.955441\n","  65200/300000: episode: 326, duration: 2.259s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 7.972684, mae: 43.227783, mean_q: -64.234802\n","  65400/300000: episode: 327, duration: 2.186s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.058164, mae: 43.433086, mean_q: -64.583725\n","  65600/300000: episode: 328, duration: 2.264s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 8.682789, mae: 43.520309, mean_q: -64.649651\n","  65800/300000: episode: 329, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.718778, mae: 43.605614, mean_q: -64.736282\n","  66000/300000: episode: 330, duration: 2.200s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.428450, mae: 43.657127, mean_q: -64.646530\n","  66200/300000: episode: 331, duration: 2.190s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 10.506831, mae: 43.575851, mean_q: -64.577049\n","  66400/300000: episode: 332, duration: 2.187s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.791142, mae: 43.677071, mean_q: -64.920853\n","  66600/300000: episode: 333, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 10.049484, mae: 43.679932, mean_q: -64.729973\n","  66800/300000: episode: 334, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000],  loss: 10.257561, mae: 43.604790, mean_q: -64.604416\n","  67000/300000: episode: 335, duration: 2.198s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.829874, mae: 43.586151, mean_q: -64.673019\n","  67200/300000: episode: 336, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 9.018555, mae: 43.592598, mean_q: -64.687012\n","  67400/300000: episode: 337, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 10.204331, mae: 43.611206, mean_q: -64.614990\n","  67600/300000: episode: 338, duration: 2.100s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 10.526049, mae: 43.585297, mean_q: -64.664581\n","  67800/300000: episode: 339, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 10.749499, mae: 43.519562, mean_q: -64.470306\n","  68000/300000: episode: 340, duration: 2.157s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 10.525776, mae: 43.530205, mean_q: -64.549080\n","  68200/300000: episode: 341, duration: 2.093s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.160392, mae: 43.445190, mean_q: -64.524170\n","  68400/300000: episode: 342, duration: 2.168s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.964979, mae: 43.462135, mean_q: -64.307793\n","  68600/300000: episode: 343, duration: 2.136s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 9.302827, mae: 43.477852, mean_q: -64.561012\n","  68800/300000: episode: 344, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.902714, mae: 43.487938, mean_q: -64.556557\n","  69000/300000: episode: 345, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 10.986492, mae: 43.489731, mean_q: -64.481750\n","  69200/300000: episode: 346, duration: 2.211s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 11.520468, mae: 43.422958, mean_q: -64.331993\n","  69400/300000: episode: 347, duration: 2.129s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 10.389837, mae: 43.365597, mean_q: -64.253685\n","  69600/300000: episode: 348, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 11.012378, mae: 43.300095, mean_q: -64.228439\n","  69800/300000: episode: 349, duration: 2.107s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 8.532436, mae: 43.352890, mean_q: -64.288742\n","  70000/300000: episode: 350, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 13.427682, mae: 43.244450, mean_q: -63.962383\n","  70200/300000: episode: 351, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 9.556890, mae: 43.148449, mean_q: -64.025772\n","  70400/300000: episode: 352, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 11.354179, mae: 43.109055, mean_q: -63.851463\n","  70600/300000: episode: 353, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 11.699092, mae: 43.002373, mean_q: -63.800907\n","  70800/300000: episode: 354, duration: 2.233s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 11.067776, mae: 42.969501, mean_q: -63.619869\n","  71000/300000: episode: 355, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.335022, mae: 42.930195, mean_q: -63.789951\n","  71200/300000: episode: 356, duration: 2.139s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.295073, mae: 42.984474, mean_q: -63.900822\n","  71400/300000: episode: 357, duration: 2.128s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 10.706038, mae: 42.965919, mean_q: -63.602665\n","  71600/300000: episode: 358, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 10.329432, mae: 42.980282, mean_q: -63.702713\n","  71800/300000: episode: 359, duration: 2.153s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.499421, mae: 42.978722, mean_q: -63.831718\n","  72000/300000: episode: 360, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.507635, mae: 43.102573, mean_q: -64.014297\n","  72200/300000: episode: 361, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 10.344968, mae: 43.145020, mean_q: -64.067451\n","  72400/300000: episode: 362, duration: 2.136s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 8.963331, mae: 43.103077, mean_q: -63.886456\n","  72600/300000: episode: 363, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 11.929893, mae: 43.114025, mean_q: -63.710281\n","  72800/300000: episode: 364, duration: 2.162s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 9.684720, mae: 43.029366, mean_q: -63.719624\n","  73000/300000: episode: 365, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 9.895362, mae: 43.023960, mean_q: -63.838272\n","  73200/300000: episode: 366, duration: 2.237s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 8.780300, mae: 43.051613, mean_q: -63.965229\n","  73400/300000: episode: 367, duration: 2.228s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 11.924736, mae: 43.030987, mean_q: -63.733990\n","  73600/300000: episode: 368, duration: 2.277s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.331137, mae: 42.989174, mean_q: -63.747715\n","  73800/300000: episode: 369, duration: 2.290s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.429170, mae: 43.066525, mean_q: -63.813457\n","  74000/300000: episode: 370, duration: 2.304s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 8.658607, mae: 43.013428, mean_q: -63.784168\n","  74200/300000: episode: 371, duration: 2.265s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.255687, mae: 43.207207, mean_q: -64.233757\n","  74400/300000: episode: 372, duration: 2.218s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.035555, mae: 43.264130, mean_q: -64.265663\n","  74600/300000: episode: 373, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.408879, mae: 43.358810, mean_q: -64.354515\n","  74800/300000: episode: 374, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 10.584376, mae: 43.391205, mean_q: -64.267517\n","  75000/300000: episode: 375, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 7.610327, mae: 43.358898, mean_q: -64.370987\n","  75200/300000: episode: 376, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.098191, mae: 43.498436, mean_q: -64.574852\n","  75400/300000: episode: 377, duration: 2.186s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 8.790530, mae: 43.456036, mean_q: -64.511719\n","  75600/300000: episode: 378, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.617467, mae: 43.455429, mean_q: -64.565544\n","  75800/300000: episode: 379, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.071940, mae: 43.486176, mean_q: -64.447952\n","  76000/300000: episode: 380, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 11.886877, mae: 43.455887, mean_q: -64.385330\n","  76200/300000: episode: 381, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 8.907899, mae: 43.408287, mean_q: -64.467659\n","  76400/300000: episode: 382, duration: 2.222s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 10.158047, mae: 43.423309, mean_q: -64.371758\n","  76600/300000: episode: 383, duration: 2.240s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 8.533767, mae: 43.474510, mean_q: -64.513664\n","  76800/300000: episode: 384, duration: 2.219s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 11.351461, mae: 43.447678, mean_q: -64.399094\n","  77000/300000: episode: 385, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.362596, mae: 43.367897, mean_q: -64.372581\n","  77200/300000: episode: 386, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.691221, mae: 43.406406, mean_q: -64.414909\n","  77400/300000: episode: 387, duration: 2.114s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.073740, mae: 43.455135, mean_q: -64.510620\n","  77600/300000: episode: 388, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 11.312634, mae: 43.413193, mean_q: -64.409348\n","  77800/300000: episode: 389, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 10.422495, mae: 43.463112, mean_q: -64.321075\n","  78000/300000: episode: 390, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.200609, mae: 43.451359, mean_q: -64.488922\n","  78200/300000: episode: 391, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 9.742022, mae: 43.379063, mean_q: -64.332756\n","  78400/300000: episode: 392, duration: 2.267s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 8.547704, mae: 43.417248, mean_q: -64.422707\n","  78600/300000: episode: 393, duration: 2.262s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 11.947467, mae: 43.324631, mean_q: -64.197357\n","  78800/300000: episode: 394, duration: 2.279s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.617232, mae: 43.328976, mean_q: -64.302223\n","  79000/300000: episode: 395, duration: 2.221s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 11.074821, mae: 43.124390, mean_q: -63.812675\n","  79200/300000: episode: 396, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 9.774153, mae: 43.050915, mean_q: -63.852859\n","  79400/300000: episode: 397, duration: 2.233s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.772466, mae: 43.065094, mean_q: -63.911057\n","  79600/300000: episode: 398, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.789814, mae: 43.072109, mean_q: -63.881523\n","  79800/300000: episode: 399, duration: 2.272s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 8.946509, mae: 43.116058, mean_q: -63.893574\n","  80000/300000: episode: 400, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 10.619000, mae: 43.032303, mean_q: -63.784950\n","  80200/300000: episode: 401, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 10.365527, mae: 42.936733, mean_q: -63.657608\n","  80400/300000: episode: 402, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 10.798916, mae: 42.910126, mean_q: -63.581711\n","  80600/300000: episode: 403, duration: 2.256s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.474102, mae: 42.947365, mean_q: -63.700104\n","  80800/300000: episode: 404, duration: 2.233s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 9.983495, mae: 42.792404, mean_q: -63.485199\n","  81000/300000: episode: 405, duration: 2.236s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.897522, mae: 42.886147, mean_q: -63.589340\n","  81200/300000: episode: 406, duration: 2.216s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 12.839017, mae: 42.749054, mean_q: -63.233906\n","  81400/300000: episode: 407, duration: 2.292s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 8.143725, mae: 42.578751, mean_q: -63.061085\n","  81600/300000: episode: 408, duration: 2.304s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.539960, mae: 42.527088, mean_q: -63.141579\n","  81800/300000: episode: 409, duration: 2.305s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 9.585201, mae: 42.486309, mean_q: -63.057617\n","  82000/300000: episode: 410, duration: 2.290s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.387303, mae: 42.534687, mean_q: -62.970909\n","  82200/300000: episode: 411, duration: 2.272s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 10.470303, mae: 42.439869, mean_q: -62.838985\n","  82400/300000: episode: 412, duration: 2.260s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 10.826588, mae: 42.427120, mean_q: -62.844513\n","  82600/300000: episode: 413, duration: 2.232s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 7.749520, mae: 42.372086, mean_q: -62.758888\n","  82800/300000: episode: 414, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.605042, mae: 42.426308, mean_q: -62.855453\n","  83000/300000: episode: 415, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 9.764920, mae: 42.465744, mean_q: -62.931881\n","  83200/300000: episode: 416, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 7.460482, mae: 42.265537, mean_q: -62.696259\n","  83400/300000: episode: 417, duration: 2.216s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 7.488545, mae: 42.408607, mean_q: -62.937607\n","  83600/300000: episode: 418, duration: 2.199s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 8.727084, mae: 42.461533, mean_q: -63.012299\n","  83800/300000: episode: 419, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 9.385944, mae: 42.387573, mean_q: -62.891624\n","  84000/300000: episode: 420, duration: 2.168s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.925397, mae: 42.387138, mean_q: -62.980885\n","  84200/300000: episode: 421, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 10.605899, mae: 42.483490, mean_q: -62.896561\n","  84400/300000: episode: 422, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.055642, mae: 42.351009, mean_q: -62.809185\n","  84600/300000: episode: 423, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.202937, mae: 42.347363, mean_q: -62.829750\n","  84800/300000: episode: 424, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 12.325405, mae: 42.303154, mean_q: -62.480515\n","  85000/300000: episode: 425, duration: 2.220s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 11.042991, mae: 42.283169, mean_q: -62.625668\n","  85200/300000: episode: 426, duration: 2.235s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 11.017246, mae: 42.206680, mean_q: -62.462399\n","  85400/300000: episode: 427, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 10.382059, mae: 41.971455, mean_q: -62.237667\n","  85600/300000: episode: 428, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.199004, mae: 41.927597, mean_q: -62.142982\n","  85800/300000: episode: 429, duration: 2.309s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.038801, mae: 41.913280, mean_q: -62.246700\n","  86000/300000: episode: 430, duration: 2.307s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 12.417089, mae: 41.921005, mean_q: -62.055458\n","  86200/300000: episode: 431, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.825061, mae: 41.849804, mean_q: -62.054619\n","  86400/300000: episode: 432, duration: 2.242s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 10.538774, mae: 41.933006, mean_q: -62.126976\n","  86600/300000: episode: 433, duration: 2.230s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 8.604218, mae: 41.935883, mean_q: -62.152908\n","  86800/300000: episode: 434, duration: 2.199s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 11.537602, mae: 41.827579, mean_q: -61.931919\n","  87000/300000: episode: 435, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.755219, mae: 41.838398, mean_q: -61.968262\n","  87200/300000: episode: 436, duration: 2.312s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.987654, mae: 41.834908, mean_q: -62.066681\n","  87400/300000: episode: 437, duration: 2.239s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 6.585243, mae: 41.908154, mean_q: -62.170757\n","  87600/300000: episode: 438, duration: 2.200s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 7.948851, mae: 41.928417, mean_q: -62.230778\n","  87800/300000: episode: 439, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 6.853180, mae: 42.174721, mean_q: -62.563332\n","  88000/300000: episode: 440, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 11.962783, mae: 42.160801, mean_q: -62.399784\n","  88200/300000: episode: 441, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 7.382908, mae: 42.017799, mean_q: -62.371124\n","  88400/300000: episode: 442, duration: 2.227s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.893268, mae: 42.126778, mean_q: -62.392353\n","  88600/300000: episode: 443, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 8.512698, mae: 42.176373, mean_q: -62.523651\n","  88800/300000: episode: 444, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 8.843361, mae: 42.115402, mean_q: -62.360020\n","  89000/300000: episode: 445, duration: 2.108s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 9.669100, mae: 41.894352, mean_q: -61.929443\n","  89200/300000: episode: 446, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.408583, mae: 41.886478, mean_q: -61.984169\n","  89400/300000: episode: 447, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 6.902284, mae: 41.912064, mean_q: -62.189991\n","  89600/300000: episode: 448, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 5.273350, mae: 41.987694, mean_q: -62.334824\n","  89800/300000: episode: 449, duration: 2.204s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 9.337006, mae: 42.005238, mean_q: -62.177784\n","  90000/300000: episode: 450, duration: 2.173s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.852625, mae: 42.065548, mean_q: -62.372021\n","  90200/300000: episode: 451, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 12.158708, mae: 41.772339, mean_q: -61.729141\n","  90400/300000: episode: 452, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.247526, mae: 41.919731, mean_q: -62.177364\n","  90600/300000: episode: 453, duration: 2.186s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 10.969255, mae: 41.853848, mean_q: -61.965664\n","  90800/300000: episode: 454, duration: 2.241s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 7.764647, mae: 41.872856, mean_q: -61.976547\n","  91000/300000: episode: 455, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.140923, mae: 41.903488, mean_q: -62.056095\n","  91200/300000: episode: 456, duration: 2.215s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 8.922153, mae: 41.801125, mean_q: -61.922714\n","  91400/300000: episode: 457, duration: 2.205s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 7.771832, mae: 41.902973, mean_q: -62.042011\n","  91600/300000: episode: 458, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 9.676148, mae: 41.801815, mean_q: -61.991699\n","  91800/300000: episode: 459, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.187345, mae: 42.061035, mean_q: -62.394077\n","  92000/300000: episode: 460, duration: 2.157s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 11.381682, mae: 41.993202, mean_q: -62.088955\n","  92200/300000: episode: 461, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 10.867690, mae: 41.878876, mean_q: -62.013287\n","  92400/300000: episode: 462, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 7.325410, mae: 41.932999, mean_q: -62.277775\n","  92600/300000: episode: 463, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.884212, mae: 41.967491, mean_q: -62.100670\n","  92800/300000: episode: 464, duration: 2.187s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.356443, mae: 42.221764, mean_q: -62.658119\n","  93000/300000: episode: 465, duration: 2.110s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.193340, mae: 42.186802, mean_q: -62.548168\n","  93200/300000: episode: 466, duration: 2.235s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 8.315679, mae: 42.019863, mean_q: -62.292294\n","  93400/300000: episode: 467, duration: 2.269s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 8.912086, mae: 42.002197, mean_q: -62.151798\n","  93600/300000: episode: 468, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 11.875452, mae: 41.909657, mean_q: -61.939838\n","  93800/300000: episode: 469, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.536103, mae: 42.046406, mean_q: -62.377247\n","  94000/300000: episode: 470, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.704318, mae: 41.958691, mean_q: -62.177959\n","  94200/300000: episode: 471, duration: 2.136s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 8.101667, mae: 42.038223, mean_q: -62.207352\n","  94400/300000: episode: 472, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 11.606853, mae: 42.076855, mean_q: -62.308594\n","  94600/300000: episode: 473, duration: 2.110s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.865172, mae: 42.038910, mean_q: -62.215858\n","  94800/300000: episode: 474, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 9.755780, mae: 42.046799, mean_q: -62.238564\n","  95000/300000: episode: 475, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 8.266078, mae: 41.942013, mean_q: -62.119354\n","  95200/300000: episode: 476, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 8.460901, mae: 42.038086, mean_q: -62.282841\n","  95400/300000: episode: 477, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 13.851545, mae: 42.012020, mean_q: -61.880051\n","  95600/300000: episode: 478, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.575199, mae: 41.693737, mean_q: -61.761795\n","  95800/300000: episode: 479, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 9.437440, mae: 41.596512, mean_q: -61.544823\n","  96000/300000: episode: 480, duration: 2.236s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 10.275726, mae: 41.672287, mean_q: -61.666981\n","  96200/300000: episode: 481, duration: 2.186s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 12.674895, mae: 41.616734, mean_q: -61.526257\n","  96400/300000: episode: 482, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.547588, mae: 41.377052, mean_q: -61.144428\n","  96600/300000: episode: 483, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 10.423388, mae: 41.517838, mean_q: -61.434883\n","  96800/300000: episode: 484, duration: 2.147s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 6.194053, mae: 41.458237, mean_q: -61.455135\n","  97000/300000: episode: 485, duration: 2.226s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 9.142854, mae: 41.347485, mean_q: -61.074806\n","  97200/300000: episode: 486, duration: 2.250s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 7.513996, mae: 41.398666, mean_q: -61.320328\n","  97400/300000: episode: 487, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.039688, mae: 41.451641, mean_q: -61.312084\n","  97600/300000: episode: 488, duration: 2.247s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 10.329227, mae: 41.524609, mean_q: -61.412441\n","  97800/300000: episode: 489, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.189746, mae: 41.431705, mean_q: -61.308174\n","  98000/300000: episode: 490, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.869769, mae: 41.547131, mean_q: -61.577217\n","  98200/300000: episode: 491, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 11.104553, mae: 41.430225, mean_q: -61.221836\n","  98400/300000: episode: 492, duration: 2.226s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.762870, mae: 41.516258, mean_q: -61.441383\n","  98600/300000: episode: 493, duration: 2.235s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 9.688730, mae: 41.475674, mean_q: -61.372658\n","  98800/300000: episode: 494, duration: 2.265s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 9.062356, mae: 41.561817, mean_q: -61.528419\n","  99000/300000: episode: 495, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 12.619381, mae: 41.380650, mean_q: -61.175880\n","  99200/300000: episode: 496, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 5.306449, mae: 41.340176, mean_q: -61.365482\n","  99400/300000: episode: 497, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.460400, mae: 41.478973, mean_q: -61.417759\n","  99600/300000: episode: 498, duration: 2.168s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.095798, mae: 41.551682, mean_q: -61.619152\n","  99800/300000: episode: 499, duration: 2.153s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.757015, mae: 41.501453, mean_q: -61.485111\n"," 100000/300000: episode: 500, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.037235, mae: 41.405251, mean_q: -61.347847\n"," 100200/300000: episode: 501, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.517170, mae: 41.566265, mean_q: -61.558735\n"," 100400/300000: episode: 502, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 9.424351, mae: 41.421913, mean_q: -61.197567\n"," 100600/300000: episode: 503, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 5.704236, mae: 41.499100, mean_q: -61.562332\n"," 100800/300000: episode: 504, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 9.287323, mae: 41.648548, mean_q: -61.672550\n"," 101000/300000: episode: 505, duration: 2.139s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.605284, mae: 41.695011, mean_q: -61.715752\n"," 101200/300000: episode: 506, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 9.112245, mae: 41.524708, mean_q: -61.397251\n"," 101400/300000: episode: 507, duration: 2.136s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 10.353574, mae: 41.371750, mean_q: -61.163036\n"," 101600/300000: episode: 508, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 8.523242, mae: 41.509167, mean_q: -61.508984\n"," 101800/300000: episode: 509, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 6.508130, mae: 41.494576, mean_q: -61.570854\n"," 102000/300000: episode: 510, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.155288, mae: 41.484013, mean_q: -61.378719\n"," 102200/300000: episode: 511, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.127107, mae: 41.582821, mean_q: -61.596317\n"," 102400/300000: episode: 512, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.829158, mae: 41.495461, mean_q: -61.455841\n"," 102600/300000: episode: 513, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 8.513580, mae: 41.676113, mean_q: -61.640236\n"," 102800/300000: episode: 514, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 8.493953, mae: 41.580734, mean_q: -61.532314\n"," 103000/300000: episode: 515, duration: 2.186s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.396995, mae: 41.502853, mean_q: -61.440186\n"," 103200/300000: episode: 516, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.406814, mae: 41.655773, mean_q: -61.763084\n"," 103400/300000: episode: 517, duration: 2.203s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 7.643136, mae: 41.634365, mean_q: -61.764481\n"," 103600/300000: episode: 518, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 6.656793, mae: 41.738468, mean_q: -61.975266\n"," 103800/300000: episode: 519, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.850008, mae: 41.788170, mean_q: -61.911922\n"," 104000/300000: episode: 520, duration: 2.269s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 8.051605, mae: 41.938862, mean_q: -62.125153\n"," 104200/300000: episode: 521, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 10.065686, mae: 42.034279, mean_q: -62.292534\n"," 104400/300000: episode: 522, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 5.856945, mae: 41.916092, mean_q: -62.305157\n"," 104600/300000: episode: 523, duration: 2.109s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.693079, mae: 42.021099, mean_q: -62.296146\n"," 104800/300000: episode: 524, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.448461, mae: 42.141544, mean_q: -62.570526\n"," 105000/300000: episode: 525, duration: 2.102s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.178533, mae: 42.103447, mean_q: -62.436539\n"," 105200/300000: episode: 526, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 8.050153, mae: 42.196583, mean_q: -62.579556\n"," 105400/300000: episode: 527, duration: 2.204s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.673650, mae: 42.167801, mean_q: -62.408329\n"," 105600/300000: episode: 528, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.850488, mae: 42.261528, mean_q: -62.704220\n"," 105800/300000: episode: 529, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 7.437400, mae: 42.156864, mean_q: -62.435596\n"," 106000/300000: episode: 530, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.031997, mae: 42.262909, mean_q: -62.714386\n"," 106200/300000: episode: 531, duration: 2.228s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 5.957757, mae: 42.304787, mean_q: -62.843006\n"," 106400/300000: episode: 532, duration: 2.199s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 7.649004, mae: 42.479198, mean_q: -63.056099\n"," 106600/300000: episode: 533, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 10.935299, mae: 42.506329, mean_q: -62.889977\n"," 106800/300000: episode: 534, duration: 2.221s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 11.223569, mae: 42.297630, mean_q: -62.602604\n"," 107000/300000: episode: 535, duration: 2.236s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 11.280222, mae: 42.210308, mean_q: -62.465370\n"," 107200/300000: episode: 536, duration: 2.302s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 11.862893, mae: 42.067558, mean_q: -62.266109\n"," 107400/300000: episode: 537, duration: 2.295s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 11.157577, mae: 42.078320, mean_q: -62.335106\n"," 107600/300000: episode: 538, duration: 2.284s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 10.094936, mae: 42.015938, mean_q: -62.305458\n"," 107800/300000: episode: 539, duration: 2.258s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.337825, mae: 42.083897, mean_q: -62.440258\n"," 108000/300000: episode: 540, duration: 2.266s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 9.757339, mae: 41.972149, mean_q: -62.162540\n"," 108200/300000: episode: 541, duration: 2.290s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.499874, mae: 41.952606, mean_q: -62.150410\n"," 108400/300000: episode: 542, duration: 2.285s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.329323, mae: 42.035030, mean_q: -62.507736\n"," 108600/300000: episode: 543, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 8.935901, mae: 42.087955, mean_q: -62.454258\n"," 108800/300000: episode: 544, duration: 2.282s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 10.507892, mae: 42.033791, mean_q: -62.291435\n"," 109000/300000: episode: 545, duration: 2.307s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 11.065308, mae: 42.026455, mean_q: -62.181377\n"," 109200/300000: episode: 546, duration: 2.284s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 11.849597, mae: 41.863468, mean_q: -61.990734\n"," 109400/300000: episode: 547, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.583945, mae: 41.878174, mean_q: -62.137909\n"," 109600/300000: episode: 548, duration: 2.225s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.583794, mae: 41.866730, mean_q: -62.033524\n"," 109800/300000: episode: 549, duration: 2.319s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.827724, mae: 41.942245, mean_q: -62.270000\n"," 110000/300000: episode: 550, duration: 2.345s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.070700, mae: 42.016075, mean_q: -62.227501\n"," 110200/300000: episode: 551, duration: 2.327s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 9.795676, mae: 41.955982, mean_q: -62.148766\n"," 110400/300000: episode: 552, duration: 2.319s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 8.549548, mae: 42.008026, mean_q: -62.270226\n"," 110600/300000: episode: 553, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 8.847836, mae: 41.937855, mean_q: -62.110928\n"," 110800/300000: episode: 554, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.525722, mae: 41.940956, mean_q: -62.272594\n"," 111000/300000: episode: 555, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.849112, mae: 42.057308, mean_q: -62.387630\n"," 111200/300000: episode: 556, duration: 2.224s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 10.577009, mae: 42.161816, mean_q: -62.398525\n"," 111400/300000: episode: 557, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 6.228098, mae: 42.120136, mean_q: -62.552197\n"," 111600/300000: episode: 558, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.257321, mae: 42.280304, mean_q: -62.734604\n"," 111800/300000: episode: 559, duration: 2.167s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 9.633687, mae: 42.361496, mean_q: -62.800068\n"," 112000/300000: episode: 560, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 10.261165, mae: 42.251781, mean_q: -62.618263\n"," 112200/300000: episode: 561, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.770316, mae: 42.258190, mean_q: -62.693592\n"," 112400/300000: episode: 562, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 4.837098, mae: 42.453350, mean_q: -63.101574\n"," 112600/300000: episode: 563, duration: 2.267s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 12.904865, mae: 42.439476, mean_q: -62.756313\n"," 112800/300000: episode: 564, duration: 2.208s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.601906, mae: 42.491688, mean_q: -62.965401\n"," 113000/300000: episode: 565, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 9.273423, mae: 42.493217, mean_q: -62.933720\n"," 113200/300000: episode: 566, duration: 2.208s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.940779, mae: 42.467636, mean_q: -62.948254\n"," 113400/300000: episode: 567, duration: 2.290s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 11.946262, mae: 42.539520, mean_q: -63.046787\n"," 113600/300000: episode: 568, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 8.198835, mae: 42.490047, mean_q: -62.956982\n"," 113800/300000: episode: 569, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 7.798490, mae: 42.609581, mean_q: -63.223007\n"," 114000/300000: episode: 570, duration: 2.276s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 8.498016, mae: 42.582432, mean_q: -63.151199\n"," 114200/300000: episode: 571, duration: 2.265s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.297362, mae: 42.702499, mean_q: -63.428768\n"," 114400/300000: episode: 572, duration: 2.304s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.215510, mae: 42.762257, mean_q: -63.404980\n"," 114600/300000: episode: 573, duration: 2.284s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 8.359171, mae: 42.713085, mean_q: -63.306416\n"," 114800/300000: episode: 574, duration: 2.275s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 10.749787, mae: 42.766865, mean_q: -63.327816\n"," 115000/300000: episode: 575, duration: 2.308s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 9.970014, mae: 42.612137, mean_q: -63.172138\n"," 115200/300000: episode: 576, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.299141, mae: 42.705494, mean_q: -63.344860\n"," 115400/300000: episode: 577, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 8.881625, mae: 42.770309, mean_q: -63.341122\n"," 115600/300000: episode: 578, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 12.121171, mae: 42.524803, mean_q: -62.861515\n"," 115800/300000: episode: 579, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 9.219112, mae: 42.617294, mean_q: -63.153057\n"," 116000/300000: episode: 580, duration: 2.239s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 12.032892, mae: 42.487186, mean_q: -62.897972\n"," 116200/300000: episode: 581, duration: 2.237s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 10.522104, mae: 42.361008, mean_q: -62.687992\n"," 116400/300000: episode: 582, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.202113, mae: 42.367332, mean_q: -62.826202\n"," 116600/300000: episode: 583, duration: 2.208s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 11.632086, mae: 42.209530, mean_q: -62.508621\n"," 116800/300000: episode: 584, duration: 2.238s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 9.875696, mae: 42.124523, mean_q: -62.373379\n"," 117000/300000: episode: 585, duration: 2.260s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 9.699216, mae: 42.016903, mean_q: -62.242119\n"," 117200/300000: episode: 586, duration: 2.250s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 10.081032, mae: 41.971489, mean_q: -62.208076\n"," 117400/300000: episode: 587, duration: 2.226s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.712827, mae: 41.925018, mean_q: -62.159931\n"," 117600/300000: episode: 588, duration: 2.227s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 9.900193, mae: 41.981075, mean_q: -62.167137\n"," 117800/300000: episode: 589, duration: 2.242s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 8.285206, mae: 41.862324, mean_q: -62.058350\n"," 118000/300000: episode: 590, duration: 2.272s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 8.020503, mae: 41.823143, mean_q: -62.028877\n"," 118200/300000: episode: 591, duration: 2.226s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 10.491694, mae: 41.814198, mean_q: -62.015060\n"," 118400/300000: episode: 592, duration: 2.237s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 11.065987, mae: 41.765919, mean_q: -61.897072\n"," 118600/300000: episode: 593, duration: 2.217s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 9.819280, mae: 41.650558, mean_q: -61.632801\n"," 118800/300000: episode: 594, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.419461, mae: 41.815029, mean_q: -61.957085\n"," 119000/300000: episode: 595, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.505135, mae: 41.912197, mean_q: -62.054432\n"," 119200/300000: episode: 596, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.705520, mae: 41.833797, mean_q: -62.046612\n"," 119400/300000: episode: 597, duration: 2.198s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 9.534428, mae: 41.829269, mean_q: -62.038433\n"," 119600/300000: episode: 598, duration: 2.139s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 12.016021, mae: 41.876408, mean_q: -61.959385\n"," 119800/300000: episode: 599, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.896599, mae: 41.884243, mean_q: -62.228451\n"," 120000/300000: episode: 600, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.528574, mae: 41.839893, mean_q: -61.988888\n"," 120200/300000: episode: 601, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.557823, mae: 41.962627, mean_q: -62.172871\n"," 120400/300000: episode: 602, duration: 2.240s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 10.839922, mae: 41.928043, mean_q: -62.128361\n"," 120600/300000: episode: 603, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 12.719683, mae: 41.834591, mean_q: -61.783764\n"," 120800/300000: episode: 604, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 9.198265, mae: 41.725861, mean_q: -61.919819\n"," 121000/300000: episode: 605, duration: 2.190s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 10.351759, mae: 41.682423, mean_q: -61.812584\n"," 121200/300000: episode: 606, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 8.158189, mae: 41.745323, mean_q: -61.852901\n"," 121400/300000: episode: 607, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 8.810849, mae: 41.783485, mean_q: -61.930859\n"," 121600/300000: episode: 608, duration: 2.217s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.780199, mae: 41.762978, mean_q: -61.921387\n"," 121800/300000: episode: 609, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.593834, mae: 41.790504, mean_q: -61.899582\n"," 122000/300000: episode: 610, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.167665, mae: 41.688179, mean_q: -61.826893\n"," 122200/300000: episode: 611, duration: 2.205s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 9.353715, mae: 41.886261, mean_q: -62.058525\n"," 122400/300000: episode: 612, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 7.066683, mae: 41.880394, mean_q: -62.141010\n"," 122600/300000: episode: 613, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 7.585890, mae: 42.028023, mean_q: -62.330429\n"," 122800/300000: episode: 614, duration: 2.204s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.202620, mae: 41.943924, mean_q: -62.145920\n"," 123000/300000: episode: 615, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.738600, mae: 42.025875, mean_q: -62.331608\n"," 123200/300000: episode: 616, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.430297, mae: 42.022686, mean_q: -62.305206\n"," 123400/300000: episode: 617, duration: 2.204s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.995555, mae: 42.103428, mean_q: -62.457443\n"," 123600/300000: episode: 618, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.139103, mae: 42.073360, mean_q: -62.397038\n"," 123800/300000: episode: 619, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.282769, mae: 42.114723, mean_q: -62.502941\n"," 124000/300000: episode: 620, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.463519, mae: 42.047684, mean_q: -62.269482\n"," 124200/300000: episode: 621, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.838310, mae: 41.947647, mean_q: -62.154316\n"," 124400/300000: episode: 622, duration: 2.200s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 6.985215, mae: 41.922535, mean_q: -62.177929\n"," 124600/300000: episode: 623, duration: 2.216s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.478386, mae: 41.879696, mean_q: -61.967541\n"," 124800/300000: episode: 624, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.821768, mae: 41.789696, mean_q: -61.959053\n"," 125000/300000: episode: 625, duration: 2.204s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 9.478612, mae: 42.038116, mean_q: -62.251354\n"," 125200/300000: episode: 626, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.400320, mae: 42.007183, mean_q: -62.309547\n"," 125400/300000: episode: 627, duration: 2.218s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.608786, mae: 41.725113, mean_q: -61.844151\n"," 125600/300000: episode: 628, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 5.416746, mae: 41.969589, mean_q: -62.271400\n"," 125800/300000: episode: 629, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 10.487290, mae: 41.869442, mean_q: -61.915771\n"," 126000/300000: episode: 630, duration: 2.144s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.277143, mae: 41.770432, mean_q: -61.990421\n"," 126200/300000: episode: 631, duration: 2.198s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.927527, mae: 41.922211, mean_q: -62.177513\n"," 126400/300000: episode: 632, duration: 2.221s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 6.786971, mae: 42.008133, mean_q: -62.299049\n"," 126600/300000: episode: 633, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.623159, mae: 41.953182, mean_q: -62.116085\n"," 126800/300000: episode: 634, duration: 2.250s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 6.499479, mae: 42.013153, mean_q: -62.307735\n"," 127000/300000: episode: 635, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 9.178951, mae: 42.044659, mean_q: -62.317764\n"," 127200/300000: episode: 636, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 9.092657, mae: 41.937447, mean_q: -62.065617\n"," 127400/300000: episode: 637, duration: 2.256s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.849102, mae: 41.992420, mean_q: -62.226959\n"," 127600/300000: episode: 638, duration: 2.264s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 7.963806, mae: 41.927559, mean_q: -62.110107\n"," 127800/300000: episode: 639, duration: 2.305s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.021832, mae: 41.910053, mean_q: -62.101803\n"," 128000/300000: episode: 640, duration: 2.278s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.723635, mae: 41.912582, mean_q: -62.087162\n"," 128200/300000: episode: 641, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 8.731288, mae: 41.759045, mean_q: -61.754444\n"," 128400/300000: episode: 642, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 9.153489, mae: 41.853374, mean_q: -62.007755\n"," 128600/300000: episode: 643, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.430237, mae: 41.685429, mean_q: -61.696327\n"," 128800/300000: episode: 644, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 6.312057, mae: 41.649715, mean_q: -61.708954\n"," 129000/300000: episode: 645, duration: 2.186s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 8.234793, mae: 41.504093, mean_q: -61.467949\n"," 129200/300000: episode: 646, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 9.604851, mae: 41.362118, mean_q: -61.165436\n"," 129400/300000: episode: 647, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 11.657249, mae: 41.249989, mean_q: -60.900803\n"," 129600/300000: episode: 648, duration: 2.189s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 8.109765, mae: 41.065639, mean_q: -60.811840\n"," 129800/300000: episode: 649, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 6.986784, mae: 40.929108, mean_q: -60.692730\n"," 130000/300000: episode: 650, duration: 2.200s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.439866, mae: 41.004101, mean_q: -60.704853\n"," 130200/300000: episode: 651, duration: 2.150s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 6.134252, mae: 41.020496, mean_q: -60.827675\n"," 130400/300000: episode: 652, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.919248, mae: 41.118427, mean_q: -60.860001\n"," 130600/300000: episode: 653, duration: 2.206s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 7.911150, mae: 40.890873, mean_q: -60.471542\n"," 130800/300000: episode: 654, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.266829, mae: 40.831692, mean_q: -60.497311\n"," 131000/300000: episode: 655, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.728917, mae: 40.917694, mean_q: -60.625111\n"," 131200/300000: episode: 656, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.441198, mae: 40.844635, mean_q: -60.412033\n"," 131400/300000: episode: 657, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.521322, mae: 40.858345, mean_q: -60.529327\n"," 131600/300000: episode: 658, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 9.389261, mae: 41.073940, mean_q: -60.733604\n"," 131800/300000: episode: 659, duration: 2.157s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 7.092475, mae: 40.847042, mean_q: -60.560184\n"," 132000/300000: episode: 660, duration: 2.130s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 9.948261, mae: 40.833763, mean_q: -60.390507\n"," 132200/300000: episode: 661, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 8.194210, mae: 40.838017, mean_q: -60.490669\n"," 132400/300000: episode: 662, duration: 2.237s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.836288, mae: 40.754646, mean_q: -60.336826\n"," 132600/300000: episode: 663, duration: 2.136s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 7.588826, mae: 40.824406, mean_q: -60.438496\n"," 132800/300000: episode: 664, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.918420, mae: 40.858341, mean_q: -60.559521\n"," 133000/300000: episode: 665, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.559086, mae: 40.879646, mean_q: -60.549194\n"," 133200/300000: episode: 666, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.566348, mae: 40.787350, mean_q: -60.324539\n"," 133400/300000: episode: 667, duration: 2.232s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 7.059257, mae: 40.832874, mean_q: -60.437725\n"," 133600/300000: episode: 668, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 7.735689, mae: 40.773434, mean_q: -60.421642\n"," 133800/300000: episode: 669, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.428643, mae: 40.762032, mean_q: -60.382065\n"," 134000/300000: episode: 670, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 9.735324, mae: 40.737431, mean_q: -60.287663\n"," 134200/300000: episode: 671, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.789867, mae: 40.746632, mean_q: -60.294247\n"," 134400/300000: episode: 672, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 9.431184, mae: 40.830425, mean_q: -60.356579\n"," 134600/300000: episode: 673, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.332130, mae: 40.615822, mean_q: -60.142246\n"," 134800/300000: episode: 674, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.374516, mae: 40.756084, mean_q: -60.373390\n"," 135000/300000: episode: 675, duration: 2.224s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 9.262444, mae: 40.661179, mean_q: -60.067257\n"," 135200/300000: episode: 676, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 8.172535, mae: 40.535309, mean_q: -60.022667\n"," 135400/300000: episode: 677, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 10.050205, mae: 40.581566, mean_q: -59.957958\n"," 135600/300000: episode: 678, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.852255, mae: 40.474300, mean_q: -59.825687\n"," 135800/300000: episode: 679, duration: 2.222s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.894770, mae: 40.464420, mean_q: -59.763672\n"," 136000/300000: episode: 680, duration: 2.150s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 11.343659, mae: 40.300053, mean_q: -59.544731\n"," 136200/300000: episode: 681, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 7.481434, mae: 40.255531, mean_q: -59.586113\n"," 136400/300000: episode: 682, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 8.121411, mae: 40.327675, mean_q: -59.706367\n"," 136600/300000: episode: 683, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 8.702032, mae: 40.485165, mean_q: -59.834835\n"," 136800/300000: episode: 684, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 8.104327, mae: 40.414600, mean_q: -59.819279\n"," 137000/300000: episode: 685, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.155594, mae: 40.514847, mean_q: -60.026039\n"," 137200/300000: episode: 686, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 6.698212, mae: 40.552994, mean_q: -60.057892\n"," 137400/300000: episode: 687, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.327818, mae: 40.465763, mean_q: -59.948994\n"," 137600/300000: episode: 688, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 8.456246, mae: 40.395184, mean_q: -59.717003\n"," 137800/300000: episode: 689, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 9.789851, mae: 40.323257, mean_q: -59.628750\n"," 138000/300000: episode: 690, duration: 2.326s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 7.708594, mae: 40.332272, mean_q: -59.691803\n"," 138200/300000: episode: 691, duration: 2.316s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 7.209120, mae: 40.335850, mean_q: -59.757599\n"," 138400/300000: episode: 692, duration: 2.290s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 5.354516, mae: 40.419838, mean_q: -59.950386\n"," 138600/300000: episode: 693, duration: 2.271s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 8.959009, mae: 40.399399, mean_q: -59.699776\n"," 138800/300000: episode: 694, duration: 2.269s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 9.208205, mae: 40.293865, mean_q: -59.655621\n"," 139000/300000: episode: 695, duration: 2.296s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 8.259171, mae: 40.355457, mean_q: -59.761993\n"," 139200/300000: episode: 696, duration: 2.289s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 6.934396, mae: 40.289722, mean_q: -59.703663\n"," 139400/300000: episode: 697, duration: 2.280s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.673344, mae: 40.421082, mean_q: -59.872295\n"," 139600/300000: episode: 698, duration: 2.276s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 6.955487, mae: 40.609470, mean_q: -60.145252\n"," 139800/300000: episode: 699, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.459326, mae: 40.554871, mean_q: -59.932480\n"," 140000/300000: episode: 700, duration: 2.157s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 8.006408, mae: 40.424961, mean_q: -59.767685\n"," 140200/300000: episode: 701, duration: 2.215s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 6.442908, mae: 40.276596, mean_q: -59.682793\n"," 140400/300000: episode: 702, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 6.876139, mae: 40.314491, mean_q: -59.665840\n"," 140600/300000: episode: 703, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.582223, mae: 40.133263, mean_q: -59.441586\n"," 140800/300000: episode: 704, duration: 2.224s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 5.288850, mae: 40.318459, mean_q: -59.750404\n"," 141000/300000: episode: 705, duration: 2.216s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 7.308911, mae: 40.290985, mean_q: -59.598526\n"," 141200/300000: episode: 706, duration: 2.172s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 6.698693, mae: 40.330605, mean_q: -59.626122\n"," 141400/300000: episode: 707, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 7.327305, mae: 40.198490, mean_q: -59.465366\n"," 141600/300000: episode: 708, duration: 2.223s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.617661, mae: 40.163242, mean_q: -59.291019\n"," 141800/300000: episode: 709, duration: 2.208s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 6.056918, mae: 39.941994, mean_q: -59.139454\n"," 142000/300000: episode: 710, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.014538, mae: 39.933640, mean_q: -58.984966\n"," 142200/300000: episode: 711, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.950137, mae: 39.582977, mean_q: -58.445545\n"," 142400/300000: episode: 712, duration: 2.233s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.492975, mae: 39.524548, mean_q: -58.352428\n"," 142600/300000: episode: 713, duration: 2.221s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.215199, mae: 39.453541, mean_q: -58.212196\n"," 142800/300000: episode: 714, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 5.973823, mae: 39.123875, mean_q: -57.801491\n"," 143000/300000: episode: 715, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 7.402874, mae: 38.809799, mean_q: -57.166172\n"," 143200/300000: episode: 716, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 7.055533, mae: 38.268803, mean_q: -56.338551\n"," 143400/300000: episode: 717, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.406097, mae: 37.848976, mean_q: -55.753677\n"," 143600/300000: episode: 718, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 3.587794, mae: 37.598698, mean_q: -55.428951\n"," 143800/300000: episode: 719, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 4.588653, mae: 37.440659, mean_q: -55.177658\n"," 144000/300000: episode: 720, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 5.179436, mae: 37.012653, mean_q: -54.509308\n"," 144200/300000: episode: 721, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 5.852982, mae: 36.413425, mean_q: -53.448048\n"," 144400/300000: episode: 722, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 6.287258, mae: 35.866776, mean_q: -52.628231\n"," 144600/300000: episode: 723, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 6.451898, mae: 35.438656, mean_q: -51.954674\n"," 144800/300000: episode: 724, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 4.715371, mae: 35.017258, mean_q: -51.372597\n"," 145000/300000: episode: 725, duration: 2.106s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 4.698061, mae: 34.587090, mean_q: -50.706329\n"," 145200/300000: episode: 726, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 4.006770, mae: 34.008312, mean_q: -49.833168\n"," 145400/300000: episode: 727, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 6.298508, mae: 33.495419, mean_q: -48.903831\n"," 145600/300000: episode: 728, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 5.398179, mae: 33.014698, mean_q: -48.285992\n"," 145800/300000: episode: 729, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 5.113936, mae: 32.563656, mean_q: -47.622147\n"," 146000/300000: episode: 730, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 4.525072, mae: 32.068459, mean_q: -46.884464\n"," 146200/300000: episode: 731, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 3.890084, mae: 31.576660, mean_q: -46.208916\n"," 146400/300000: episode: 732, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 4.245219, mae: 31.076012, mean_q: -45.452950\n"," 146600/300000: episode: 733, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 4.625095, mae: 30.817070, mean_q: -44.999908\n"," 146800/300000: episode: 734, duration: 2.224s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 4.061120, mae: 30.456305, mean_q: -44.525089\n"," 147000/300000: episode: 735, duration: 2.228s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.455 [0.000, 2.000],  loss: 5.457541, mae: 29.789505, mean_q: -43.415909\n"," 147200/300000: episode: 736, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 4.190053, mae: 29.482332, mean_q: -43.097557\n"," 147400/300000: episode: 737, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.375 [0.000, 2.000],  loss: 5.127224, mae: 29.085186, mean_q: -42.356319\n"," 147600/300000: episode: 738, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000],  loss: 3.997880, mae: 28.829947, mean_q: -42.178158\n"," 147800/300000: episode: 739, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 3.616838, mae: 28.541109, mean_q: -41.677914\n"," 148000/300000: episode: 740, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 3.622844, mae: 28.282192, mean_q: -41.436974\n"," 148200/300000: episode: 741, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000],  loss: 3.455081, mae: 28.105345, mean_q: -41.098709\n"," 148400/300000: episode: 742, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 3.608570, mae: 27.773308, mean_q: -40.600277\n"," 148600/300000: episode: 743, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 3.039375, mae: 27.327286, mean_q: -39.996708\n"," 148800/300000: episode: 744, duration: 2.173s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.224986, mae: 27.382357, mean_q: -40.202118\n"," 149000/300000: episode: 745, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 4.492586, mae: 27.112499, mean_q: -39.659489\n"," 149200/300000: episode: 746, duration: 2.082s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000],  loss: 3.939502, mae: 27.067638, mean_q: -39.541409\n"," 149400/300000: episode: 747, duration: 2.114s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 3.836182, mae: 26.995684, mean_q: -39.530758\n"," 149600/300000: episode: 748, duration: 2.108s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 1.885027, mae: 26.954073, mean_q: -39.569687\n"," 149800/300000: episode: 749, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 3.657653, mae: 26.753019, mean_q: -39.146751\n"," 150000/300000: episode: 750, duration: 2.216s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 3.797352, mae: 26.788603, mean_q: -39.230247\n"," 150200/300000: episode: 751, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 4.409319, mae: 26.639229, mean_q: -39.007915\n"," 150400/300000: episode: 752, duration: 2.225s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.843941, mae: 26.582270, mean_q: -38.988636\n"," 150600/300000: episode: 753, duration: 2.267s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 3.126798, mae: 26.432428, mean_q: -38.779476\n"," 150800/300000: episode: 754, duration: 2.239s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 3.697173, mae: 26.404076, mean_q: -38.696442\n"," 151000/300000: episode: 755, duration: 2.241s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.132578, mae: 26.494156, mean_q: -38.942215\n"," 151200/300000: episode: 756, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.819292, mae: 26.422270, mean_q: -38.735050\n"," 151400/300000: episode: 757, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.098551, mae: 26.465811, mean_q: -38.825211\n"," 151600/300000: episode: 758, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.154121, mae: 26.406738, mean_q: -38.734837\n"," 151800/300000: episode: 759, duration: 2.190s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 3.121765, mae: 26.355040, mean_q: -38.633537\n"," 152000/300000: episode: 760, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 2.496414, mae: 26.417166, mean_q: -38.726120\n"," 152200/300000: episode: 761, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 2.310857, mae: 26.282619, mean_q: -38.572437\n"," 152400/300000: episode: 762, duration: 2.247s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.793612, mae: 26.312159, mean_q: -38.470284\n"," 152600/300000: episode: 763, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.889830, mae: 26.169109, mean_q: -38.368755\n"," 152800/300000: episode: 764, duration: 2.115s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.536633, mae: 26.255211, mean_q: -38.422764\n"," 153000/300000: episode: 765, duration: 2.129s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.778962, mae: 26.195269, mean_q: -38.323994\n"," 153200/300000: episode: 766, duration: 2.069s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 2.625671, mae: 26.106462, mean_q: -38.269917\n"," 153400/300000: episode: 767, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.263050, mae: 25.987944, mean_q: -37.987808\n"," 153600/300000: episode: 768, duration: 2.032s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.633757, mae: 25.947412, mean_q: -37.965919\n"," 153800/300000: episode: 769, duration: 2.129s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.467648, mae: 25.615278, mean_q: -37.534409\n"," 154000/300000: episode: 770, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 3.435736, mae: 25.598589, mean_q: -37.424702\n"," 154200/300000: episode: 771, duration: 2.043s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 3.060043, mae: 25.615894, mean_q: -37.492496\n"," 154400/300000: episode: 772, duration: 2.097s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.974028, mae: 25.464972, mean_q: -37.272892\n"," 154600/300000: episode: 773, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 2.625715, mae: 25.341543, mean_q: -37.056332\n"," 154800/300000: episode: 774, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 3.657043, mae: 25.267921, mean_q: -36.858849\n"," 155000/300000: episode: 775, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.564976, mae: 25.205132, mean_q: -36.857288\n"," 155200/300000: episode: 776, duration: 2.076s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.719187, mae: 25.234474, mean_q: -36.961430\n"," 155400/300000: episode: 777, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.009394, mae: 25.144932, mean_q: -36.833988\n"," 155600/300000: episode: 778, duration: 2.061s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 2.314717, mae: 25.065096, mean_q: -36.779686\n"," 155800/300000: episode: 779, duration: 2.049s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 2.639361, mae: 25.004091, mean_q: -36.653091\n"," 156000/300000: episode: 780, duration: 2.081s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 2.913634, mae: 25.193777, mean_q: -36.969574\n"," 156200/300000: episode: 781, duration: 2.130s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 3.272699, mae: 25.221306, mean_q: -37.049999\n"," 156400/300000: episode: 782, duration: 2.066s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.403383, mae: 25.446968, mean_q: -37.488243\n"," 156600/300000: episode: 783, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 3.143297, mae: 25.681480, mean_q: -37.799469\n"," 156800/300000: episode: 784, duration: 2.145s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 3.051034, mae: 25.861965, mean_q: -38.093178\n"," 157000/300000: episode: 785, duration: 2.200s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 3.542898, mae: 25.928917, mean_q: -38.173580\n"," 157200/300000: episode: 786, duration: 2.198s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 3.487291, mae: 26.078009, mean_q: -38.420166\n"," 157400/300000: episode: 787, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 3.848806, mae: 26.267197, mean_q: -38.674049\n"," 157600/300000: episode: 788, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 3.182048, mae: 26.603989, mean_q: -39.264267\n"," 157800/300000: episode: 789, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 2.629450, mae: 26.776152, mean_q: -39.538052\n"," 158000/300000: episode: 790, duration: 2.115s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.195964, mae: 26.965479, mean_q: -39.814701\n"," 158200/300000: episode: 791, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 3.004097, mae: 27.169994, mean_q: -40.044327\n"," 158400/300000: episode: 792, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 3.525141, mae: 27.362921, mean_q: -40.371918\n"," 158600/300000: episode: 793, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.522144, mae: 27.540178, mean_q: -40.642990\n"," 158800/300000: episode: 794, duration: 2.157s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 4.346579, mae: 27.875498, mean_q: -41.028500\n"," 159000/300000: episode: 795, duration: 2.240s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 3.637160, mae: 27.994053, mean_q: -41.224117\n"," 159200/300000: episode: 796, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 3.806310, mae: 27.961164, mean_q: -41.167450\n"," 159400/300000: episode: 797, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 3.492346, mae: 28.198828, mean_q: -41.559940\n"," 159600/300000: episode: 798, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 3.271941, mae: 28.126387, mean_q: -41.401104\n"," 159800/300000: episode: 799, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 4.188874, mae: 28.311800, mean_q: -41.698048\n"," 160000/300000: episode: 800, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 3.868819, mae: 28.343748, mean_q: -41.737167\n"," 160200/300000: episode: 801, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 2.963607, mae: 28.450588, mean_q: -41.916138\n"," 160400/300000: episode: 802, duration: 2.203s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 3.813493, mae: 28.612339, mean_q: -42.096146\n"," 160600/300000: episode: 803, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 4.757959, mae: 28.592775, mean_q: -41.979713\n"," 160800/300000: episode: 804, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 2.403738, mae: 28.711050, mean_q: -42.306904\n"," 161000/300000: episode: 805, duration: 2.200s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 3.846535, mae: 28.812796, mean_q: -42.345654\n"," 161200/300000: episode: 806, duration: 2.172s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 3.790291, mae: 28.716354, mean_q: -42.189022\n"," 161400/300000: episode: 807, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 3.737453, mae: 28.625847, mean_q: -42.083431\n"," 161600/300000: episode: 808, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 2.674705, mae: 28.684990, mean_q: -42.218330\n"," 161800/300000: episode: 809, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000],  loss: 4.447442, mae: 28.701611, mean_q: -42.073277\n"," 162000/300000: episode: 810, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 3.837310, mae: 28.693419, mean_q: -42.101418\n"," 162200/300000: episode: 811, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 3.024860, mae: 28.600588, mean_q: -42.144371\n"," 162400/300000: episode: 812, duration: 2.215s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 3.990970, mae: 28.645979, mean_q: -42.156456\n"," 162600/300000: episode: 813, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 5.708822, mae: 28.956848, mean_q: -42.587509\n"," 162800/300000: episode: 814, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 3.175795, mae: 29.044233, mean_q: -42.855713\n"," 163000/300000: episode: 815, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 3.395048, mae: 29.254351, mean_q: -43.261044\n"," 163200/300000: episode: 816, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 3.369686, mae: 29.567017, mean_q: -43.708179\n"," 163400/300000: episode: 817, duration: 2.261s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 3.079210, mae: 29.870449, mean_q: -44.164757\n"," 163600/300000: episode: 818, duration: 2.261s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 3.900286, mae: 30.195032, mean_q: -44.637779\n"," 163800/300000: episode: 819, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 4.306696, mae: 30.274937, mean_q: -44.780659\n"," 164000/300000: episode: 820, duration: 2.130s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 3.119284, mae: 30.603966, mean_q: -45.258633\n"," 164200/300000: episode: 821, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.047019, mae: 30.715881, mean_q: -45.213165\n"," 164400/300000: episode: 822, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 5.771827, mae: 30.739180, mean_q: -45.234058\n"," 164600/300000: episode: 823, duration: 2.129s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 3.920564, mae: 30.701086, mean_q: -45.304676\n"," 164800/300000: episode: 824, duration: 2.172s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 4.501906, mae: 30.755823, mean_q: -45.389736\n"," 165000/300000: episode: 825, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 4.374143, mae: 30.735884, mean_q: -45.354496\n"," 165200/300000: episode: 826, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.952709, mae: 30.858980, mean_q: -45.606720\n"," 165400/300000: episode: 827, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 5.202931, mae: 30.946558, mean_q: -45.518959\n"," 165600/300000: episode: 828, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 4.167655, mae: 30.960373, mean_q: -45.638649\n"," 165800/300000: episode: 829, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.431392, mae: 30.996704, mean_q: -45.711788\n"," 166000/300000: episode: 830, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 4.007041, mae: 30.891020, mean_q: -45.483665\n"," 166200/300000: episode: 831, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 4.301810, mae: 30.860340, mean_q: -45.393681\n"," 166400/300000: episode: 832, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 5.000779, mae: 30.703930, mean_q: -45.104576\n"," 166600/300000: episode: 833, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.255856, mae: 30.665474, mean_q: -45.140068\n"," 166800/300000: episode: 834, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 3.972319, mae: 30.463448, mean_q: -44.836395\n"," 167000/300000: episode: 835, duration: 2.222s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 3.297495, mae: 30.638735, mean_q: -45.148800\n"," 167200/300000: episode: 836, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 4.530857, mae: 30.500767, mean_q: -44.841202\n"," 167400/300000: episode: 837, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 4.080428, mae: 30.340836, mean_q: -44.633877\n"," 167600/300000: episode: 838, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.994406, mae: 30.297163, mean_q: -44.600353\n"," 167800/300000: episode: 839, duration: 2.331s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 3.128819, mae: 30.242136, mean_q: -44.549469\n"," 168000/300000: episode: 840, duration: 2.271s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 4.624653, mae: 30.134192, mean_q: -44.275623\n"," 168200/300000: episode: 841, duration: 2.367s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 4.377698, mae: 30.119350, mean_q: -44.295536\n"," 168400/300000: episode: 842, duration: 2.250s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 3.134654, mae: 29.876884, mean_q: -43.975956\n"," 168600/300000: episode: 843, duration: 2.272s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 5.385443, mae: 29.888777, mean_q: -43.880131\n"," 168800/300000: episode: 844, duration: 2.261s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 3.461360, mae: 29.730846, mean_q: -43.790157\n"," 169000/300000: episode: 845, duration: 2.248s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.531262, mae: 29.724287, mean_q: -43.722092\n"," 169200/300000: episode: 846, duration: 2.362s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 3.873565, mae: 29.786367, mean_q: -43.920376\n"," 169400/300000: episode: 847, duration: 2.241s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 4.315706, mae: 29.752615, mean_q: -43.775429\n"," 169600/300000: episode: 848, duration: 2.199s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 3.313076, mae: 29.683365, mean_q: -43.743954\n"," 169800/300000: episode: 849, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 3.309734, mae: 29.711920, mean_q: -43.764084\n"," 170000/300000: episode: 850, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.837998, mae: 29.879698, mean_q: -44.106102\n"," 170200/300000: episode: 851, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 3.096754, mae: 29.790413, mean_q: -43.996281\n"," 170400/300000: episode: 852, duration: 2.186s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.527555, mae: 29.921148, mean_q: -44.186798\n"," 170600/300000: episode: 853, duration: 2.222s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 4.063243, mae: 29.950943, mean_q: -44.133823\n"," 170800/300000: episode: 854, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 3.816098, mae: 29.968599, mean_q: -44.188629\n"," 171000/300000: episode: 855, duration: 2.101s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 3.967283, mae: 29.979607, mean_q: -44.166847\n"," 171200/300000: episode: 856, duration: 2.129s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 4.111160, mae: 29.891695, mean_q: -43.965561\n"," 171400/300000: episode: 857, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 4.253153, mae: 29.922844, mean_q: -44.078915\n"," 171600/300000: episode: 858, duration: 2.186s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.202226, mae: 29.787193, mean_q: -43.930714\n"," 171800/300000: episode: 859, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 3.824583, mae: 29.721209, mean_q: -43.716408\n"," 172000/300000: episode: 860, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 3.591692, mae: 29.657181, mean_q: -43.649967\n"," 172200/300000: episode: 861, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 4.252459, mae: 29.501854, mean_q: -43.378658\n"," 172400/300000: episode: 862, duration: 2.107s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 4.168247, mae: 29.360746, mean_q: -43.200401\n"," 172600/300000: episode: 863, duration: 2.109s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 3.449630, mae: 29.308874, mean_q: -43.195744\n"," 172800/300000: episode: 864, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 3.405157, mae: 29.199305, mean_q: -42.996162\n"," 173000/300000: episode: 865, duration: 2.066s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 2.874650, mae: 29.247553, mean_q: -43.096222\n"," 173200/300000: episode: 866, duration: 2.153s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.430440, mae: 29.140034, mean_q: -42.894432\n"," 173400/300000: episode: 867, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 3.776383, mae: 29.130302, mean_q: -42.832237\n"," 173600/300000: episode: 868, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 3.146102, mae: 29.040352, mean_q: -42.818897\n"," 173800/300000: episode: 869, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 4.419819, mae: 29.062353, mean_q: -42.724472\n"," 174000/300000: episode: 870, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.459867, mae: 28.970339, mean_q: -42.709194\n"," 174200/300000: episode: 871, duration: 2.162s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 3.538151, mae: 28.962296, mean_q: -42.722626\n"," 174400/300000: episode: 872, duration: 2.189s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.118311, mae: 29.057581, mean_q: -42.818150\n"," 174600/300000: episode: 873, duration: 2.206s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 2.994174, mae: 29.014158, mean_q: -42.764400\n"," 174800/300000: episode: 874, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 3.060977, mae: 28.993679, mean_q: -42.818958\n"," 175000/300000: episode: 875, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.333472, mae: 29.165176, mean_q: -43.063282\n"," 175200/300000: episode: 876, duration: 2.162s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 3.105595, mae: 29.173304, mean_q: -43.075058\n"," 175400/300000: episode: 877, duration: 2.232s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.383250, mae: 29.414883, mean_q: -43.420685\n"," 175600/300000: episode: 878, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 4.353431, mae: 29.376236, mean_q: -43.221348\n"," 175800/300000: episode: 879, duration: 2.353s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 4.897322, mae: 29.317184, mean_q: -43.068405\n"," 176000/300000: episode: 880, duration: 2.104s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 3.264477, mae: 29.286673, mean_q: -43.169079\n"," 176200/300000: episode: 881, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 3.221884, mae: 29.316313, mean_q: -43.205555\n"," 176400/300000: episode: 882, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.930292, mae: 29.392017, mean_q: -43.354279\n"," 176600/300000: episode: 883, duration: 2.228s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.494457, mae: 29.341808, mean_q: -43.265636\n"," 176800/300000: episode: 884, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 3.289206, mae: 29.339506, mean_q: -43.141651\n"," 177000/300000: episode: 885, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 4.334866, mae: 29.380083, mean_q: -43.137100\n"," 177200/300000: episode: 886, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.633972, mae: 29.231291, mean_q: -43.098438\n"," 177400/300000: episode: 887, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 2.802113, mae: 29.036959, mean_q: -42.729649\n"," 177600/300000: episode: 888, duration: 2.113s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 4.036544, mae: 28.899912, mean_q: -42.400772\n"," 177800/300000: episode: 889, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 2.760867, mae: 28.678123, mean_q: -42.152843\n"," 178000/300000: episode: 890, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.208575, mae: 28.708113, mean_q: -42.270996\n"," 178200/300000: episode: 891, duration: 2.206s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 3.224952, mae: 28.667778, mean_q: -42.126163\n"," 178400/300000: episode: 892, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 3.204219, mae: 28.581955, mean_q: -42.014793\n"," 178600/300000: episode: 893, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.731557, mae: 28.696890, mean_q: -42.291553\n"," 178800/300000: episode: 894, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 2.889896, mae: 28.662535, mean_q: -42.236359\n"," 179000/300000: episode: 895, duration: 2.211s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 3.462873, mae: 28.545044, mean_q: -41.994442\n"," 179200/300000: episode: 896, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 3.895723, mae: 28.483047, mean_q: -41.890678\n"," 179400/300000: episode: 897, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 2.689877, mae: 28.485996, mean_q: -41.957363\n"," 179600/300000: episode: 898, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.412486, mae: 28.554266, mean_q: -42.044342\n"," 179800/300000: episode: 899, duration: 2.238s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 3.568294, mae: 28.627075, mean_q: -42.141796\n"," 180000/300000: episode: 900, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 3.441319, mae: 28.542095, mean_q: -42.131214\n"," 180200/300000: episode: 901, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 3.077617, mae: 28.482807, mean_q: -41.963848\n"," 180400/300000: episode: 902, duration: 2.242s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.929594, mae: 28.669052, mean_q: -42.378319\n"," 180600/300000: episode: 903, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 2.831798, mae: 28.706406, mean_q: -42.317398\n"," 180800/300000: episode: 904, duration: 2.251s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 2.693235, mae: 28.737968, mean_q: -42.398418\n"," 181000/300000: episode: 905, duration: 2.246s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.201309, mae: 28.908613, mean_q: -42.626041\n"," 181200/300000: episode: 906, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.983925, mae: 28.862328, mean_q: -42.497192\n"," 181400/300000: episode: 907, duration: 2.228s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.180598, mae: 28.892935, mean_q: -42.574306\n"," 181600/300000: episode: 908, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.322557, mae: 28.825966, mean_q: -42.353775\n"," 181800/300000: episode: 909, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 3.218411, mae: 28.704792, mean_q: -42.202187\n"," 182000/300000: episode: 910, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 2.680253, mae: 28.790211, mean_q: -42.364113\n"," 182200/300000: episode: 911, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.987214, mae: 28.723373, mean_q: -42.304043\n"," 182400/300000: episode: 912, duration: 2.293s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 4.197999, mae: 28.832813, mean_q: -42.307247\n"," 182600/300000: episode: 913, duration: 2.229s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 2.588136, mae: 28.639668, mean_q: -42.136147\n"," 182800/300000: episode: 914, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.055646, mae: 28.603247, mean_q: -42.171616\n"," 183000/300000: episode: 915, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.675978, mae: 28.849072, mean_q: -42.549801\n"," 183200/300000: episode: 916, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 3.662937, mae: 28.779106, mean_q: -42.296017\n"," 183400/300000: episode: 917, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.206827, mae: 28.621994, mean_q: -42.116211\n"," 183600/300000: episode: 918, duration: 2.205s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.836673, mae: 28.846064, mean_q: -42.444477\n"," 183800/300000: episode: 919, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.933510, mae: 28.709352, mean_q: -42.364471\n"," 184000/300000: episode: 920, duration: 2.205s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.011318, mae: 28.749590, mean_q: -42.413475\n"," 184200/300000: episode: 921, duration: 2.267s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.185483, mae: 28.786386, mean_q: -42.489880\n"," 184400/300000: episode: 922, duration: 2.220s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 2.180037, mae: 28.916451, mean_q: -42.612072\n"," 184600/300000: episode: 923, duration: 2.203s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 3.495351, mae: 29.026222, mean_q: -42.650745\n"," 184800/300000: episode: 924, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.734174, mae: 28.911236, mean_q: -42.534477\n"," 185000/300000: episode: 925, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.836674, mae: 28.948584, mean_q: -42.668438\n"," 185200/300000: episode: 926, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.016363, mae: 28.795176, mean_q: -42.314743\n"," 185400/300000: episode: 927, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 2.297003, mae: 28.643562, mean_q: -42.059132\n"," 185600/300000: episode: 928, duration: 2.153s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.452678, mae: 28.665817, mean_q: -42.148270\n"," 185800/300000: episode: 929, duration: 2.110s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 2.428361, mae: 28.437389, mean_q: -41.763683\n"," 186000/300000: episode: 930, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 2.417075, mae: 28.384649, mean_q: -41.704689\n"," 186200/300000: episode: 931, duration: 2.110s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 2.495487, mae: 28.378992, mean_q: -41.681839\n"," 186400/300000: episode: 932, duration: 2.039s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.908887, mae: 28.181364, mean_q: -41.408321\n"," 186600/300000: episode: 933, duration: 2.040s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.022193, mae: 28.128540, mean_q: -41.338146\n"," 186800/300000: episode: 934, duration: 2.070s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.549063, mae: 27.855413, mean_q: -40.921268\n"," 187000/300000: episode: 935, duration: 2.106s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 2.624170, mae: 27.707878, mean_q: -40.700466\n"," 187200/300000: episode: 936, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.876022, mae: 27.642935, mean_q: -40.628307\n"," 187400/300000: episode: 937, duration: 2.144s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 1.861526, mae: 27.984596, mean_q: -41.210526\n"," 187600/300000: episode: 938, duration: 2.144s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.808613, mae: 27.588335, mean_q: -40.585686\n"," 187800/300000: episode: 939, duration: 2.116s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.614062, mae: 27.530890, mean_q: -40.433475\n"," 188000/300000: episode: 940, duration: 2.101s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.649468, mae: 27.440895, mean_q: -40.383038\n"," 188200/300000: episode: 941, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.452232, mae: 27.734224, mean_q: -40.776451\n"," 188400/300000: episode: 942, duration: 2.092s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.442019, mae: 27.679419, mean_q: -40.684235\n"," 188600/300000: episode: 943, duration: 2.075s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 2.537099, mae: 27.662369, mean_q: -40.622574\n"," 188800/300000: episode: 944, duration: 2.057s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.734315, mae: 27.494356, mean_q: -40.518517\n"," 189000/300000: episode: 945, duration: 2.076s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.587137, mae: 27.671352, mean_q: -40.749535\n"," 189200/300000: episode: 946, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 2.135407, mae: 27.476629, mean_q: -40.386398\n"," 189400/300000: episode: 947, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 2.991986, mae: 27.537188, mean_q: -40.408878\n"," 189600/300000: episode: 948, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 2.226653, mae: 27.751921, mean_q: -40.738689\n"," 189800/300000: episode: 949, duration: 2.089s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 2.143215, mae: 27.682304, mean_q: -40.651321\n"," 190000/300000: episode: 950, duration: 2.107s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.617903, mae: 27.327866, mean_q: -40.082718\n"," 190200/300000: episode: 951, duration: 2.167s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 1.414810, mae: 27.271198, mean_q: -40.090637\n"," 190400/300000: episode: 952, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 2.285853, mae: 27.257378, mean_q: -40.044086\n"," 190600/300000: episode: 953, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 1.792337, mae: 27.537382, mean_q: -40.414783\n"," 190800/300000: episode: 954, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.967870, mae: 27.060024, mean_q: -39.702930\n"," 191000/300000: episode: 955, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.524728, mae: 27.142490, mean_q: -39.916088\n"," 191200/300000: episode: 956, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 1.910836, mae: 27.019695, mean_q: -39.740402\n"," 191400/300000: episode: 957, duration: 2.187s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.026961, mae: 27.090395, mean_q: -39.757008\n"," 191600/300000: episode: 958, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.135474, mae: 26.979277, mean_q: -39.612320\n"," 191800/300000: episode: 959, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.769818, mae: 26.922241, mean_q: -39.461704\n"," 192000/300000: episode: 960, duration: 2.103s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.646693, mae: 26.790815, mean_q: -39.315357\n"," 192200/300000: episode: 961, duration: 2.106s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.858648, mae: 26.701912, mean_q: -39.198044\n"," 192400/300000: episode: 962, duration: 2.139s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.616737, mae: 26.694618, mean_q: -39.207619\n"," 192600/300000: episode: 963, duration: 2.072s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.075186, mae: 26.545717, mean_q: -38.950836\n"," 192800/300000: episode: 964, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.467838, mae: 26.658913, mean_q: -39.097309\n"," 193000/300000: episode: 965, duration: 2.204s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.167403, mae: 26.400259, mean_q: -38.775043\n"," 193200/300000: episode: 966, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.551469, mae: 26.322552, mean_q: -38.630577\n"," 193400/300000: episode: 967, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.583485, mae: 26.150541, mean_q: -38.355854\n"," 193600/300000: episode: 968, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000],  loss: 2.307528, mae: 26.227974, mean_q: -38.475647\n"," 193800/300000: episode: 969, duration: 2.145s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.629266, mae: 26.239952, mean_q: -38.509266\n"," 194000/300000: episode: 970, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.171081, mae: 26.219126, mean_q: -38.449390\n"," 194200/300000: episode: 971, duration: 2.254s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.953467, mae: 26.087421, mean_q: -38.290016\n"," 194400/300000: episode: 972, duration: 2.228s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 1.106096, mae: 26.075029, mean_q: -38.322701\n"," 194600/300000: episode: 973, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 1.544188, mae: 26.358831, mean_q: -38.699825\n"," 194800/300000: episode: 974, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.962976, mae: 26.136116, mean_q: -38.304581\n"," 195000/300000: episode: 975, duration: 2.100s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 1.253912, mae: 26.456291, mean_q: -38.861385\n"," 195200/300000: episode: 976, duration: 2.080s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 1.390883, mae: 26.303364, mean_q: -38.612114\n"," 195400/300000: episode: 977, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.610104, mae: 26.280821, mean_q: -38.540615\n"," 195600/300000: episode: 978, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 1.464735, mae: 26.298634, mean_q: -38.612064\n"," 195754/300000: episode: 979, duration: 1.674s, episode steps: 154, steps per second:  92, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 2.175221, mae: 26.383841, mean_q: -38.574749\n"," 195954/300000: episode: 980, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.599158, mae: 26.067217, mean_q: -38.247581\n"," 196154/300000: episode: 981, duration: 2.241s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.197101, mae: 26.248857, mean_q: -38.543560\n"," 196354/300000: episode: 982, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 1.273096, mae: 26.342653, mean_q: -38.674328\n"," 196554/300000: episode: 983, duration: 2.248s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 1.614538, mae: 26.172258, mean_q: -38.374439\n"," 196754/300000: episode: 984, duration: 2.278s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.817008, mae: 26.143749, mean_q: -38.299061\n"," 196954/300000: episode: 985, duration: 2.247s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 1.558695, mae: 25.995522, mean_q: -38.126488\n"," 197154/300000: episode: 986, duration: 2.367s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.454964, mae: 26.233364, mean_q: -38.480141\n"," 197354/300000: episode: 987, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.077400, mae: 26.144127, mean_q: -38.399471\n"," 197554/300000: episode: 988, duration: 2.312s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 1.284945, mae: 25.969658, mean_q: -38.125145\n"," 197754/300000: episode: 989, duration: 2.348s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 2.073038, mae: 25.969023, mean_q: -37.994442\n"," 197954/300000: episode: 990, duration: 2.280s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.111672, mae: 25.768547, mean_q: -37.825775\n"," 198154/300000: episode: 991, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 1.623090, mae: 25.729820, mean_q: -37.681259\n"," 198354/300000: episode: 992, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 2.006567, mae: 25.818838, mean_q: -37.839611\n"," 198554/300000: episode: 993, duration: 2.203s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.054237, mae: 25.980293, mean_q: -38.179688\n"," 198754/300000: episode: 994, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.538803, mae: 25.915167, mean_q: -38.046043\n"," 198954/300000: episode: 995, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.122421, mae: 25.997137, mean_q: -38.209007\n"," 199154/300000: episode: 996, duration: 2.150s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.043805, mae: 25.965576, mean_q: -38.110386\n"," 199354/300000: episode: 997, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.711290, mae: 26.062725, mean_q: -38.279701\n"," 199554/300000: episode: 998, duration: 2.094s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.337119, mae: 26.134775, mean_q: -38.387451\n"," 199754/300000: episode: 999, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.113810, mae: 26.058832, mean_q: -38.192730\n"," 199954/300000: episode: 1000, duration: 2.109s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.679047, mae: 26.084410, mean_q: -38.310791\n"," 200154/300000: episode: 1001, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.784456, mae: 26.208885, mean_q: -38.494350\n"," 200354/300000: episode: 1002, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.661447, mae: 26.265074, mean_q: -38.526588\n"," 200554/300000: episode: 1003, duration: 2.093s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.386150, mae: 26.183228, mean_q: -38.462120\n"," 200754/300000: episode: 1004, duration: 2.107s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 1.247030, mae: 26.194195, mean_q: -38.489853\n"," 200951/300000: episode: 1005, duration: 2.072s, episode steps: 197, steps per second:  95, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.091 [0.000, 2.000],  loss: 1.839589, mae: 26.239521, mean_q: -38.509098\n"," 201151/300000: episode: 1006, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.715155, mae: 26.519138, mean_q: -38.915783\n"," 201351/300000: episode: 1007, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.142080, mae: 26.414043, mean_q: -38.761925\n"," 201551/300000: episode: 1008, duration: 2.055s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 2.014655, mae: 26.254631, mean_q: -38.506485\n"," 201751/300000: episode: 1009, duration: 2.052s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.016207, mae: 26.405140, mean_q: -38.819462\n"," 201951/300000: episode: 1010, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.701903, mae: 26.233450, mean_q: -38.464264\n"," 202151/300000: episode: 1011, duration: 2.198s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.403661, mae: 26.296759, mean_q: -38.559460\n"," 202351/300000: episode: 1012, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.559606, mae: 26.233223, mean_q: -38.472588\n"," 202551/300000: episode: 1013, duration: 2.110s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.547509, mae: 26.364302, mean_q: -38.674026\n"," 202751/300000: episode: 1014, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 1.501856, mae: 26.353928, mean_q: -38.643585\n"," 202951/300000: episode: 1015, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 2.183648, mae: 26.146055, mean_q: -38.296249\n"," 203151/300000: episode: 1016, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.376088, mae: 26.190498, mean_q: -38.439728\n"," 203351/300000: episode: 1017, duration: 2.224s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 1.240426, mae: 26.302586, mean_q: -38.640953\n"," 203551/300000: episode: 1018, duration: 2.233s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 1.457732, mae: 26.275959, mean_q: -38.468361\n"," 203751/300000: episode: 1019, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.270158, mae: 26.210810, mean_q: -38.418056\n"," 203951/300000: episode: 1020, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 1.652885, mae: 26.232798, mean_q: -38.487595\n"," 204151/300000: episode: 1021, duration: 2.173s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 1.345214, mae: 26.063271, mean_q: -38.281273\n"," 204351/300000: episode: 1022, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.302801, mae: 26.319004, mean_q: -38.687618\n"," 204551/300000: episode: 1023, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.606380, mae: 26.268888, mean_q: -38.560108\n"," 204751/300000: episode: 1024, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.223379, mae: 26.250986, mean_q: -38.575230\n"," 204951/300000: episode: 1025, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.165754, mae: 26.379223, mean_q: -38.762104\n"," 205151/300000: episode: 1026, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.834830, mae: 26.330093, mean_q: -38.660206\n"," 205351/300000: episode: 1027, duration: 2.136s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.598627, mae: 26.522171, mean_q: -38.968990\n"," 205551/300000: episode: 1028, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.267201, mae: 26.324638, mean_q: -38.690868\n"," 205751/300000: episode: 1029, duration: 2.144s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.930534, mae: 26.552378, mean_q: -39.096207\n"," 205951/300000: episode: 1030, duration: 2.190s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 1.759763, mae: 26.614286, mean_q: -39.093430\n"," 206151/300000: episode: 1031, duration: 2.129s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.515815, mae: 26.417356, mean_q: -38.776596\n"," 206351/300000: episode: 1032, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.278644, mae: 26.618027, mean_q: -39.123158\n"," 206551/300000: episode: 1033, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 1.660338, mae: 26.581413, mean_q: -39.063343\n"," 206751/300000: episode: 1034, duration: 2.113s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 1.819576, mae: 26.641768, mean_q: -39.152260\n"," 206951/300000: episode: 1035, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 1.769869, mae: 26.563431, mean_q: -38.986393\n"," 207151/300000: episode: 1036, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 1.415360, mae: 26.623484, mean_q: -39.096039\n"," 207351/300000: episode: 1037, duration: 2.075s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 1.717769, mae: 26.773218, mean_q: -39.317112\n"," 207551/300000: episode: 1038, duration: 2.084s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.632231, mae: 26.461384, mean_q: -38.837513\n"," 207751/300000: episode: 1039, duration: 2.092s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.618968, mae: 26.616169, mean_q: -39.082577\n"," 207951/300000: episode: 1040, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 1.571366, mae: 26.617466, mean_q: -39.090527\n"," 208151/300000: episode: 1041, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.752785, mae: 26.560205, mean_q: -39.014835\n"," 208351/300000: episode: 1042, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.509512, mae: 26.658369, mean_q: -39.164783\n"," 208551/300000: episode: 1043, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 1.851717, mae: 26.737221, mean_q: -39.253685\n"," 208751/300000: episode: 1044, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 1.862628, mae: 26.650679, mean_q: -39.075279\n"," 208951/300000: episode: 1045, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.276795, mae: 26.608286, mean_q: -39.065414\n"," 209151/300000: episode: 1046, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.998855, mae: 26.780840, mean_q: -39.259121\n"," 209351/300000: episode: 1047, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 1.924043, mae: 26.656372, mean_q: -39.071171\n"," 209551/300000: episode: 1048, duration: 2.167s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.453036, mae: 26.631750, mean_q: -39.096378\n"," 209751/300000: episode: 1049, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.658504, mae: 26.665033, mean_q: -39.209381\n"," 209951/300000: episode: 1050, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.517617, mae: 26.700909, mean_q: -39.189430\n"," 210151/300000: episode: 1051, duration: 2.139s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.100590, mae: 26.706997, mean_q: -39.267330\n"," 210351/300000: episode: 1052, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.622412, mae: 26.761143, mean_q: -39.263260\n"," 210551/300000: episode: 1053, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.149803, mae: 26.633667, mean_q: -39.081409\n"," 210751/300000: episode: 1054, duration: 2.168s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.882090, mae: 26.903677, mean_q: -39.565739\n"," 210951/300000: episode: 1055, duration: 2.217s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.333832, mae: 26.694130, mean_q: -39.163040\n"," 211151/300000: episode: 1056, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.264009, mae: 26.722805, mean_q: -39.284618\n"," 211351/300000: episode: 1057, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.306819, mae: 26.742781, mean_q: -39.266933\n"," 211551/300000: episode: 1058, duration: 2.089s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 1.385453, mae: 26.705965, mean_q: -39.172421\n"," 211751/300000: episode: 1059, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.405783, mae: 26.779385, mean_q: -39.319458\n"," 211951/300000: episode: 1060, duration: 2.203s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.324575, mae: 26.806143, mean_q: -39.394596\n"," 212151/300000: episode: 1061, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.482135, mae: 26.742142, mean_q: -39.245377\n"," 212351/300000: episode: 1062, duration: 2.258s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.042958, mae: 26.856480, mean_q: -39.413101\n"," 212551/300000: episode: 1063, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 1.618245, mae: 26.704336, mean_q: -39.173775\n"," 212751/300000: episode: 1064, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.975961, mae: 26.567524, mean_q: -39.014332\n"," 212951/300000: episode: 1065, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.137650, mae: 26.536760, mean_q: -38.977909\n"," 213151/300000: episode: 1066, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.051849, mae: 26.474443, mean_q: -38.869251\n"," 213351/300000: episode: 1067, duration: 2.240s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.945100, mae: 26.647541, mean_q: -39.160873\n"," 213551/300000: episode: 1068, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.327794, mae: 26.640781, mean_q: -39.114952\n"," 213719/300000: episode: 1069, duration: 1.931s, episode steps: 168, steps per second:  87, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 1.037022, mae: 26.648304, mean_q: -39.088909\n"," 213919/300000: episode: 1070, duration: 2.281s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.511732, mae: 26.503994, mean_q: -38.856441\n"," 214119/300000: episode: 1071, duration: 2.162s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 1.432756, mae: 26.576895, mean_q: -38.994835\n"," 214319/300000: episode: 1072, duration: 2.178s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 1.341675, mae: 26.578421, mean_q: -38.996140\n"," 214519/300000: episode: 1073, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.412982, mae: 26.377119, mean_q: -38.683048\n"," 214719/300000: episode: 1074, duration: 2.265s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.143085, mae: 26.394478, mean_q: -38.717129\n"," 214906/300000: episode: 1075, duration: 2.068s, episode steps: 187, steps per second:  90, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.141910, mae: 26.360947, mean_q: -38.678921\n"," 215106/300000: episode: 1076, duration: 2.239s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.222267, mae: 26.418037, mean_q: -38.742672\n"," 215306/300000: episode: 1077, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 1.392046, mae: 26.291260, mean_q: -38.575085\n"," 215506/300000: episode: 1078, duration: 2.221s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.139084, mae: 26.190210, mean_q: -38.399750\n"," 215706/300000: episode: 1079, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 1.064106, mae: 26.233173, mean_q: -38.502739\n"," 215906/300000: episode: 1080, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 1.078654, mae: 26.069117, mean_q: -38.267052\n"," 216106/300000: episode: 1081, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.818020, mae: 26.403627, mean_q: -38.790405\n"," 216306/300000: episode: 1082, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.186282, mae: 26.236778, mean_q: -38.533188\n"," 216506/300000: episode: 1083, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.744979, mae: 26.405228, mean_q: -38.777451\n"," 216706/300000: episode: 1084, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 1.479791, mae: 26.368481, mean_q: -38.679527\n"," 216906/300000: episode: 1085, duration: 2.240s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.324082, mae: 26.231831, mean_q: -38.507790\n"," 217106/300000: episode: 1086, duration: 2.221s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.928969, mae: 26.538336, mean_q: -38.993324\n"," 217306/300000: episode: 1087, duration: 2.230s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.809624, mae: 26.598696, mean_q: -39.045296\n"," 217506/300000: episode: 1088, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 1.188774, mae: 26.526466, mean_q: -38.917076\n"," 217706/300000: episode: 1089, duration: 2.251s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.398130, mae: 26.638046, mean_q: -39.092342\n"," 217906/300000: episode: 1090, duration: 2.287s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.916664, mae: 26.636562, mean_q: -39.138966\n"," 218106/300000: episode: 1091, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 1.145283, mae: 26.467983, mean_q: -38.846210\n"," 218306/300000: episode: 1092, duration: 2.183s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.362287, mae: 26.769447, mean_q: -39.280266\n"," 218506/300000: episode: 1093, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 1.244196, mae: 26.762266, mean_q: -39.288559\n"," 218706/300000: episode: 1094, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 0.874758, mae: 26.853531, mean_q: -39.497856\n"," 218906/300000: episode: 1095, duration: 2.229s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 1.012261, mae: 26.882484, mean_q: -39.475876\n"," 219106/300000: episode: 1096, duration: 2.267s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 1.191882, mae: 26.771301, mean_q: -39.315304\n"," 219306/300000: episode: 1097, duration: 2.300s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 1.270271, mae: 26.865028, mean_q: -39.434425\n"," 219506/300000: episode: 1098, duration: 2.200s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.094343, mae: 26.898033, mean_q: -39.506889\n"," 219706/300000: episode: 1099, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.151975, mae: 27.040152, mean_q: -39.730858\n"," 219906/300000: episode: 1100, duration: 2.265s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.966930, mae: 27.132952, mean_q: -39.867424\n"," 220106/300000: episode: 1101, duration: 2.283s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.350160, mae: 27.031319, mean_q: -39.685429\n"," 220306/300000: episode: 1102, duration: 2.281s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.293680, mae: 26.995108, mean_q: -39.641068\n"," 220506/300000: episode: 1103, duration: 2.278s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 1.212281, mae: 26.897778, mean_q: -39.489559\n"," 220706/300000: episode: 1104, duration: 2.286s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.158373, mae: 26.817217, mean_q: -39.406738\n"," 220906/300000: episode: 1105, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.365133, mae: 27.178545, mean_q: -39.898037\n"," 221106/300000: episode: 1106, duration: 2.232s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.265261, mae: 27.256557, mean_q: -40.031967\n"," 221306/300000: episode: 1107, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.985116, mae: 27.179523, mean_q: -39.931923\n"," 221506/300000: episode: 1108, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.153519, mae: 27.240437, mean_q: -40.028385\n"," 221706/300000: episode: 1109, duration: 2.200s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.054362, mae: 27.330494, mean_q: -40.177757\n"," 221906/300000: episode: 1110, duration: 2.239s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 1.213611, mae: 27.185677, mean_q: -39.945255\n"," 222106/300000: episode: 1111, duration: 2.175s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.866086, mae: 27.314028, mean_q: -40.170837\n"," 222306/300000: episode: 1112, duration: 2.250s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.351443, mae: 27.298582, mean_q: -40.108063\n"," 222506/300000: episode: 1113, duration: 2.264s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.039685, mae: 27.342125, mean_q: -40.155655\n"," 222706/300000: episode: 1114, duration: 2.262s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.205847, mae: 27.424736, mean_q: -40.253822\n"," 222906/300000: episode: 1115, duration: 2.236s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 1.388394, mae: 27.196234, mean_q: -39.902584\n"," 223106/300000: episode: 1116, duration: 2.267s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 1.032782, mae: 27.428761, mean_q: -40.322403\n"," 223306/300000: episode: 1117, duration: 2.289s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.940537, mae: 27.289541, mean_q: -40.075958\n"," 223506/300000: episode: 1118, duration: 2.235s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 1.394642, mae: 27.308994, mean_q: -40.072956\n"," 223706/300000: episode: 1119, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.956269, mae: 27.485357, mean_q: -40.337517\n"," 223906/300000: episode: 1120, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 1.530656, mae: 27.165251, mean_q: -39.819710\n"," 224106/300000: episode: 1121, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.496860, mae: 27.237329, mean_q: -39.913525\n"," 224306/300000: episode: 1122, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 1.249590, mae: 27.146284, mean_q: -39.795116\n"," 224506/300000: episode: 1123, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.099375, mae: 27.158562, mean_q: -39.820145\n"," 224706/300000: episode: 1124, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 1.040036, mae: 27.331841, mean_q: -40.128544\n"," 224874/300000: episode: 1125, duration: 1.847s, episode steps: 168, steps per second:  91, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000],  loss: 1.181090, mae: 27.018705, mean_q: -39.588055\n"," 225074/300000: episode: 1126, duration: 2.279s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 0.926199, mae: 26.906178, mean_q: -39.498032\n"," 225274/300000: episode: 1127, duration: 2.279s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.946773, mae: 26.964050, mean_q: -39.613392\n"," 225438/300000: episode: 1128, duration: 1.867s, episode steps: 164, steps per second:  88, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.857371, mae: 26.977503, mean_q: -39.597721\n"," 225638/300000: episode: 1129, duration: 2.281s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.214335, mae: 27.035194, mean_q: -39.647060\n"," 225838/300000: episode: 1130, duration: 2.352s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 1.461835, mae: 26.853891, mean_q: -39.370449\n"," 226038/300000: episode: 1131, duration: 2.313s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 1.251529, mae: 27.062815, mean_q: -39.719311\n"," 226238/300000: episode: 1132, duration: 2.347s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.886617, mae: 26.946402, mean_q: -39.579315\n"," 226438/300000: episode: 1133, duration: 2.293s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.932742, mae: 27.046328, mean_q: -39.712833\n"," 226638/300000: episode: 1134, duration: 2.246s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.912114, mae: 26.735153, mean_q: -39.265568\n"," 226838/300000: episode: 1135, duration: 2.199s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.173719, mae: 26.864941, mean_q: -39.439426\n"," 227038/300000: episode: 1136, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.071623, mae: 27.048454, mean_q: -39.725254\n"," 227238/300000: episode: 1137, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.641550, mae: 27.060431, mean_q: -39.781715\n"," 227438/300000: episode: 1138, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.725884, mae: 26.953285, mean_q: -39.601620\n"," 227638/300000: episode: 1139, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.019430, mae: 27.048708, mean_q: -39.715282\n"," 227805/300000: episode: 1140, duration: 1.855s, episode steps: 167, steps per second:  90, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.366401, mae: 27.115629, mean_q: -39.781574\n"," 228005/300000: episode: 1141, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.218499, mae: 27.129566, mean_q: -39.747272\n"," 228168/300000: episode: 1142, duration: 1.813s, episode steps: 163, steps per second:  90, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.067 [0.000, 2.000],  loss: 1.506447, mae: 27.002264, mean_q: -39.578632\n"," 228353/300000: episode: 1143, duration: 2.120s, episode steps: 185, steps per second:  87, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000],  loss: 1.383141, mae: 27.124725, mean_q: -39.708645\n"," 228553/300000: episode: 1144, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.891470, mae: 26.940571, mean_q: -39.570587\n"," 228753/300000: episode: 1145, duration: 2.247s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.127001, mae: 26.793800, mean_q: -39.267372\n"," 228911/300000: episode: 1146, duration: 1.728s, episode steps: 158, steps per second:  91, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000],  loss: 0.719342, mae: 26.730732, mean_q: -39.265648\n"," 229111/300000: episode: 1147, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.266710, mae: 27.146397, mean_q: -39.824326\n"," 229311/300000: episode: 1148, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.248709, mae: 26.938528, mean_q: -39.464062\n"," 229511/300000: episode: 1149, duration: 2.239s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.646326, mae: 26.967310, mean_q: -39.632999\n"," 229681/300000: episode: 1150, duration: 1.892s, episode steps: 170, steps per second:  90, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.112 [0.000, 2.000],  loss: 1.204925, mae: 26.870293, mean_q: -39.376877\n"," 229881/300000: episode: 1151, duration: 2.238s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.092990, mae: 26.661152, mean_q: -39.088116\n"," 230081/300000: episode: 1152, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 1.550963, mae: 26.557564, mean_q: -38.943935\n"," 230281/300000: episode: 1153, duration: 2.167s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.914284, mae: 26.802160, mean_q: -39.385529\n"," 230481/300000: episode: 1154, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.928607, mae: 26.783867, mean_q: -39.332348\n"," 230681/300000: episode: 1155, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.997068, mae: 26.539345, mean_q: -38.943478\n"," 230881/300000: episode: 1156, duration: 2.107s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 1.067632, mae: 26.466770, mean_q: -38.835724\n"," 231081/300000: episode: 1157, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.725233, mae: 26.722445, mean_q: -39.249676\n"," 231281/300000: episode: 1158, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.905242, mae: 26.719439, mean_q: -39.235085\n"," 231480/300000: episode: 1159, duration: 2.195s, episode steps: 199, steps per second:  91, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.877849, mae: 26.777094, mean_q: -39.293125\n"," 231680/300000: episode: 1160, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.814151, mae: 26.674126, mean_q: -39.164799\n"," 231880/300000: episode: 1161, duration: 2.227s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.064477, mae: 26.696568, mean_q: -39.147457\n"," 232080/300000: episode: 1162, duration: 2.087s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 1.134844, mae: 26.533424, mean_q: -38.888504\n"," 232280/300000: episode: 1163, duration: 2.073s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.966415, mae: 26.598398, mean_q: -39.015640\n"," 232480/300000: episode: 1164, duration: 2.078s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.654723, mae: 26.488585, mean_q: -38.893436\n"," 232680/300000: episode: 1165, duration: 2.139s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.812012, mae: 26.452644, mean_q: -38.797039\n"," 232880/300000: episode: 1166, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.068858, mae: 26.561089, mean_q: -38.938793\n"," 233080/300000: episode: 1167, duration: 2.231s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.881516, mae: 26.642855, mean_q: -39.082054\n"," 233280/300000: episode: 1168, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.816659, mae: 26.446814, mean_q: -38.792660\n"," 233480/300000: episode: 1169, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.854730, mae: 26.571993, mean_q: -39.008671\n"," 233680/300000: episode: 1170, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.830224, mae: 26.693727, mean_q: -39.175919\n"," 233880/300000: episode: 1171, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 1.113170, mae: 26.677784, mean_q: -39.126900\n"," 234080/300000: episode: 1172, duration: 2.222s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.096945, mae: 26.461924, mean_q: -38.777500\n"," 234280/300000: episode: 1173, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.139713, mae: 26.362226, mean_q: -38.640190\n"," 234480/300000: episode: 1174, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.773274, mae: 26.596241, mean_q: -39.046131\n"," 234680/300000: episode: 1175, duration: 2.101s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.984443, mae: 26.532631, mean_q: -38.907539\n"," 234880/300000: episode: 1176, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.802192, mae: 26.356060, mean_q: -38.640583\n"," 235080/300000: episode: 1177, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.920523, mae: 26.585443, mean_q: -39.027863\n"," 235280/300000: episode: 1178, duration: 2.102s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.807009, mae: 26.446114, mean_q: -38.830643\n"," 235480/300000: episode: 1179, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.856534, mae: 26.381498, mean_q: -38.706558\n"," 235680/300000: episode: 1180, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.814322, mae: 26.622547, mean_q: -39.087719\n"," 235880/300000: episode: 1181, duration: 2.071s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.890864, mae: 26.466692, mean_q: -38.830002\n"," 236080/300000: episode: 1182, duration: 2.068s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.620705, mae: 26.492903, mean_q: -38.912788\n"," 236280/300000: episode: 1183, duration: 2.081s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.988640, mae: 26.810053, mean_q: -39.374683\n"," 236480/300000: episode: 1184, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.912413, mae: 26.802994, mean_q: -39.319191\n"," 236680/300000: episode: 1185, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.755592, mae: 26.562010, mean_q: -38.986855\n"," 236880/300000: episode: 1186, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.809256, mae: 26.833740, mean_q: -39.379433\n"," 237080/300000: episode: 1187, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.879879, mae: 26.613560, mean_q: -39.060570\n"," 237279/300000: episode: 1188, duration: 2.070s, episode steps: 199, steps per second:  96, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.676392, mae: 26.595238, mean_q: -39.058933\n"," 237479/300000: episode: 1189, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.986215, mae: 26.672417, mean_q: -39.123272\n"," 237668/300000: episode: 1190, duration: 2.050s, episode steps: 189, steps per second:  92, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.037 [0.000, 2.000],  loss: 0.888891, mae: 26.592667, mean_q: -39.012650\n"," 237868/300000: episode: 1191, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.965926, mae: 26.579853, mean_q: -38.984547\n"," 238063/300000: episode: 1192, duration: 2.050s, episode steps: 195, steps per second:  95, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.118 [0.000, 2.000],  loss: 0.871974, mae: 26.915045, mean_q: -39.495060\n"," 238263/300000: episode: 1193, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.756914, mae: 26.415833, mean_q: -38.722076\n"," 238463/300000: episode: 1194, duration: 2.123s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.765173, mae: 26.733217, mean_q: -39.252647\n"," 238663/300000: episode: 1195, duration: 2.105s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.911612, mae: 26.654936, mean_q: -39.036308\n"," 238863/300000: episode: 1196, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 1.008801, mae: 26.564495, mean_q: -38.974800\n"," 239063/300000: episode: 1197, duration: 2.144s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.953734, mae: 26.738394, mean_q: -39.240532\n"," 239263/300000: episode: 1198, duration: 2.189s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.418630, mae: 26.660915, mean_q: -39.187557\n"," 239463/300000: episode: 1199, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.637465, mae: 26.672947, mean_q: -39.182632\n"," 239663/300000: episode: 1200, duration: 2.187s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.876254, mae: 26.703236, mean_q: -39.182030\n"," 239863/300000: episode: 1201, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.865472, mae: 26.934141, mean_q: -39.510399\n"," 240063/300000: episode: 1202, duration: 2.101s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.855485, mae: 26.751509, mean_q: -39.292416\n"," 240263/300000: episode: 1203, duration: 2.066s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.764685, mae: 26.640642, mean_q: -39.109512\n"," 240432/300000: episode: 1204, duration: 1.778s, episode steps: 169, steps per second:  95, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000],  loss: 0.752393, mae: 26.748041, mean_q: -39.250374\n"," 240632/300000: episode: 1205, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.569459, mae: 26.789936, mean_q: -39.349873\n"," 240822/300000: episode: 1206, duration: 2.002s, episode steps: 190, steps per second:  95, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.053 [0.000, 2.000],  loss: 1.118772, mae: 26.885962, mean_q: -39.453892\n"," 241022/300000: episode: 1207, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.878929, mae: 27.126915, mean_q: -39.781216\n"," 241222/300000: episode: 1208, duration: 2.216s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.960399, mae: 26.912031, mean_q: -39.471859\n"," 241422/300000: episode: 1209, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.775924, mae: 27.030630, mean_q: -39.680874\n"," 241622/300000: episode: 1210, duration: 2.220s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.912880, mae: 26.851259, mean_q: -39.387806\n"," 241822/300000: episode: 1211, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 1.074830, mae: 26.638262, mean_q: -39.025421\n"," 242022/300000: episode: 1212, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.725423, mae: 26.701332, mean_q: -39.148979\n"," 242222/300000: episode: 1213, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.803910, mae: 26.598116, mean_q: -38.998562\n"," 242422/300000: episode: 1214, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.886608, mae: 26.493530, mean_q: -38.823631\n"," 242585/300000: episode: 1215, duration: 1.757s, episode steps: 163, steps per second:  93, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.551238, mae: 26.640865, mean_q: -39.111992\n"," 242785/300000: episode: 1216, duration: 2.217s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.873793, mae: 26.696487, mean_q: -39.183659\n"," 242983/300000: episode: 1217, duration: 2.260s, episode steps: 198, steps per second:  88, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.131 [0.000, 2.000],  loss: 0.424282, mae: 26.771755, mean_q: -39.321548\n"," 243183/300000: episode: 1218, duration: 2.260s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.642494, mae: 26.756554, mean_q: -39.279163\n"," 243383/300000: episode: 1219, duration: 2.235s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.737145, mae: 26.839365, mean_q: -39.350414\n"," 243583/300000: episode: 1220, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.735815, mae: 26.848818, mean_q: -39.367313\n"," 243767/300000: episode: 1221, duration: 2.059s, episode steps: 184, steps per second:  89, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 1.064360, mae: 26.521673, mean_q: -38.878174\n"," 243967/300000: episode: 1222, duration: 2.083s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.643858, mae: 26.855925, mean_q: -39.413521\n"," 244167/300000: episode: 1223, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.893362, mae: 26.657671, mean_q: -39.109241\n"," 244367/300000: episode: 1224, duration: 2.092s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.706582, mae: 26.702330, mean_q: -39.191261\n"," 244567/300000: episode: 1225, duration: 2.187s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.696290, mae: 26.800299, mean_q: -39.333927\n"," 244767/300000: episode: 1226, duration: 2.150s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 1.017135, mae: 26.493891, mean_q: -38.865051\n"," 244967/300000: episode: 1227, duration: 2.139s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.640728, mae: 26.677458, mean_q: -39.140724\n"," 245167/300000: episode: 1228, duration: 2.326s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.668632, mae: 26.805664, mean_q: -39.374168\n"," 245367/300000: episode: 1229, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.850704, mae: 26.674726, mean_q: -39.102081\n"," 245567/300000: episode: 1230, duration: 2.288s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 1.108894, mae: 26.875708, mean_q: -39.389584\n"," 245767/300000: episode: 1231, duration: 2.240s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 0.786886, mae: 26.378696, mean_q: -38.685841\n"," 245967/300000: episode: 1232, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.537638, mae: 26.411694, mean_q: -38.775116\n"," 246167/300000: episode: 1233, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.792418, mae: 26.471689, mean_q: -38.827496\n"," 246367/300000: episode: 1234, duration: 2.145s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.706484, mae: 26.421150, mean_q: -38.748554\n"," 246567/300000: episode: 1235, duration: 2.186s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.687674, mae: 26.555962, mean_q: -38.968544\n"," 246767/300000: episode: 1236, duration: 2.158s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.536026, mae: 26.680244, mean_q: -39.142227\n"," 246967/300000: episode: 1237, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 0.537687, mae: 26.495033, mean_q: -38.871124\n"," 247167/300000: episode: 1238, duration: 2.168s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.552480, mae: 26.505772, mean_q: -38.899818\n"," 247367/300000: episode: 1239, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.665949, mae: 26.643042, mean_q: -39.106243\n"," 247567/300000: episode: 1240, duration: 2.111s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.655160, mae: 26.446630, mean_q: -38.759560\n"," 247767/300000: episode: 1241, duration: 2.139s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.557297, mae: 26.517664, mean_q: -38.922508\n"," 247965/300000: episode: 1242, duration: 2.165s, episode steps: 198, steps per second:  91, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.141 [0.000, 2.000],  loss: 0.958211, mae: 26.586706, mean_q: -38.973896\n"," 248165/300000: episode: 1243, duration: 2.170s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.773044, mae: 26.487715, mean_q: -38.815582\n"," 248365/300000: episode: 1244, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 1.005486, mae: 26.715027, mean_q: -39.145985\n"," 248565/300000: episode: 1245, duration: 2.230s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.620519, mae: 26.420111, mean_q: -38.737164\n"," 248765/300000: episode: 1246, duration: 2.176s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.586315, mae: 26.572300, mean_q: -38.957790\n"," 248965/300000: episode: 1247, duration: 2.232s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.785196, mae: 26.428787, mean_q: -38.709366\n"," 249165/300000: episode: 1248, duration: 2.215s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.704275, mae: 26.542130, mean_q: -38.890270\n"," 249365/300000: episode: 1249, duration: 2.136s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 0.583619, mae: 26.317188, mean_q: -38.607487\n"," 249540/300000: episode: 1250, duration: 1.872s, episode steps: 175, steps per second:  93, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.983 [0.000, 2.000],  loss: 0.758019, mae: 26.287544, mean_q: -38.565620\n"," 249740/300000: episode: 1251, duration: 2.150s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 1.017850, mae: 26.554207, mean_q: -38.922729\n"," 249899/300000: episode: 1252, duration: 1.710s, episode steps: 159, steps per second:  93, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.452687, mae: 26.396172, mean_q: -38.742500\n"," 250099/300000: episode: 1253, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.493024, mae: 26.182367, mean_q: -38.392563\n"," 250299/300000: episode: 1254, duration: 2.260s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.465901, mae: 26.127998, mean_q: -38.299088\n"," 250499/300000: episode: 1255, duration: 2.186s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.810778, mae: 26.183643, mean_q: -38.328720\n"," 250699/300000: episode: 1256, duration: 2.189s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.679469, mae: 26.375349, mean_q: -38.627735\n"," 250899/300000: episode: 1257, duration: 2.208s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.637066, mae: 26.121284, mean_q: -38.276875\n"," 251099/300000: episode: 1258, duration: 2.125s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.830365, mae: 26.078011, mean_q: -38.199562\n"," 251293/300000: episode: 1259, duration: 2.019s, episode steps: 194, steps per second:  96, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.611324, mae: 26.027697, mean_q: -38.139526\n"," 251493/300000: episode: 1260, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.618250, mae: 26.033291, mean_q: -38.163414\n"," 251693/300000: episode: 1261, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.838006, mae: 26.142969, mean_q: -38.309990\n"," 251858/300000: episode: 1262, duration: 1.791s, episode steps: 165, steps per second:  92, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.127 [0.000, 2.000],  loss: 0.550723, mae: 26.127914, mean_q: -38.340378\n"," 252021/300000: episode: 1263, duration: 1.776s, episode steps: 163, steps per second:  92, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.037 [0.000, 2.000],  loss: 0.516824, mae: 26.062578, mean_q: -38.240059\n"," 252221/300000: episode: 1264, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.552388, mae: 26.172913, mean_q: -38.418594\n"," 252421/300000: episode: 1265, duration: 2.060s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 0.370682, mae: 26.203526, mean_q: -38.455605\n"," 252621/300000: episode: 1266, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.391514, mae: 26.196997, mean_q: -38.447968\n"," 252821/300000: episode: 1267, duration: 2.130s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.650007, mae: 26.157066, mean_q: -38.398438\n"," 253021/300000: episode: 1268, duration: 2.160s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.511751, mae: 26.300819, mean_q: -38.597023\n"," 253221/300000: episode: 1269, duration: 2.136s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.399334, mae: 26.445068, mean_q: -38.815895\n"," 253421/300000: episode: 1270, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.344689, mae: 26.417925, mean_q: -38.777477\n"," 253614/300000: episode: 1271, duration: 2.105s, episode steps: 193, steps per second:  92, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.047 [0.000, 2.000],  loss: 0.863329, mae: 26.391050, mean_q: -38.645691\n"," 253814/300000: episode: 1272, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.436170, mae: 26.374781, mean_q: -38.699451\n"," 254014/300000: episode: 1273, duration: 2.278s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.577911, mae: 26.302876, mean_q: -38.592896\n"," 254214/300000: episode: 1274, duration: 2.277s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.748370, mae: 26.447369, mean_q: -38.765404\n"," 254414/300000: episode: 1275, duration: 2.285s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.528197, mae: 26.448509, mean_q: -38.839340\n"," 254614/300000: episode: 1276, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.679149, mae: 26.302240, mean_q: -38.566162\n"," 254814/300000: episode: 1277, duration: 2.226s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.665025, mae: 26.450615, mean_q: -38.807777\n"," 255014/300000: episode: 1278, duration: 2.324s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.900962, mae: 26.383120, mean_q: -38.679790\n"," 255210/300000: episode: 1279, duration: 2.226s, episode steps: 196, steps per second:  88, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000],  loss: 0.809004, mae: 26.380461, mean_q: -38.682011\n"," 255410/300000: episode: 1280, duration: 2.292s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.384151, mae: 26.309874, mean_q: -38.642982\n"," 255610/300000: episode: 1281, duration: 2.258s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.584884, mae: 26.362246, mean_q: -38.652782\n"," 255810/300000: episode: 1282, duration: 2.243s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.720095, mae: 26.408087, mean_q: -38.749706\n"," 255966/300000: episode: 1283, duration: 1.709s, episode steps: 156, steps per second:  91, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.635771, mae: 26.391930, mean_q: -38.719902\n"," 256166/300000: episode: 1284, duration: 2.237s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.933459, mae: 26.164179, mean_q: -38.343998\n"," 256366/300000: episode: 1285, duration: 2.269s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.470864, mae: 26.473351, mean_q: -38.910500\n"," 256566/300000: episode: 1286, duration: 2.236s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.635334, mae: 26.491777, mean_q: -38.892166\n"," 256727/300000: episode: 1287, duration: 1.765s, episode steps: 161, steps per second:  91, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000],  loss: 0.665887, mae: 26.341608, mean_q: -38.616928\n"," 256914/300000: episode: 1288, duration: 2.061s, episode steps: 187, steps per second:  91, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000],  loss: 0.359678, mae: 26.341139, mean_q: -38.675571\n"," 257114/300000: episode: 1289, duration: 2.229s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.867425, mae: 26.476181, mean_q: -38.815567\n"," 257314/300000: episode: 1290, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.665813, mae: 26.207474, mean_q: -38.454399\n"," 257514/300000: episode: 1291, duration: 2.220s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.643804, mae: 26.365425, mean_q: -38.687138\n"," 257714/300000: episode: 1292, duration: 2.223s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.743079, mae: 26.668964, mean_q: -39.146072\n"," 257914/300000: episode: 1293, duration: 2.194s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.838489, mae: 26.415215, mean_q: -38.745998\n"," 258111/300000: episode: 1294, duration: 2.170s, episode steps: 197, steps per second:  91, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000],  loss: 0.864534, mae: 26.432037, mean_q: -38.779881\n"," 258311/300000: episode: 1295, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 0.436706, mae: 26.453272, mean_q: -38.841339\n"," 258511/300000: episode: 1296, duration: 2.115s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.472616, mae: 26.604958, mean_q: -39.067501\n"," 258711/300000: episode: 1297, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.609947, mae: 26.497631, mean_q: -38.907494\n"," 258911/300000: episode: 1298, duration: 2.085s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.713032, mae: 26.672592, mean_q: -39.128174\n"," 259111/300000: episode: 1299, duration: 2.023s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.625612, mae: 26.506636, mean_q: -38.892242\n"," 259311/300000: episode: 1300, duration: 2.101s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.911323, mae: 26.763477, mean_q: -39.266930\n"," 259511/300000: episode: 1301, duration: 2.053s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.819851, mae: 26.334393, mean_q: -38.582592\n"," 259711/300000: episode: 1302, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.843990, mae: 26.535173, mean_q: -38.919937\n"," 259911/300000: episode: 1303, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.678554, mae: 26.603525, mean_q: -39.027534\n"," 260111/300000: episode: 1304, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.639131, mae: 26.401091, mean_q: -38.747837\n"," 260311/300000: episode: 1305, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.661645, mae: 26.294624, mean_q: -38.566463\n"," 260510/300000: episode: 1306, duration: 2.198s, episode steps: 199, steps per second:  91, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000],  loss: 1.012384, mae: 26.574751, mean_q: -38.892387\n"," 260710/300000: episode: 1307, duration: 2.249s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.763637, mae: 26.303032, mean_q: -38.552494\n"," 260910/300000: episode: 1308, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.858737, mae: 26.351585, mean_q: -38.606083\n"," 261110/300000: episode: 1309, duration: 2.128s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.730651, mae: 26.296572, mean_q: -38.592564\n"," 261310/300000: episode: 1310, duration: 2.147s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.856562, mae: 26.334368, mean_q: -38.589890\n"," 261510/300000: episode: 1311, duration: 2.113s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.581772, mae: 25.989389, mean_q: -38.122772\n"," 261710/300000: episode: 1312, duration: 2.156s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.580804, mae: 26.431248, mean_q: -38.792713\n"," 261910/300000: episode: 1313, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.543669, mae: 26.563427, mean_q: -39.001457\n"," 262110/300000: episode: 1314, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.468492, mae: 26.397757, mean_q: -38.751469\n"," 262310/300000: episode: 1315, duration: 2.223s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.377551, mae: 26.204348, mean_q: -38.493423\n"," 262510/300000: episode: 1316, duration: 2.227s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.679382, mae: 26.374941, mean_q: -38.722034\n"," 262710/300000: episode: 1317, duration: 2.280s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.974353, mae: 26.334192, mean_q: -38.570309\n"," 262876/300000: episode: 1318, duration: 1.834s, episode steps: 166, steps per second:  91, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.808298, mae: 26.410063, mean_q: -38.722328\n"," 263076/300000: episode: 1319, duration: 2.208s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.714720, mae: 26.293486, mean_q: -38.570118\n"," 263245/300000: episode: 1320, duration: 1.888s, episode steps: 169, steps per second:  90, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 0.917938, mae: 26.506304, mean_q: -38.859711\n"," 263445/300000: episode: 1321, duration: 2.157s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.679418, mae: 26.283823, mean_q: -38.533924\n"," 263645/300000: episode: 1322, duration: 2.294s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.491412, mae: 26.281065, mean_q: -38.558300\n"," 263845/300000: episode: 1323, duration: 2.224s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.488454, mae: 26.518877, mean_q: -38.928154\n"," 264011/300000: episode: 1324, duration: 1.892s, episode steps: 166, steps per second:  88, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000],  loss: 0.720328, mae: 26.133419, mean_q: -38.310619\n"," 264211/300000: episode: 1325, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.577338, mae: 26.226738, mean_q: -38.497929\n"," 264411/300000: episode: 1326, duration: 2.150s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.624347, mae: 26.397114, mean_q: -38.735558\n"," 264611/300000: episode: 1327, duration: 2.157s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.705572, mae: 26.440794, mean_q: -38.786488\n"," 264811/300000: episode: 1328, duration: 2.157s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.862697, mae: 26.341179, mean_q: -38.613705\n"," 265011/300000: episode: 1329, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.664648, mae: 26.475447, mean_q: -38.844112\n"," 265211/300000: episode: 1330, duration: 2.223s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.789462, mae: 26.452110, mean_q: -38.801998\n"," 265411/300000: episode: 1331, duration: 2.214s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.680346, mae: 26.656588, mean_q: -39.127487\n"," 265611/300000: episode: 1332, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.925684, mae: 26.649433, mean_q: -39.096756\n"," 265769/300000: episode: 1333, duration: 1.783s, episode steps: 158, steps per second:  89, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.044 [0.000, 2.000],  loss: 0.499424, mae: 26.481899, mean_q: -38.914284\n"," 265969/300000: episode: 1334, duration: 2.199s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 0.566245, mae: 26.509327, mean_q: -38.902744\n"," 266169/300000: episode: 1335, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.636626, mae: 26.609926, mean_q: -39.074772\n"," 266369/300000: episode: 1336, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.869544, mae: 26.623911, mean_q: -39.049458\n"," 266569/300000: episode: 1337, duration: 2.206s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.565027, mae: 26.599024, mean_q: -39.049496\n"," 266769/300000: episode: 1338, duration: 2.211s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 0.730823, mae: 26.571306, mean_q: -38.977161\n"," 266969/300000: episode: 1339, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.710408, mae: 26.530092, mean_q: -38.900600\n"," 267169/300000: episode: 1340, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.482854, mae: 26.672871, mean_q: -39.167500\n"," 267369/300000: episode: 1341, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.766820, mae: 26.621607, mean_q: -39.067017\n"," 267569/300000: episode: 1342, duration: 2.212s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.684657, mae: 26.655161, mean_q: -39.105167\n"," 267769/300000: episode: 1343, duration: 2.202s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.870762, mae: 26.806164, mean_q: -39.332516\n"," 267969/300000: episode: 1344, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.622225, mae: 26.521084, mean_q: -38.936214\n"," 268169/300000: episode: 1345, duration: 2.238s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.481886, mae: 26.551498, mean_q: -39.030865\n"," 268369/300000: episode: 1346, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.640433, mae: 26.920183, mean_q: -39.581711\n"," 268568/300000: episode: 1347, duration: 2.182s, episode steps: 199, steps per second:  91, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.166 [0.000, 2.000],  loss: 0.721007, mae: 26.709063, mean_q: -39.243870\n"," 268768/300000: episode: 1348, duration: 2.103s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.694383, mae: 26.732986, mean_q: -39.283520\n"," 268968/300000: episode: 1349, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.943847, mae: 26.731760, mean_q: -39.199295\n"," 269168/300000: episode: 1350, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.424111, mae: 26.824791, mean_q: -39.424805\n"," 269368/300000: episode: 1351, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.826216, mae: 26.961729, mean_q: -39.534847\n"," 269568/300000: episode: 1352, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.586620, mae: 26.818052, mean_q: -39.386524\n"," 269768/300000: episode: 1353, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 1.159722, mae: 26.821383, mean_q: -39.328186\n"," 269968/300000: episode: 1354, duration: 2.106s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.369267, mae: 26.843601, mean_q: -39.473549\n"," 270168/300000: episode: 1355, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 0.596909, mae: 26.676207, mean_q: -39.153439\n"," 270368/300000: episode: 1356, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.579161, mae: 26.690912, mean_q: -39.173290\n"," 270565/300000: episode: 1357, duration: 2.200s, episode steps: 197, steps per second:  90, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 0.721007, mae: 26.789583, mean_q: -39.330143\n"," 270765/300000: episode: 1358, duration: 2.187s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.545997, mae: 26.754213, mean_q: -39.302685\n"," 270965/300000: episode: 1359, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.662969, mae: 26.740181, mean_q: -39.242065\n"," 271165/300000: episode: 1360, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.611629, mae: 26.666113, mean_q: -39.158134\n"," 271365/300000: episode: 1361, duration: 2.162s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.607185, mae: 26.829750, mean_q: -39.387875\n"," 271565/300000: episode: 1362, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.957185, mae: 26.726772, mean_q: -39.205814\n"," 271765/300000: episode: 1363, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.195277, mae: 26.953249, mean_q: -39.530888\n"," 271957/300000: episode: 1364, duration: 2.031s, episode steps: 192, steps per second:  95, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.047 [0.000, 2.000],  loss: 0.559167, mae: 26.992411, mean_q: -39.652248\n"," 272157/300000: episode: 1365, duration: 2.141s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.908297, mae: 26.860327, mean_q: -39.391270\n"," 272357/300000: episode: 1366, duration: 2.242s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 0.583660, mae: 26.784548, mean_q: -39.333656\n"," 272557/300000: episode: 1367, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 0.873300, mae: 26.832996, mean_q: -39.391739\n"," 272757/300000: episode: 1368, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.617637, mae: 27.108482, mean_q: -39.846443\n"," 272957/300000: episode: 1369, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.953880, mae: 26.792849, mean_q: -39.354809\n"," 273157/300000: episode: 1370, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.712049, mae: 26.802532, mean_q: -39.369801\n"," 273357/300000: episode: 1371, duration: 2.203s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.879594, mae: 26.921110, mean_q: -39.507286\n"," 273557/300000: episode: 1372, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.477006, mae: 26.876596, mean_q: -39.532272\n"," 273749/300000: episode: 1373, duration: 2.064s, episode steps: 192, steps per second:  93, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.031 [0.000, 2.000],  loss: 0.745072, mae: 27.076920, mean_q: -39.780212\n"," 273949/300000: episode: 1374, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 0.747049, mae: 26.976229, mean_q: -39.643227\n"," 274143/300000: episode: 1375, duration: 2.068s, episode steps: 194, steps per second:  94, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.735954, mae: 26.932108, mean_q: -39.551384\n"," 274343/300000: episode: 1376, duration: 2.034s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.659003, mae: 26.994438, mean_q: -39.656322\n"," 274543/300000: episode: 1377, duration: 2.059s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.696683, mae: 27.103222, mean_q: -39.790043\n"," 274743/300000: episode: 1378, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 0.427917, mae: 26.905718, mean_q: -39.533588\n"," 274943/300000: episode: 1379, duration: 2.195s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.573438, mae: 27.298393, mean_q: -40.151646\n"," 275143/300000: episode: 1380, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.554924, mae: 27.302725, mean_q: -40.121384\n"," 275343/300000: episode: 1381, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.562295, mae: 26.951397, mean_q: -39.563900\n"," 275543/300000: episode: 1382, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.703302, mae: 27.052668, mean_q: -39.709518\n"," 275743/300000: episode: 1383, duration: 2.128s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.784174, mae: 27.059805, mean_q: -39.735409\n"," 275943/300000: episode: 1384, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.592782, mae: 27.383877, mean_q: -40.261410\n"," 276143/300000: episode: 1385, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.901447, mae: 27.355938, mean_q: -40.120605\n"," 276343/300000: episode: 1386, duration: 2.130s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.516218, mae: 27.122265, mean_q: -39.824234\n"," 276543/300000: episode: 1387, duration: 2.199s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.920431, mae: 27.341202, mean_q: -40.149952\n"," 276743/300000: episode: 1388, duration: 2.279s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.947942, mae: 27.223061, mean_q: -39.959995\n"," 276943/300000: episode: 1389, duration: 2.251s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 0.825973, mae: 27.197294, mean_q: -39.937485\n"," 277143/300000: episode: 1390, duration: 2.219s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.591783, mae: 27.145332, mean_q: -39.878387\n"," 277340/300000: episode: 1391, duration: 2.131s, episode steps: 197, steps per second:  92, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000],  loss: 1.034276, mae: 27.290951, mean_q: -40.073288\n"," 277540/300000: episode: 1392, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.885772, mae: 27.201641, mean_q: -39.982662\n"," 277740/300000: episode: 1393, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 0.695593, mae: 27.341152, mean_q: -40.171963\n"," 277940/300000: episode: 1394, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 0.913078, mae: 27.401688, mean_q: -40.224106\n"," 278140/300000: episode: 1395, duration: 2.153s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 1.119612, mae: 27.440166, mean_q: -40.298134\n"," 278340/300000: episode: 1396, duration: 2.201s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.590010, mae: 27.381414, mean_q: -40.273270\n"," 278540/300000: episode: 1397, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.828435, mae: 27.361641, mean_q: -40.177921\n"," 278740/300000: episode: 1398, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 0.498356, mae: 27.571167, mean_q: -40.527237\n"," 278908/300000: episode: 1399, duration: 1.784s, episode steps: 168, steps per second:  94, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.875875, mae: 27.303257, mean_q: -40.078228\n"," 279108/300000: episode: 1400, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.710965, mae: 27.320162, mean_q: -40.127235\n"," 279308/300000: episode: 1401, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 1.025210, mae: 27.252583, mean_q: -40.005741\n"," 279476/300000: episode: 1402, duration: 1.790s, episode steps: 168, steps per second:  94, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.676907, mae: 27.507565, mean_q: -40.436443\n"," 279676/300000: episode: 1403, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.829438, mae: 27.690289, mean_q: -40.653404\n"," 279876/300000: episode: 1404, duration: 2.050s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.631143, mae: 27.350340, mean_q: -40.169525\n"," 280076/300000: episode: 1405, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.854936, mae: 27.394375, mean_q: -40.230721\n"," 280276/300000: episode: 1406, duration: 2.109s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.492346, mae: 27.292448, mean_q: -40.079090\n"," 280476/300000: episode: 1407, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.849490, mae: 27.367199, mean_q: -40.147533\n"," 280676/300000: episode: 1408, duration: 2.124s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.660607, mae: 27.302130, mean_q: -40.087860\n"," 280876/300000: episode: 1409, duration: 2.179s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.645107, mae: 27.440161, mean_q: -40.275360\n"," 281076/300000: episode: 1410, duration: 2.163s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.563865, mae: 27.352278, mean_q: -40.160641\n"," 281272/300000: episode: 1411, duration: 2.185s, episode steps: 196, steps per second:  90, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.082 [0.000, 2.000],  loss: 0.938306, mae: 27.408083, mean_q: -40.161663\n"," 281472/300000: episode: 1412, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.556789, mae: 27.106951, mean_q: -39.803543\n"," 281672/300000: episode: 1413, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.448201, mae: 27.326693, mean_q: -40.150997\n"," 281872/300000: episode: 1414, duration: 2.182s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 1.006961, mae: 27.209385, mean_q: -39.911976\n"," 282072/300000: episode: 1415, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.617952, mae: 27.067377, mean_q: -39.737518\n"," 282272/300000: episode: 1416, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.725197, mae: 27.426758, mean_q: -40.258598\n"," 282472/300000: episode: 1417, duration: 2.166s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.696451, mae: 27.298067, mean_q: -40.043228\n"," 282672/300000: episode: 1418, duration: 2.159s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.658391, mae: 26.989492, mean_q: -39.642452\n"," 282872/300000: episode: 1419, duration: 2.239s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 0.631637, mae: 27.202011, mean_q: -39.898621\n"," 283072/300000: episode: 1420, duration: 2.116s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.628269, mae: 26.971951, mean_q: -39.587585\n"," 283272/300000: episode: 1421, duration: 2.086s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.560593, mae: 26.897964, mean_q: -39.460716\n"," 283472/300000: episode: 1422, duration: 2.025s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.972008, mae: 26.927917, mean_q: -39.466221\n"," 283672/300000: episode: 1423, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.535156, mae: 27.234098, mean_q: -40.007450\n"," 283872/300000: episode: 1424, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.667015, mae: 27.060287, mean_q: -39.715832\n"," 284072/300000: episode: 1425, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.857728, mae: 26.948353, mean_q: -39.487152\n"," 284272/300000: episode: 1426, duration: 2.191s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.692818, mae: 26.703901, mean_q: -39.173717\n"," 284439/300000: episode: 1427, duration: 1.791s, episode steps: 167, steps per second:  93, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.802876, mae: 26.713726, mean_q: -39.192978\n"," 284639/300000: episode: 1428, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.671307, mae: 26.713003, mean_q: -39.178535\n"," 284839/300000: episode: 1429, duration: 2.289s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.587428, mae: 26.774591, mean_q: -39.294510\n"," 285003/300000: episode: 1430, duration: 1.836s, episode steps: 164, steps per second:  89, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.171 [0.000, 2.000],  loss: 0.655011, mae: 26.614449, mean_q: -39.028843\n"," 285165/300000: episode: 1431, duration: 1.778s, episode steps: 162, steps per second:  91, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.093 [0.000, 2.000],  loss: 0.649722, mae: 26.756718, mean_q: -39.250244\n"," 285365/300000: episode: 1432, duration: 2.232s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.455089, mae: 26.766741, mean_q: -39.293312\n"," 285565/300000: episode: 1433, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.535268, mae: 26.613659, mean_q: -39.064693\n"," 285733/300000: episode: 1434, duration: 1.815s, episode steps: 168, steps per second:  93, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000],  loss: 0.460292, mae: 26.375229, mean_q: -38.683147\n"," 285933/300000: episode: 1435, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 0.643103, mae: 26.579239, mean_q: -38.997414\n"," 286133/300000: episode: 1436, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 0.926802, mae: 26.667768, mean_q: -39.077103\n"," 286333/300000: episode: 1437, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.833750, mae: 26.795351, mean_q: -39.254917\n"," 286501/300000: episode: 1438, duration: 1.859s, episode steps: 168, steps per second:  90, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000],  loss: 0.379089, mae: 26.561218, mean_q: -38.977081\n"," 286701/300000: episode: 1439, duration: 2.185s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.432841, mae: 26.396624, mean_q: -38.760914\n"," 286901/300000: episode: 1440, duration: 2.105s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 0.509414, mae: 26.508825, mean_q: -38.913349\n"," 287101/300000: episode: 1441, duration: 2.115s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.826897, mae: 26.720329, mean_q: -39.185455\n"," 287266/300000: episode: 1442, duration: 1.719s, episode steps: 165, steps per second:  96, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 1.022556, mae: 26.596899, mean_q: -38.983524\n"," 287428/300000: episode: 1443, duration: 1.719s, episode steps: 162, steps per second:  94, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000],  loss: 0.803878, mae: 26.596836, mean_q: -38.986393\n"," 287628/300000: episode: 1444, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.963739, mae: 26.560171, mean_q: -38.927860\n"," 287828/300000: episode: 1445, duration: 2.105s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 0.522113, mae: 26.451199, mean_q: -38.801991\n"," 288028/300000: episode: 1446, duration: 2.130s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.782602, mae: 26.665762, mean_q: -39.093616\n"," 288228/300000: episode: 1447, duration: 2.174s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.889667, mae: 26.540163, mean_q: -38.925812\n"," 288428/300000: episode: 1448, duration: 2.300s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 0.774467, mae: 26.713568, mean_q: -39.161266\n"," 288628/300000: episode: 1449, duration: 2.169s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.539203, mae: 26.665861, mean_q: -39.124027\n"," 288823/300000: episode: 1450, duration: 2.147s, episode steps: 195, steps per second:  91, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000],  loss: 0.781300, mae: 26.516081, mean_q: -38.876156\n"," 289023/300000: episode: 1451, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 0.619050, mae: 26.751947, mean_q: -39.263607\n"," 289223/300000: episode: 1452, duration: 2.223s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.558123, mae: 26.476191, mean_q: -38.833176\n"," 289423/300000: episode: 1453, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.606050, mae: 26.680620, mean_q: -39.152222\n"," 289623/300000: episode: 1454, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 0.585124, mae: 26.647123, mean_q: -39.095009\n"," 289823/300000: episode: 1455, duration: 2.188s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.372115, mae: 26.691944, mean_q: -39.196602\n"," 290023/300000: episode: 1456, duration: 2.207s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.866167, mae: 26.616552, mean_q: -38.991383\n"," 290223/300000: episode: 1457, duration: 2.314s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 0.755774, mae: 26.490440, mean_q: -38.828869\n"," 290422/300000: episode: 1458, duration: 2.251s, episode steps: 199, steps per second:  88, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.116 [0.000, 2.000],  loss: 0.761100, mae: 26.960667, mean_q: -39.535030\n"," 290622/300000: episode: 1459, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.543159, mae: 26.533239, mean_q: -38.891670\n"," 290822/300000: episode: 1460, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 0.808572, mae: 26.837053, mean_q: -39.347237\n"," 291022/300000: episode: 1461, duration: 2.143s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.715547, mae: 26.820763, mean_q: -39.318459\n"," 291222/300000: episode: 1462, duration: 2.100s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.736129, mae: 26.429897, mean_q: -38.747543\n"," 291422/300000: episode: 1463, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.635343, mae: 26.461206, mean_q: -38.783821\n"," 291622/300000: episode: 1464, duration: 2.152s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.553150, mae: 26.709778, mean_q: -39.172314\n"," 291822/300000: episode: 1465, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 0.664940, mae: 26.504263, mean_q: -38.873913\n"," 291985/300000: episode: 1466, duration: 1.731s, episode steps: 163, steps per second:  94, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.969 [0.000, 2.000],  loss: 0.914249, mae: 26.632057, mean_q: -39.038761\n"," 292185/300000: episode: 1467, duration: 2.116s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.818450, mae: 26.499083, mean_q: -38.865864\n"," 292385/300000: episode: 1468, duration: 2.135s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 0.802771, mae: 26.650599, mean_q: -39.071373\n"," 292585/300000: episode: 1469, duration: 2.127s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 0.645046, mae: 26.528896, mean_q: -38.916016\n"," 292785/300000: episode: 1470, duration: 2.024s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 0.724056, mae: 26.592974, mean_q: -39.002808\n"," 292985/300000: episode: 1471, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.517424, mae: 26.178112, mean_q: -38.405243\n"," 293185/300000: episode: 1472, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 0.645685, mae: 26.338251, mean_q: -38.652012\n"," 293385/300000: episode: 1473, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.524514, mae: 26.463730, mean_q: -38.834663\n"," 293585/300000: episode: 1474, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.515362, mae: 26.585543, mean_q: -39.018402\n"," 293749/300000: episode: 1475, duration: 1.752s, episode steps: 164, steps per second:  94, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.037 [0.000, 2.000],  loss: 0.644397, mae: 26.733456, mean_q: -39.210915\n"," 293949/300000: episode: 1476, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.668257, mae: 26.811338, mean_q: -39.336273\n"," 294149/300000: episode: 1477, duration: 2.171s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.641850, mae: 26.624746, mean_q: -39.046349\n"," 294348/300000: episode: 1478, duration: 2.127s, episode steps: 199, steps per second:  94, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 0.636649, mae: 26.692230, mean_q: -39.146343\n"," 294542/300000: episode: 1479, duration: 2.075s, episode steps: 194, steps per second:  93, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 0.447585, mae: 26.574421, mean_q: -39.009850\n"," 294742/300000: episode: 1480, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.755821, mae: 26.449020, mean_q: -38.768154\n"," 294942/300000: episode: 1481, duration: 2.055s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 0.355809, mae: 26.536135, mean_q: -38.948841\n"," 295142/300000: episode: 1482, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 0.978671, mae: 26.669422, mean_q: -39.106972\n"," 295342/300000: episode: 1483, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.470365, mae: 26.373379, mean_q: -38.698219\n"," 295542/300000: episode: 1484, duration: 2.116s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.501808, mae: 26.738636, mean_q: -39.240376\n"," 295742/300000: episode: 1485, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 0.672838, mae: 26.692123, mean_q: -39.150776\n"," 295942/300000: episode: 1486, duration: 2.114s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 1.167594, mae: 26.703527, mean_q: -39.090843\n"," 296142/300000: episode: 1487, duration: 2.128s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 0.622250, mae: 26.468447, mean_q: -38.848076\n"," 296342/300000: episode: 1488, duration: 2.137s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 0.579458, mae: 26.562603, mean_q: -38.976429\n"," 296542/300000: episode: 1489, duration: 2.076s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.592998, mae: 26.641899, mean_q: -39.102955\n"," 296700/300000: episode: 1490, duration: 1.654s, episode steps: 158, steps per second:  96, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.076 [0.000, 2.000],  loss: 0.639652, mae: 26.695436, mean_q: -39.163136\n"," 296900/300000: episode: 1491, duration: 2.162s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.622528, mae: 26.726404, mean_q: -39.219601\n"," 297100/300000: episode: 1492, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 0.628738, mae: 27.004601, mean_q: -39.641808\n"," 297300/300000: episode: 1493, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 0.562802, mae: 26.613848, mean_q: -39.048443\n"," 297500/300000: episode: 1494, duration: 2.198s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 0.884545, mae: 26.616510, mean_q: -39.013317\n"," 297700/300000: episode: 1495, duration: 2.120s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.794041, mae: 26.820068, mean_q: -39.370510\n"," 297900/300000: episode: 1496, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.152792, mae: 26.821632, mean_q: -39.307808\n"," 298100/300000: episode: 1497, duration: 2.150s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 0.804061, mae: 26.907576, mean_q: -39.517822\n"," 298300/300000: episode: 1498, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 0.483515, mae: 26.688374, mean_q: -39.217804\n"," 298500/300000: episode: 1499, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 0.520753, mae: 26.697304, mean_q: -39.172756\n"," 298700/300000: episode: 1500, duration: 2.140s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 0.717580, mae: 26.952026, mean_q: -39.552177\n"," 298898/300000: episode: 1501, duration: 2.116s, episode steps: 198, steps per second:  94, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.051 [0.000, 2.000],  loss: 1.102111, mae: 26.658136, mean_q: -39.076073\n"," 299098/300000: episode: 1502, duration: 2.117s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 0.908605, mae: 27.005259, mean_q: -39.602257\n"," 299291/300000: episode: 1503, duration: 2.078s, episode steps: 193, steps per second:  93, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 0.734306, mae: 26.760736, mean_q: -39.309700\n"," 299491/300000: episode: 1504, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.726906, mae: 26.840185, mean_q: -39.376453\n"," 299691/300000: episode: 1505, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.628129, mae: 26.656990, mean_q: -39.114342\n"," 299885/300000: episode: 1506, duration: 2.078s, episode steps: 194, steps per second:  93, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000],  loss: 0.477445, mae: 26.794575, mean_q: -39.343819\n","done, took 3252.513 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":826},"id":"toA3Am5T83iY","executionInfo":{"status":"ok","timestamp":1620588688756,"user_tz":-60,"elapsed":1002,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"189a4fb2-3207-4293-bdd1-8841eb38b593"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","episodes = range(1, len(history_single.history[\"episode_reward\"]) + 1)\n","double_episodes = range(1, len(history_double.history[\"episode_reward\"]) + 1)\n","print(len(double_episodes))\n","plt.title(\"DeepQ Learning Reward\")\n","plt.plot(episodes, history_single.history[\"episode_reward\"], \"r\", label=\"DeepQ Learning\")\n","plt.legend()\n","plt.show()\n","\n","plt.title(\"Double Deep Q Learning Reward\")\n","plt.plot(double_episodes, history_double.history[\"episode_reward\"], \"b\", label=\"Double DeepQ Learning\")\n","plt.legend()\n","plt.show()\n","\n","plt.title(\"DeepQ Learning Reward vs Double Deep Q Learning Reward\")\n","plt.plot(episodes, history_single.history[\"episode_reward\"], \"r\", label=\"DeepQ Learning\")\n","plt.plot(double_episodes, history_double.history[\"episode_reward\"], \"b\", label=\"Double DeepQ Learning\")\n","plt.legend()\n","plt.show()\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["1506\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAYAAAC3Y/QeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wU9Znv8c/jDIICiiIuhouDl6BAcNQxwupRExXQKEaNl2gMyhE1kcRoVtfL2eS4mz0ma9x4UKJgFGNI1KyKetQEQi7qJiqCInIJAXHAISojKneUgef8UTXQDN0z05fqqpr+vl+vfk111a+qnq6efupXT1VXm7sjIiKVZbe4AxARkfJT8hcRqUBK/iIiFUjJX0SkAin5i4hUICV/EZEKpOQvEhEzu9jMZsQdR1zM7FIz+++445DslPylTWZWb2abzGydmX1sZn8xs6vMrCz/P2bWw8zuMbP3zGyjmb1pZmPamMfN7JByxJeLu//S3UdEseyM92R9uF0eNLNuUaxLOiYlf2mvM929O3Ag8EPgn4H7o16pme0OzAzXOxzYG7ge+A8z+3bU628lLivXzq8VZ7p7N6AWOBK4Ka5AzKw6rnVLYeL+55WUcfc17v40cAEwxsyGAJhZZzP7sZmtMLP3zexeM9ujeT4zO8PM5mYcOQzNmFZvZjeZ2UIz+8jMpphZl3DyJUB/4Dx3f9vdt7j7b4FvAz/It7fbWpxmto+ZPWNmjWEcz5hZ34x5/2Rm/25mfwY2AgeFRxhXmdmS8LVNNDML2+9U9mijbZWZ3WFmH5jZ22Y2PmzfZlJ19/eA6QQ7geZ1DQu388dm9oaZnRSO/4KZvZnR7ndm9mrG8xfN7Mvh8I1m9lZ4xLfQzM7OaHepmf3ZzH5iZquB/21mPc3saTNba2azgIPzeW+kvJT8pSDuPgtoAP5HOOqHwGcJEtAhQB/gewBmdiTwAHAl0BOYBDxtZp0zFnkxMJIgYXwW+F/h+FOB37j7hhYhPA7sSXA0kI+ccRJ8HqYQHGX0BzYBd7eY/xLgCqA7sDwcdwZwDDAUOD98HbnkajsOOC2M6yjgy+19QeEO6jRgafi8D/As8ANgX+CfgMfNrBfwMnCome1nZp3COD5jZt3DnWAd8GK46LcI3t+9gVuBqWZ2QMaqjwWWAf8A/DswEdgMHACMDR+SVO6uhx6tPoB64JQs418GbgEM2AAcnDFtOPB2OHwP8G8t5l0MnJix/Ksypp0OvBUOzwR+mCOu94CLckxz4JAW41qNM8syaoGPMp7/CfjXLOs5PuP5r4Ebw+FLgf9uZ9s/AFdmTDslbF/dynuyHlgXtvs90COc9s/AL1q0nw6MCYdfBM4BhgEzwjhGAV8A5rXyfzAXOCvjta3ImFYFbAEOyxj3fzJfvx7JeqhOJ8XoA3wI9CLohc8JqxgQJNqqcPhAghLRtzLm3R34TMbzdzKGl2dM+4CgJ7mTsByyXzi9vVqN08z2BH5CkAj3Cad3N7Mqd9+aJc5m72UMbwRaK0XlavuZFsvOtp6WvuzuM83sROBXBNvjY4LtfZ6ZnZnRthPwx3D4eeAkgiO354GPgBOBT8LnAJjZ14HrgJpwVLdwHdli7AVUs+v7KAmlso8UxMyOIUj+/02QgDcBg929R/jY24OTkRAkhH/PmNbD3fd094czFtkvY7g/8PdweCZwmpl1bRHCucCnwCt5hN1WnN8FBgLHuvtewAnNLzdjGVHdBvddoG/G8365Grbk7s8DDwI/Dke9Q9Dzz9zeXd39h+H05uR/Qjj8PEHyPzEcxswOBO4DxgM93b0HMJ/c26IRaGLX91ESSslf8mJme5nZGcAjwFR3f9PdtxEkip+Y2f5huz5m1lzPvg+4ysyODa+S6WpmXzKz7hmLvtrM+prZvgSlpEfD8b8g6KH+l5nVmFmncLkTgNvdfU0r4e5uZl2aHwSJq7U4uxPsHD4O4/h+MdsqT78Grgnj6UFQusnHncCpZnYEMBU408xGhieSu5jZSRknr/9CsJP7PDDL3RcQHC0cC7wQtulKkNwbAczsMmBIrpWHR0ZPEJz43dPMBgGtXo4r8VLyl/b6f2a2jqBXeQvwn8BlGdP/meCE48tmtpagxz4QwN1nE5zQvJugxLCUoGac6VcE9edlBCcafxDO+wlB/fsdgl7+JuC3BMnu1jZiXhC2b35c1lqc4TL3IDhCeDlcT7ncR/D65wGvA88R9KS3tjZTM3dvBB4Cvufu7wBnATcTJO93CC6P3S1suwF4DVjg7p+Gi3gJWO7uq8I2C4E7wvHvA58D/txGGOMJSkPvERyJTGlP7BIPc9ePuUi8zKweuNzdZ7ajbSfgN8BK4FLvoP/AZnYacK+7Hxh3LNIxqecvqeLuWwjq/W+xo8eeema2h5mdbmbV4aWa3wemxR2XdFzq+Uvs8un5d1ThlUbPA4cRlKieBa5x97WxBiYdlpK/iEgFUtlHRKQCpeZLXvvtt5/X1NTEHYaISGrMmTPnA3fvlW1aapJ/TU0Ns2fPjjsMEZHUMLOc37JW2UdEpAIp+YuIVCAlfxGRCpSamn82W7ZsoaGhgc2bN8cdihSpS5cu9O3bl06dOsUdikhFSHXyb2hooHv37tTU1JBxi15JGXdn9erVNDQ0MGDAgLjDEakIqS77bN68mZ49eyrxp5yZ0bNnTx3BiZRRqpM/oMTfQeh9FCmv1Cd/EZHE2LYNpkyBLVvijqRNSv5Fqqqqora2lsGDB3PEEUdwxx13sG3btpKv59NPP+U73/kOhxxyCIcccghnnHEGK1asyNq2pqaGDz7I59cNi3P66afz8ccfl219Ion185/D2LFwxx1xR9KmVJ/wTYI99tiDuXPnArBq1Souuugi1q5dy623tvU7I/m5+eabWbduHYsXL6aqqoopU6Zw1llnMWfOHHbbLdp9eFNTE9XVuf9VnnvuuUjXL5IaH34Y/G1sjDeOdlDPv4T2339/Jk+ezN133427s3XrVq6//nqOOeYYhg4dyqRJk7a3vf3227eP//73g18LrK+v57DDDuPiiy/m8MMP5ytf+QobN25k48aNTJkyhZ/85CdUVQW/iX7ZZZfRrVs3Zs5s312QGxsbOffccznmmGM45phj+POfgx9lmjVrFsOHD+fII4/kH//xH1m8eDEADz74IKNHj+aLX/wiJ598Mg8++CDnnHMOo0aN4tBDD+WGG27YvuzmI436+noOP/xwxo0bx+DBgxkxYgSbNm0C4NVXX2Xo0KHU1tZy/fXXM2RIzl8EFJEy6Dg9/+98B8IeeMnU1sKdd+Y1y0EHHcTWrVtZtWoVTz31FHvvvTevvvoqn3zyCccddxwjRoxgyZIlLFmyhFmzZuHujB49mhdeeIH+/fuzePFi7r//fo477jjGjh3LT3/6U0aMGEH//v3Za6+9dlpXXV0dCxcuZMSIEW3Gdc0113Dttddy/PHHs2LFCkaOHMmiRYs47LDDePHFF6murmbmzJncfPPNPP744wC89tprzJs3j3333ZcHH3yQuXPn8vrrr9O5c2cGDhzIt771Lfr12/l3xpcsWcLDDz/Mfffdx/nnn8/jjz/O1772NS677DLuu+8+hg8fzo033pjXNhVJjRTdIr/jJP8EmjFjBvPmzeOxxx4DYM2aNSxZsoQZM2YwY8YMjjzySADWr1/PkiVL6N+/P/369eO4444D4Gtf+xoTJkxoV3Jvy8yZM1m4cOH252vXrmX9+vWsWbOGMWPGsGTJEsyMLRknqk499VT23Xff7c9PPvlk9t57bwAGDRrE8uXLd0n+AwYMoLa2FoCjjz6a+vp6Pv74Y9atW8fw4cMBuOiii3jmmWeKfk0iUriOk/zz7KFHZdmyZVRVVbH//vvj7tx1112MHDlypzbTp0/npptu4sorr9xpfH19/S6XPJoZBx98MCtWrGDdunV07959+7Q5c+Zw7rnntiuubdu28fLLL9OlS5edxo8fP54vfOELTJs2jfr6ek466aTt07p27bpT286dO28frqqqoqmpaZf1tGzTXPYRqQgpumRZNf8Samxs5KqrrmL8+PGYGSNHjuSee+7Z3pv+29/+xoYNGxg5ciQPPPAA69evB2DlypWsWrUKgBUrVvDSSy8B8Ktf/Yrjjz+erl27MmbMGK677jq2bt0KwEMPPUSXLl22HyW0ZcSIEdx1113bnzefpF6zZg19+vQBgjp/FHr06EH37t155ZVXAHjkkUciWY9I7FT2qRybNm2itraWLVu2UF1dzSWXXMJ1110HwOWXX059fT1HHXUU7k6vXr148sknGTFiBIsWLdpeBunWrRtTp06lqqqKgQMHMnHiRMaOHcugQYP4xje+AcBtt93G9ddfz8CBA9m0aRO9evXipZdeyvnlqKFDh26/Cuj8889nwoQJXH311QwdOpSmpiZOOOEE7r33Xm644QbGjBnDD37wA770pS9Ftp3uv/9+xo0bx2677caJJ564vXwkIjFx91Q8jj76aG9p4cKFu4xLs7ffftsHDx7cZrt3333Xa2trfdKkSWWIqjTWrVu3ffi2227zb3/727u06Wjvp1SgH//YHdyvuy7uSNzdHZjtOXKqev4p1Lt3b15//fW4w8jLs88+y2233UZTUxMHHnhgZCUmkVip7COFqKmpYf78+XGHEYkLLriACy64IO4wRMojBSd+U3/C11O0p5Xc9D5Kh5KC/+dUJ/8uXbqwevVqJY6U8/B+/i0vQxWR6KS67NO3b18aGhpoTMF9NKR1zb/kJdIhpKDsk+rk36lTJ/3yk4hIAVJd9hERkcIo+YuIVCAlfxGROFx1FYweHdvqU13zFxFJlHyuPMz4fY84FNXzN7PzzGyBmW0zs7oW04aa2Uvh9DfNrEs4/ujw+VIzm2D65W4RkbIrtuwzHzgHeCFzpJlVA1OBq9x9MHAS0Hyj+HuAccCh4WNUkTGIiCRDivqyRSV/d1/k7ouzTBoBzHP3N8J2q919q5kdAOzl7i+HNx16CPhyMTGIiCTG738fdwTtFtUJ388CbmbTzew1M2v+wdc+QENGu4ZwXFZmdoWZzTaz2foil4gk3m9/G/xNwRFAmyd8zWwm0DvLpFvc/alWlns8cAywEfi9mc0B1uQTnLtPBiYD1NXV6R4OIpIOKbjlTJvJ391PKWC5DcAL7v4BgJk9BxxFcB4g8zv8fYGVBSxfRESKEFXZZzrwOTPbMzz5eyKw0N3fBdaa2bDwKp+vA7mOHkRE0ikFZZ9iL/U828wagOHAs2Y2HcDdPwL+E3gVmAu85u7PhrN9E/gZsBR4C/hNMTGIiCRORyj7tMbdpwHTckybSlDmaTl+NjCkmPWKiEhxdHsHEZFS6+hlHxERSSclfxGJxpYtMHYsLF8edyRtu/NOeOKJtts1NcHll8OyZdHHFDHd2E1EovGHP8CUKbByJUyfHnc0rbv22uBvWydqX3oJ7r8fFi+GF1+MPq4IqecvIlKBlPxFRCqQkr+IJN+CBbB6ddxRdChK/iKSfEOGwBFHxB1Fh6LkLyLpsFK3ASslJX8RkVLTl7xERCpQCu7to+QvIun0uc/B+PHlW1/v3nDrrfnN88wzwVHARx9FE1MRlPxFJJ3mz4eJE8u3vvffb//PNDaXfX70o+Dv/PnRxFQEJX8RkQqk5C8iUoGU/EWkY5gyBVasKM+6sp3Q3bKlPOsuESV/EUm/TZuCO4ieeGJ8MUyaFN+6C6DkLyLpt21b8HfVqvKsL9t1/OvWlWfdJaLkLyLRSsE175VIyV9EJF8ffdS+nVpmm02bgkdCKPmLSLRScKuDvC1YAD/8YX7z7LUXdOsWTTwFUPIXESnEk0+23SZzx9fUtOPcRAIo+YuIRCXB5zuU/EVEKpCSv4ikxw03wBNPxB1F21JwnqM67gBERNrt9tuDvwkupwC7xpfAnYF6/iIiUUvgzkrJX0Sk1BLY029JyV9EJGoJ3Bko+YuIlFoCyzwtKfmLiFQgJX8RkUK01rtPYJmnJSV/EZGoJLj8o+QvIukXR5JNQe++NUr+IhKtBPd+i9Ke15XgHURRyd/MzjOzBWa2zczqMsZfbGZzMx7bzKw2nHa0mb1pZkvNbIJZgreOiEgxErzjK7bnPx84B3ghc6S7/9Lda929FrgEeNvd54aT7wHGAYeGj1FFxiAiSVaO/p3KPnkrKvm7+yJ3X9xGs68CjwCY2QHAXu7+srs78BDw5WJiEJEOoKkJ/u3fYMOGuCNpv+Ydzp13QkNDvLEUoBw1/wuAh8PhPkDmVmoIx2VlZleY2Wwzm93Y2BhhiCISq4cegu99D269Ne5I8rN8OVx7LYwe3Xq7BB4ltHlXTzObCfTOMukWd3+qjXmPBTa6+/xCgnP3ycBkgLq6uuQWz0SkOM2/bVtozz+usk9TUzC8Zk3rbRNY+28z+bv7KUUs/0J29PoBVgJ9M573DceJiKRLAhN6PiIr+5jZbsD5hPV+AHd/F1hrZsPCq3y+DrR69CAiUlLr17fdUy+1BJZ9ir3U82wzawCGA8+a2fSMyScA77j7shazfRP4GbAUeAv4TTExiIjk1Qvv3Rt69Ch+nfkk9AQeJRT1S17uPg2YlmPan4BhWcbPBoYUs14RkYKV44qiBPb0W9I3fEVESk0/4ygiUgYJLKsknZK/iKTf/feXf52zZsEhh2SflsCefktK/iKSfv/0T3FHkDpK/iIiFUjJX0SkAin5i0i02nMyVidsA2vXwsyZZVmVkr+ISFJccAGceiq8/37kq1LyF5FotefKlxRcHVMWCxcGfzdvjnxVSv4iIlFJcDlLyV9EpAIp+YtI/BLcQy5KgstZSv4iIlFJ8E5NyV9E4pfgHnJHpeQvIlJqKdiZKfmLiJSabuksItIOCa6Nl0QCX5+Sv4hIW/JN3gns6bek5C8i8UtBsixKAl+fkr+ISNRU9hERSaFik/ebb5YmjhJS8heR+CWwZ1xSb78ddwS7UPIXkWh19MSeUkr+IhK/BJ4Q3UkH3IEp+YtItNqT2Dtgck06JX8RSTbtGCKh5C8i0hZ9yUtERNqUgqMVJX8Rid4TT8D69bmnp6Cn3NEo+YtItObNg3PPhSuvzN2mtZ5yEnrR5Sr7lPG1KvmLSLTWrQv+1tfHGkZZNSfx5r8JPLJR8hcRSYoy7iSU/EUk2dJc9mn+m4TX0IKSv4jkZgaXXFKe9XREKvuISGpNnRr9OhLYM+7oikr+ZnaemS0ws21mVpcxvpOZ/dzM3jSzRWZ2U8a0UWa22MyWmtmNxaxfRFKg2MSehB1DEmIosWJ7/vOBc4AXWow/D+js7p8DjgauNLMaM6sCJgKnAYOAr5rZoCJjEBGRPFUXM7O7LwKwXetZDnQ1s2pgD+BTYC3weWCpuy8L53sEOAtYWEwcIpJgCax3l91jj8UdwS6iqvk/BmwA3gVWAD929w+BPsA7Ge0awnFZmdkVZjbbzGY3NjZGFKqIREplH3jrrdLEUUJt9vzNbCbQO8ukW9z9qRyzfR7YCnwG2Ad4MVxOXtx9MjAZoK6uLgH/ASIiHUObyd/dTylguRcBv3X3LcAqM/szUEfQ6++X0a4vsLKA5YtIWqjsk0hRlX1WAF8EMLOuwDDgr8CrwKFmNsDMdgcuBJ6OKAYRSYvWdhCFlFzeew8++KDweIqNIQU7vGIv9TzbzBqA4cCzZjY9nDQR6GZmCwgS/hR3n+fuTcB4YDqwCPi1uy8oJgYR6QBKXdc/4ADo1au0y8xHEs5TtKHYq32mAdOyjF9PcLlntnmeA54rZr0iIlIcfcNXRJItCb3ocpd9yvCalfxFpDySkMSTTvfzF5GKkoITpGXV0AA9e8LC6L7/quQvIuVR6BU9lXTE0LyNnngCPvwQ7rknslUp+YuItKXQHVC558uDkr+ISAVS8heRZEtz2SfB5zKU/EUkWu1J3qVKklHtKNqz3GxtVPYREWlFmnv3KaXkLyLRKmfpI84yS7Z1FxqPev4iknqVfD//fOfTl7xERCqYev4iknrNpY8k9ODLpdByTxnLVkr+IpJsSdhpJLh8UyglfxGRpCnDzqOo+/mLiKTG7bfDsGFxR9E+Sv4i0mHEfdnjDTcUPq9+xlFERNpU7KWhutpHRESioOQvIsmWhCtnkhBDiSn5i4iUWrHlG5V9RERSqNDkrS95iYiEklByKfRLXgm+6kfJX0SilYTkHReVfUREKkgK7mSq5C8i2ZUqARV7Y7ckHDno3j4iInlKQSIsOX3JS0RSqxKTdgVR8heRaBV7xUs+O6GNG+Gdd4pbX7ExFNK+mS71FJHYlbrnX8rE1lpsJ5xQuvUUSid8RSS1UpDAsmpoiGe9KaPkLyLJloRzD7raR0QqRnsTmDts2BBtLNnWmWQbN8K2bYXPr7KPiCTebbdBt27Q2Ljz+KQn6ChNmgRXX53/fLrUU0Ri194E9PDDwd/33osulrgVkox/9rPSx1FCRSV/MzvPzBaY2TYzq8sYv7uZTTGzN83sDTM7KWPa0eH4pWY2wSzBdz4SkfhlS7wd/agiBT3/+cA5wAstxo8DcPfPAacCd5hZ87ruCacfGj5GFRmDiESh1Ff7dPSEnTJFJX93X+Tui7NMGgT8IWyzCvgYqDOzA4C93P1ld3fgIeDLxcQgIhK5cu24OsCXvN4ARptZtZkNAI4G+gF9gMyLcBvCcVmZ2RVmNtvMZje2PJkkItFKSk89zWWfBN/MrrqtBmY2E+idZdIt7v5UjtkeAA4HZgPLgb8AW/MNzt0nA5MB6urqUvJui3QQpUpA+SwnracAUxh3m8nf3U/Jd6Hu3gRc2/zczP4C/A34COib0bQvsDLf5YtIChSy84iqxxt1TzrX8hO8U4ik7GNme5pZ13D4VKDJ3Re6+7vAWjMbFl7l83Ug19GDiMQpKaWVUpR94rpVRZrLPq0xs7OBu4BewLNmNtfdRwL7A9PNbBtBz/6SjNm+CTwI7AH8JnyISEeVgt+zTYwy7nCLSv7uPg2YlmV8PTAwxzyzgSHFrFdEyiCtN3bLJq5YErzD0zd8RSQ5siXLNJR9ciX5BJd9lPxFJLs4rvbpaCd8E0zJX0SiEVdCbGrKr33cidsMHn10xzCo5y8iMSp1z7+UJZCklX2K9a//Gu3ys1DyF5Hs4u4Rl1LaXot6/iKSWoX0+KO6Oqa1GEqRaBN8VU8uSv4ikl2hSXHqVFiypHTrTEPZJ21HFij5i0ipXXIJDB2643kKE2Ns9EteIhK7YhLQ5s2lWU6pRF32KTUlfxFJrVL1YtNQ9kkhJX+RfGzYsHOvtiNLSsJbvbr4ZZTjtSRle7WTkr9IPrp1g4FZb1sl7q330lu7IiZX4ly6FIZEfCuwJCVtfclLJMFWrIg7gvLINwFt27bzPKVIYEuXZh/f0cs+Sv4iEptCEmyhSau5xxvHdf6lYFbadZThewNK/iJSGu5B7z/b+HyWUei8+Sy31OtIyq0w8qDkLyLZFdLzz0z+USayfJa5bBmceWb51hfnfHlQ8heR0sjV88+cHofvfhdefDGedSfpZHILSv4ikl05a/6liiHOdZTi+wjFzpcHJX8RKY1cZZ98JPUGaYX+ktfWrfmtp4yvX8lfpJJt3AjPP599WrE1/0K094RvVD3jQi/jzRVPsdsjQkr+IpVs3Dg46SSory9+WVGUfcrtwAMLn7eYsk9UVzm1QslfpJLNmxf8Xbdu12mluton6doTZ6GvJd+yT7Hry4OSv4hkF0fZp70xpGXH0t7t0bLWry95iUhqlKLsk9QTvm3JFXeht6FQz19EYtPeBJR5M7LMnu6//Ev2dqVYZymVouyTa8enso+IdFiZvdXM5L9oUemW3d7xSaOrfUQkdQq5q2eCk10sCt0e6vmLSGq0VfNPam+9VFf7ZGuTb/LXl7xEpKyKvRtnc/tie/4tk1+xZZ9t2+DJJ4uLqT1yJe0EHwkp+YtI9hOT5Uz+ua5yKfZo4Xe/K27+9ir1N3xV9hGRsij0qpRMUXzDt9ie8yeftN0myi955Rt/GUtjSv4iEn/PP9cveeVaXtLOH+T6Ja9Sl8FKSMlfpJI1J5dS9fxLXeNuz/I+/LC06yzURx/t/LypqfRlsBJS8heR0vX8S52smprabtOzZ3HrKFXMEybsOm716sKWpZq/SIIkrdRQSnH3/HP1dDdubL19e5dbrPZ8wzcKSS37mNntZvZXM5tnZtPMrEfGtJvMbKmZLTazkRnjR4XjlprZjcWsX6SsKi35J+HGbuvXFzd/Wt+zFJR9fgcMcfehwN+AmwDMbBBwITAYGAX81MyqzKwKmAicBgwCvhq2FUm+BF+zXbS4e/65TvgWm/xL8brKqYxf8qouZmZ3n5Hx9GXgK+HwWcAj7v4J8LaZLQU+H05b6u7LAMzskbDtwmLiaFVdHWzaFNnipYJk9sIGD44vjlJaGH70xo2D7t13nvbppzuGW3u9zcu48kro1Cl3u5deyr2cDz4I/k6cCH/8447xq1Zlbz98eO71ZLrmmtzTBg0Kkm3LHVa2GDO3RTbLlrUvnlzLX7gwGN/8S2LN30+4+2544w144YX2L7+dikr+LYwFHg2H+xDsDJo1hOMA3mkx/thcCzSzK4ArAPr3719YVIcd1r5rfUXaY/Xq4ATjoA5ywFpTA889B8fm+Bj+/e8wZAi09vlruYwVK2DYMKiuhj/9Cc4+G6ZNgzPPhM6dcy/nscfg3HN37v0OGgQzZgQnfpvr/8OHQ58+wY5h40bo2jWIb+7cHfN17gz9+kFtbbBcgD32CDqCNTVBMs9MwqtXQ2NjsL5c721DA2zevOP5GWfAM88Ew6NGwezZwU7s4IPhrbd2bWMWbIvdwoJLU1NwZPP3v8NppwWvo18/mD4dTj99x/bYZ5/c26wIbSZ/M5sJ9M4y6RZ3fypscwvQBPyylMG5+2RgMkBdXV1hxa+pU0sZkohIh9Bm8nf3U1qbbmaXAmcAJ7tvPy5eCfTLaNY3HEcr40VEpEyKvdpnFHADMNrdM6/Jehq40Mw6m9kA4FBgFvAqcKiZDTCz3QlOCj9dTAwiIpK/Ymv+dwOdgd9ZUKd72d2vcvcFZvZrghO5TcDV7r4VwMzGA9OBKuABd19QZAwiIjpUCQEAAAVoSURBVJIn85RcB1tXV+ezZ8+OOwwRkdQwsznuXpdtmr7hKyJSgZT8RUQqkJK/iEgFUvIXEalAqTnha2aNwPICZ98P+KCE4ZRa0uOD5Meo+IqX9BiTHh8kL8YD3b1XtgmpSf7FMLPZuc54J0HS44Pkx6j4ipf0GJMeH6QjxmYq+4iIVCAlfxGRClQpyX9y3AG0IenxQfJjVHzFS3qMSY8P0hEjUCE1fxER2Vml9PxFRCSDkr+ISAXq0Mk/CT8Wb2b9zOyPZrbQzBaY2TXh+H3N7HdmtiT8u0843sxsQhjzPDM7qoyxVpnZ62b2TPh8gJm9EsbyaHgbbsJbdT8ajn/FzGrKEFsPM3vMzP5qZovMbHjStqGZXRu+x/PN7GEz6xL3NjSzB8xslZnNzxiX93YzszFh+yVmNibi+G4P3+d5ZjbNzHpkTLspjG+xmY3MGB/JZz1bfBnTvmtmbmb7hc/Lvv2K4u4d8kFwy+i3gIOA3YE3gEExxHEAcFQ43J3gh+4HAf8B3BiOvxH4UTh8OvAbwIBhwCtljPU64FfAM+HzXwMXhsP3At8Ih78J3BsOXwg8WobYfg5cHg7vDvRI0jYk+JnSt4E9MrbdpXFvQ+AE4Chgfsa4vLYbsC+wLPy7Tzi8T4TxjQCqw+EfZcQ3KPwcdwYGhJ/vqig/69niC8f3I7g1/XJgv7i2X1GvLe4AInthMByYnvH8JuCmBMT1FHAqsBg4IBx3ALA4HJ4EfDWj/fZ2EcfVF/g98EXgmfAf+IOMD+H27Rn+0w8Ph6vDdhZhbHuHidVajE/MNiRI/u+EH/DqcBuOTMI2BGpaJNe8thvwVWBSxvid2pU6vhbTzgZ+GQ7v9Blu3oZRf9azxQc8BhwB1LMj+cey/Qp9dOSyT/OHsVnmj8jHIjy0PxJ4BfgHd383nPQe8A/hcFxx30nwq2zbwuc9gY/dvSlLHNtjDKevCdtHZQDQCEwJy1I/M7OuJGgbuvtK4MfACuBdgm0yh+Rsw0z5brc4P0tjCXrTtBJHWeMzs7OAle7+RotJiYivvTpy8k8UM+sGPA58x93XZk7zoDsQ2zW3ZnYGsMrd58QVQxuqCQ6973H3I4ENBOWK7RKwDfcBziLYUX0G6AqMiiue9op7u7XGzG4h+CXAX8YdSzMz2xO4Gfhe3LEUqyMn/9Z+RL6szKwTQeL/pbs/EY5+38wOCKcfAKwKx8cR93HAaDOrBx4hKP38X6CHmTX/1GdmHNtjDKfvDayOML4GoMHdXwmfP0awM0jSNjwFeNvdG919C/AEwXZNyjbMlO92K/v2NLNLgTOAi8MdVFLiO5hgB/9G+HnpC7xmZr0TEl+7deTkn4gfizczA+4HFrn7f2ZMehpoPus/huBcQPP4r4dXDgwD1mQcokfC3W9y977uXkOwnf7g7hcDfwS+kiPG5ti/EraPrPfo7u8B75jZwHDUyQS/D52YbUhQ7hlmZnuG73lzjInYhi3ku92mAyPMbJ/wCGdEOC4SZjaKoAQ52t03toj7wvBKqQHAocAsyvhZd/c33X1/d68JPy8NBBd0vEdCtl+7xX3SIcoHwdn3vxFcCXBLTDEcT3BYPQ+YGz5OJ6jv/h5YAswE9g3bGzAxjPlNoK7M8Z7Ejqt9DiL4cC0F/gvoHI7vEj5fGk4/qAxx1QKzw+34JMFVE4nahsCtwF+B+cAvCK5KiXUbAg8TnIPYQpCo/mch242g9r40fFwWcXxLCWrkzZ+XezPa3xLGtxg4LWN8JJ/1bPG1mF7PjhO+Zd9+xTx0ewcRkQrUkcs+IiKSg5K/iEgFUvIXEalASv4iIhVIyV9EpAIp+YuIVCAlfxGRCvT/AY+9+oDmA+wQAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9FQE6bEyhRw","executionInfo":{"status":"ok","timestamp":1620591665149,"user_tz":-60,"elapsed":379,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}},"outputId":"588c5a0c-c06c-4f2c-9f17-2fcfbe03b3b6"},"source":["print(history_single.history.keys())"],"execution_count":14,"outputs":[{"output_type":"stream","text":["dict_keys(['episode_reward', 'nb_episode_steps', 'nb_steps'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Im7tV8L69YpG","executionInfo":{"status":"aborted","timestamp":1620587940324,"user_tz":-60,"elapsed":6071554,"user":{"displayName":"Matthew Goodhead","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyYjvinN_ckX5wiUJHpJkdO21jHFTp5HUPV4EY=s64","userId":"14397948827712679003"}}},"source":[""],"execution_count":null,"outputs":[]}]}